{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca00951b",
   "metadata": {},
   "source": [
    "# SPEAR Manuscript Figures\n",
    "\n",
    "Single-cell Prediction of gene Expression from ATAC-seq Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad505af",
   "metadata": {},
   "source": [
    "## Prereqs\n",
    "\n",
    "- Place run outputs under `output/results/spear_results/` (one subfolder per run with `models/` and metrics CSVs).\n",
    "\n",
    "- Ensure logs (if used) are under `output/logs/` with `spear_*` naming.\n",
    "\n",
    "- Keep `analysis/model_name_lookup.tsv` present (tracked in repo).\n",
    "\n",
    "- Use the `spear_env` kernel with required deps installed (see README).\n",
    "\n",
    "### How to run\n",
    "\n",
    "1. Open this notebook from the repo root.\n",
    "\n",
    "2. Adjust run include globs or paths if you want to target specific runs; otherwise leave defaults.\n",
    "\n",
    "3. Run all cells top-to-bottom after outputs are in place to regenerate figures/CSVs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec47a0c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d64e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:08.501715Z",
     "iopub.status.busy": "2026-01-04T15:24:08.501459Z",
     "iopub.status.idle": "2026-01-04T15:24:16.352423Z",
     "shell.execute_reply": "2026-01-04T15:24:16.351170Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "# Configure plotting defaults for consistent styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "sns.set_context(\"paper\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.dpi\": 160, \"savefig.dpi\": 320})\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.display.width = 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter: Set GENE_COUNT to 100 or 1000 to control which dataset to analyze\n",
    "GENE_COUNT = 100  # Change to 100 for 100-gene analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979660d8",
   "metadata": {},
   "source": [
    "## 2. Analysis Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe1fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.357692Z",
     "iopub.status.busy": "2026-01-04T15:24:16.357083Z",
     "iopub.status.idle": "2026-01-04T15:24:16.374921Z",
     "shell.execute_reply": "2026-01-04T15:24:16.374001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Centralised configuration for the notebook run\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    project_root: Path\n",
    "    results_root: Path\n",
    "    lookup_path: Path\n",
    "    fig_dir: Path\n",
    "    reports_dir: Path\n",
    "    run_include_globs: tuple[str, ...] = (\"*\",)\n",
    "    run_exclude: tuple[str, ...] = tuple()\n",
    "    primary_split: str = \"test\"\n",
    "    val_split: str = \"val\"\n",
    "    train_split: str = \"train\"\n",
    "    top_gene_count: int = 15\n",
    "    top_model_count: int = 3\n",
    "    random_seed: int = 7\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # Create required directories if they are missing.\n",
    "        self.fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.lookup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {\"analysis\", \"scripts\"}:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "config = AnalysisConfig(\n",
    "    project_root=project_root,\n",
    "    results_root=project_root / \"output\" / \"results\" / \"spear_results\",\n",
    "    lookup_path=project_root / \"analysis\" / \"model_name_lookup.tsv\",\n",
    "    fig_dir=project_root / \"analysis\" / \"figs\" / f\"manuscript_{GENE_COUNT}genes\",\n",
    "    reports_dir=project_root / \"analysis\" / \"reports\" / f\"manuscript_{GENE_COUNT}genes\",\n",
    "    run_include_globs=(f\"spear_{GENE_COUNT}genes_*\",),\n",
    "    run_exclude=tuple(),\n",
    "    random_seed=7,\n",
    "    top_gene_count=15,\n",
    "    top_model_count=3,\n",
    ")\n",
    "\n",
    "if not config.results_root.exists():\n",
    "    raise FileNotFoundError(f\"Results directory missing: {config.results_root}\")\n",
    "if not config.lookup_path.exists():\n",
    "    # Seed the lookup table if it is missing so later steps can append to it.\n",
    "    pd.DataFrame({\n",
    "        \"model_id\": [],\n",
    "        \"model_display_name\": [],\n",
    "        \"model_short_name\": [],\n",
    "    }).to_csv(\n",
    "        config.lookup_path, sep=\"\\t\", index=False\n",
    "    )\n",
    "\n",
    "np.random.seed(config.random_seed)\n",
    "rng = np.random.default_rng(config.random_seed)\n",
    "FIGURES: dict[str, plt.Figure] = {}\n",
    "TABLES: dict[str, pd.DataFrame] = {}\n",
    "EXTRA_EXPORT_PATHS: list[tuple[Path, str]] = []\n",
    "analysis_metadata: dict[str, object] = {\n",
    "    \"generated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"project_root\": config.project_root,\n",
    "    \"results_root\": config.results_root,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9dc245",
   "metadata": {},
   "source": [
    "## 3. Run Discovery Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e69628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.378949Z",
     "iopub.status.busy": "2026-01-04T15:24:16.378779Z",
     "iopub.status.idle": "2026-01-04T15:24:16.405828Z",
     "shell.execute_reply": "2026-01-04T15:24:16.405013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Canonical representation of a single trained model output folder\n",
    "@dataclass(frozen=True)\n",
    "class RunRecord:\n",
    "    run_name: str\n",
    "    model_id: str\n",
    "    run_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: Optional[Path]\n",
    "    predictions_path: Optional[Path]\n",
    "    training_history_path: Optional[Path]\n",
    "    model_display: Optional[str] = None\n",
    "\n",
    "\n",
    "LOOKUP_SPECIAL_CASES = {\n",
    "    \"cnn\": \"Convolutional Neural Network\",\n",
    "    \"rnn\": \"Recurrent Neural Network\",\n",
    "    \"lstm\": \"Long Short-Term Memory\",\n",
    "    \"mlp\": \"Multilayer Perceptron\",\n",
    "    \"svr\": \"Support Vector Regressor\",\n",
    "    \"ols\": \"Ordinary Least Squares\",\n",
    "    \"xgboost\": \"XGBoost\",\n",
    "    \"catboost\": \"CatBoost\",\n",
    "    \"hist_gradient_boosting\": \"Histogram Gradient Boosting\",\n",
    "    \"extra_trees\": \"Extra Trees\",\n",
    "    \"random_forest\": \"Random Forest\",\n",
    "    \"elastic_net\": \"Elastic Net\",\n",
    "}\n",
    "\n",
    "SHORT_NAME_FALLBACKS = {\n",
    "    \"Multilayer Perceptron\": \"MLP\",\n",
    "    \"Graph Neural Network\": \"GNN\",\n",
    "    \"Convolutional Neural Network\": \"CNN\",\n",
    "    \"Long Short-Term Memory Network\": \"LSTM\",\n",
    "    \"Recurrent Neural Network\": \"RNN\",\n",
    "    \"Transformer Encoder\": \"Transformer\",\n",
    "    \"Ordinary Least Squares\": \"OLS\",\n",
    "    \"Extra Trees\": \"Extra Trees\",\n",
    "    \"Random Forest\": \"Random Forest\",\n",
    "    \"Ridge Regression\": \"Ridge\",\n",
    "}\n",
    "\n",
    "MODEL_ID_TO_DISPLAY: dict[str, str] = {}\n",
    "MODEL_ID_TO_SHORT: dict[str, str] = {}\n",
    "MODEL_DISPLAY_TO_SHORT: dict[str, str] = {}\n",
    "\n",
    "\n",
    "def _default_short_name(display_name: str) -> str:\n",
    "    # Generate a lightweight abbreviation when none is provided.\n",
    "    if not isinstance(display_name, str) or not display_name.strip():\n",
    "        return \"\"\n",
    "    tokens = [token for token in display_name.replace(\"(\", \" \").replace(\")\", \" \").split() if token]\n",
    "    if not tokens:\n",
    "        return display_name\n",
    "    acronym = \"\".join(token[0] for token in tokens if token and token[0].isalnum()).upper()\n",
    "    if 1 < len(acronym) <= 5:\n",
    "        return acronym\n",
    "    return display_name\n",
    "\n",
    "\n",
    "def compute_heatmap_limits(\n",
    "    values: pd.DataFrame | np.ndarray,\n",
    "    lower_percentile: float = 5.0,\n",
    "    upper_percentile: float = 95.0,\n",
    "    clip: tuple[float, float] = (0.0, 1.0),\n",
    "    min_buffer: float = 0.01,\n",
    ") -> tuple[float, float]:\n",
    "    # Derive consistent vmin/vmax bounds so heatmaps emphasise the dense value range.\n",
    "    data = np.asarray(values, dtype=float)\n",
    "    data = data[np.isfinite(data)]\n",
    "    if data.size == 0:\n",
    "        return clip\n",
    "    lower = np.percentile(data, lower_percentile)\n",
    "    upper = np.percentile(data, upper_percentile)\n",
    "    buffer = max(min_buffer, (upper - lower) * 0.05)\n",
    "    vmin = max(clip[0], lower - buffer)\n",
    "    vmax = min(clip[1], upper + buffer)\n",
    "    if np.isclose(vmin, vmax):\n",
    "        spread = max(min_buffer, abs(vmin) * 0.1 or min_buffer)\n",
    "        vmin = max(clip[0], vmin - spread)\n",
    "        vmax = min(clip[1], vmax + spread)\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "def to_short_name(name: str | None) -> str:\n",
    "    # Return a concise display name for figure titles and filenames.\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    if name in MODEL_DISPLAY_TO_SHORT:\n",
    "        return MODEL_DISPLAY_TO_SHORT[name]\n",
    "    return SHORT_NAME_FALLBACKS.get(name, name)\n",
    "\n",
    "\n",
    "def _read_lookup_table(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame({\n",
    "            \"model_id\": pd.Series(dtype=\"string\"),\n",
    "            \"model_display_name\": pd.Series(dtype=\"string\"),\n",
    "            \"model_short_name\": pd.Series(dtype=\"string\"),\n",
    "        })\n",
    "    df = pd.read_csv(path, sep=\"\t\")\n",
    "    expected = {\"model_id\", \"model_display_name\"}\n",
    "    missing_cols = expected.difference(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Lookup table missing required columns: {sorted(missing_cols)}\")\n",
    "    if \"model_short_name\" not in df.columns:\n",
    "        df[\"model_short_name\"] = df[\"model_display_name\"].map(_default_short_name)\n",
    "    else:\n",
    "        df[\"model_short_name\"] = df[\"model_short_name\"].fillna(\"\")\n",
    "        missing_short = df[\"model_short_name\"].str.strip() == \"\"\n",
    "        df.loc[missing_short, \"model_short_name\"] = df.loc[missing_short, \"model_display_name\"].map(_default_short_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _update_model_name_maps(df: pd.DataFrame) -> None:\n",
    "    # Cache name lookups for downstream plotting helpers.\n",
    "    global MODEL_ID_TO_DISPLAY, MODEL_ID_TO_SHORT, MODEL_DISPLAY_TO_SHORT\n",
    "    if df.empty:\n",
    "        MODEL_ID_TO_DISPLAY = {}\n",
    "        MODEL_ID_TO_SHORT = {}\n",
    "        MODEL_DISPLAY_TO_SHORT = {}\n",
    "        return\n",
    "    standardised = df.fillna(\"\")\n",
    "    MODEL_ID_TO_DISPLAY = {\n",
    "        row.model_id: row.model_display_name or _guess_display_name(row.model_id)\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_ID_TO_SHORT = {\n",
    "        row.model_id: (row.model_short_name or MODEL_ID_TO_DISPLAY[row.model_id])\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_DISPLAY_TO_SHORT = {\n",
    "        MODEL_ID_TO_DISPLAY[row.model_id]: MODEL_ID_TO_SHORT[row.model_id]\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "\n",
    "def _guess_display_name(model_id: str) -> str:\n",
    "    if model_id in LOOKUP_SPECIAL_CASES:\n",
    "        return LOOKUP_SPECIAL_CASES[model_id]\n",
    "    parts = [part for part in model_id.replace(\"-\", \" \").replace(\"_\", \" \").split(\" \") if part]\n",
    "    if not parts:\n",
    "        return model_id\n",
    "    formatted = []\n",
    "    for token in parts:\n",
    "        if len(token) <= 3:\n",
    "            formatted.append(token.upper())\n",
    "        else:\n",
    "            formatted.append(token.capitalize())\n",
    "    return \" \".join(formatted)\n",
    "\n",
    "\n",
    "def _matches_any(value: str, patterns: Iterable[str]) -> bool:\n",
    "    return any(fnmatch(value, pattern) for pattern in patterns) if patterns else False\n",
    "\n",
    "\n",
    "def _first_existing(path: Path, candidates: Sequence[str]) -> Optional[Path]:\n",
    "    for name in candidates:\n",
    "        candidate = path / name\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def discover_model_runs(\n",
    "    results_root: Path,\n",
    "    include_globs: Iterable[str],\n",
    "    exclude_patterns: Iterable[str],\n",
    ") -> list[RunRecord]:\n",
    "    results_root = Path(results_root)\n",
    "    if not results_root.exists():\n",
    "        raise FileNotFoundError(f\"Results root missing: {results_root}\")\n",
    "    records: list[RunRecord] = []\n",
    "    include = tuple(include_globs) if include_globs else (\"*\",)\n",
    "    exclude = tuple(exclude_patterns) if exclude_patterns else tuple()\n",
    "    for run_dir in sorted(results_root.iterdir()):\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "        run_name = run_dir.name\n",
    "        if not _matches_any(run_name, include):\n",
    "            continue\n",
    "        if exclude and _matches_any(run_name, exclude):\n",
    "            continue\n",
    "        models_dir = run_dir / \"models\"\n",
    "        if not models_dir.exists():\n",
    "            continue\n",
    "        for model_dir in sorted(models_dir.iterdir()):\n",
    "            if not model_dir.is_dir():\n",
    "                continue\n",
    "            model_id = model_dir.name\n",
    "            metrics_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"metrics_per_gene.csv\",\n",
    "                    \"metrics_by_gene.csv\",\n",
    "                    \"metrics_cv.csv\",\n",
    "                ),\n",
    "            )\n",
    "            predictions_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"predictions_raw.csv\",\n",
    "                    \"predictions.csv\",\n",
    "                ),\n",
    "            )\n",
    "            history_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"training_history.csv\",\n",
    "                    \"training_history_loss.csv\",\n",
    "                ),\n",
    "            )\n",
    "            records.append(\n",
    "                RunRecord(\n",
    "                    run_name=run_name,\n",
    "                    model_id=model_id,\n",
    "                    run_path=run_dir,\n",
    "                    model_path=model_dir,\n",
    "                    metrics_path=metrics_path,\n",
    "                    predictions_path=predictions_path,\n",
    "                    training_history_path=history_path,\n",
    "                )\n",
    "            )\n",
    "    return records\n",
    "\n",
    "\n",
    "def ensure_model_lookup(path: Path, model_ids: Iterable[str]) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    df = _read_lookup_table(path)\n",
    "    existing = set(df[\"model_id\"]) if not df.empty else set()\n",
    "    new_rows = []\n",
    "    for model_id in sorted(set(model_ids).difference(existing)):\n",
    "        display_name = _guess_display_name(model_id)\n",
    "        short_name = SHORT_NAME_FALLBACKS.get(display_name, _default_short_name(display_name))\n",
    "        new_rows.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"model_display_name\": display_name,\n",
    "                \"model_short_name\": short_name,\n",
    "            }\n",
    "        )\n",
    "    if new_rows:\n",
    "        additions = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, additions], ignore_index=True) if not df.empty else additions\n",
    "        df.sort_values(\"model_id\", inplace=True)\n",
    "        df.to_csv(path, sep=\"\t\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def attach_lookup(records: Sequence[RunRecord], model_lookup: pd.DataFrame) -> list[RunRecord]:\n",
    "    if model_lookup.empty:\n",
    "        return list(records)\n",
    "    display_map = dict(zip(model_lookup[\"model_id\"], model_lookup[\"model_display_name\"]))\n",
    "    resolved: list[RunRecord] = []\n",
    "    for record in records:\n",
    "        display = display_map.get(record.model_id, _guess_display_name(record.model_id))\n",
    "        resolved.append(replace(record, model_display=display))\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def run_records_to_frame(records: Sequence[RunRecord]) -> pd.DataFrame:\n",
    "    if not records:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"run_name\",\n",
    "                \"model_id\",\n",
    "                \"model_display\",\n",
    "                \"run_path\",\n",
    "                \"model_path\",\n",
    "                \"metrics_path\",\n",
    "                \"predictions_path\",\n",
    "                \"training_history_path\",\n",
    "            ]\n",
    "        )\n",
    "    data = [\n",
    "        {\n",
    "            \"run_name\": r.run_name,\n",
    "            \"model_id\": r.model_id,\n",
    "            \"model_display\": r.model_display or _guess_display_name(r.model_id),\n",
    "            \"run_path\": r.run_path,\n",
    "            \"model_path\": r.model_path,\n",
    "            \"metrics_path\": r.metrics_path,\n",
    "            \"predictions_path\": r.predictions_path,\n",
    "            \"training_history_path\": r.training_history_path,\n",
    "        }\n",
    "        for r in records\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def to_relative_path(path_like: Optional[Path], root: Path) -> Optional[str]:\n",
    "    if path_like is None:\n",
    "        return None\n",
    "    path = Path(path_like)\n",
    "    try:\n",
    "        return str(path.resolve().relative_to(root))\n",
    "    except Exception:\n",
    "        return str(path.resolve())\n",
    "\n",
    "\n",
    "def maybe_store_table(store: dict[str, pd.DataFrame], key: str, table: pd.DataFrame) -> None:\n",
    "    if table is None or table.empty:\n",
    "        return\n",
    "    store[key] = table\n",
    "\n",
    "\n",
    "def register_figure(store: dict[str, object], key: str, fig: Optional[plt.Figure]) -> None:\n",
    "    # Track generated matplotlib figures for later export.\n",
    "    if fig is None:\n",
    "        store.pop(key, None)\n",
    "        return\n",
    "    store[key] = fig\n",
    "\n",
    "\n",
    "def load_metrics(records: Sequence[RunRecord]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames: list[pd.DataFrame] = []\n",
    "    for record in records:\n",
    "        metrics_path = record.metrics_path\n",
    "        if metrics_path is None or not metrics_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to load metrics from {metrics_path}: {exc}\")\n",
    "            continue\n",
    "        required_cols = {\"gene\", \"split\", \"pearson\"}\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            continue\n",
    "        df = df.copy()\n",
    "        df[\"run_name\"] = record.run_name\n",
    "        df[\"model_id\"] = record.model_id\n",
    "        df[\"model_display\"] = record.model_display or _guess_display_name(record.model_id)\n",
    "        long_frames.append(df)\n",
    "    if not long_frames:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    metrics_long = pd.concat(long_frames, ignore_index=True)\n",
    "    base_cols = [col for col in [\"run_name\", \"model_id\", \"model_display\", \"gene\", \"split\"] if col in metrics_long.columns]\n",
    "    metric_cols = [col for col in metrics_long.columns if col not in base_cols]\n",
    "    metrics_long = metrics_long[base_cols + metric_cols]\n",
    "\n",
    "    wide = metrics_long.pivot_table(\n",
    "        index=[\"run_name\", \"model_id\", \"model_display\", \"gene\"],\n",
    "        columns=\"split\",\n",
    "        values=\"pearson\",\n",
    "    )\n",
    "    wide.columns = [f\"{str(col).lower()}_pearson\" for col in wide.columns]\n",
    "    metrics_wide = wide.reset_index()\n",
    "\n",
    "    return metrics_long, metrics_wide\n",
    "\n",
    "\n",
    "def compute_model_summary(\n",
    "    metrics_wide: pd.DataFrame,\n",
    "    splits: Sequence[str],\n",
    ") -> pd.DataFrame:\n",
    "    if metrics_wide.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model_display\",\n",
    "                \"model_id\",\n",
    "                \"run_name\",\n",
    "                *[f\"{split}_pearson_mean\" for split in splits],\n",
    "                *[f\"{split}_pearson_std\" for split in splits],\n",
    "            ]\n",
    "        )\n",
    "    summaries = []\n",
    "    lower_splits = [split.lower() for split in splits]\n",
    "    for (run_name, model_id, model_display), group in metrics_wide.groupby([\"run_name\", \"model_id\", \"model_display\"], dropna=False):\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"model_display\": model_display,\n",
    "        }\n",
    "        for split, lower in zip(splits, lower_splits):\n",
    "            column = f\"{lower}_pearson\"\n",
    "            if column in group:\n",
    "                values = group[column].dropna()\n",
    "                if not values.empty:\n",
    "                    row[f\"{split}_pearson_mean\"] = values.mean()\n",
    "                    row[f\"{split}_pearson_std\"] = values.std(ddof=1) if len(values) > 1 else float(\"nan\")\n",
    "        summaries.append(row)\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    if \"test_pearson_mean\" in summary_df:\n",
    "        summary_df.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    summary_df.set_index([\"model_display\", \"model_id\", \"run_name\"], inplace=True)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146fb895",
   "metadata": {},
   "source": [
    "## 4. Discover and Inspect Run Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c59869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.408881Z",
     "iopub.status.busy": "2026-01-04T15:24:16.408733Z",
     "iopub.status.idle": "2026-01-04T15:24:16.629335Z",
     "shell.execute_reply": "2026-01-04T15:24:16.628444Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_run_records = discover_model_runs(\n",
    "    config.results_root,\n",
    "    config.run_include_globs,\n",
    "    config.run_exclude,\n",
    ")\n",
    "model_lookup = ensure_model_lookup(\n",
    "    config.lookup_path,\n",
    "    [record.model_id for record in raw_run_records],\n",
    ")\n",
    "_update_model_name_maps(model_lookup)\n",
    "run_records = attach_lookup(raw_run_records, model_lookup)\n",
    "run_df = run_records_to_frame(run_records)\n",
    "run_df.sort_values([\"run_name\", \"model_id\"], inplace=True)\n",
    "run_df_display = run_df.copy()\n",
    "for column in (\"run_path\", \"model_path\", \"metrics_path\", \"predictions_path\", \"training_history_path\"):\n",
    "    run_df_display[column] = run_df_display[column].map(lambda value: to_relative_path(value, config.project_root))\n",
    "analysis_metadata.update({\n",
    "    \"results_root\": to_relative_path(config.results_root, config.project_root),\n",
    "    \"fig_dir\": to_relative_path(config.fig_dir, config.project_root),\n",
    "    \"reports_dir\": to_relative_path(config.reports_dir, config.project_root),\n",
    "    \"run_count\": run_df[\"run_name\"].nunique(),\n",
    "    \"model_count\": len(run_df),\n",
    "    \"model_lookup_path\": to_relative_path(config.lookup_path, config.project_root),\n",
    "})\n",
    "analysis_metadata.setdefault(\"include_globs\", config.run_include_globs)\n",
    "display(Markdown(f\"**Scanning results root:** `{analysis_metadata['results_root']}`\"))\n",
    "include_filters = analysis_metadata.get(\"include_globs\")\n",
    "if include_filters:\n",
    "    include_text = \", \".join(str(item) for item in include_filters)\n",
    "    display(Markdown(f\"**Include filters:** `{include_text}`\"))\n",
    "display(Markdown(\n",
    "    f\"**Figure output:** `{analysis_metadata['fig_dir']}` | **Reports:** `{analysis_metadata['reports_dir']}`\"\n",
    "))\n",
    "display(run_df_display)\n",
    "print(\n",
    "    \"Discovered\",\n",
    "    analysis_metadata[\"model_count\"],\n",
    "    \"model folders across\",\n",
    "    analysis_metadata[\"run_count\"],\n",
    "    \"runs.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07624b37",
   "metadata": {},
   "source": [
    "## 5. Load Metrics and Compute Summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8d977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.632447Z",
     "iopub.status.busy": "2026-01-04T15:24:16.632274Z",
     "iopub.status.idle": "2026-01-04T15:24:16.865970Z",
     "shell.execute_reply": "2026-01-04T15:24:16.865119Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long, metrics_wide = load_metrics(run_records)\n",
    "if metrics_long.empty:\n",
    "    raise RuntimeError(\"No metrics available for plotting.\")\n",
    "\n",
    "split_filter = {config.primary_split, config.val_split, config.train_split}\n",
    "available_splits = sorted(metrics_long[\"split\"].unique())\n",
    "missing_splits = split_filter.difference(available_splits)\n",
    "if missing_splits:\n",
    "    print(\"Warning: the following splits are missing from metrics files:\", sorted(missing_splits))\n",
    "\n",
    "test_metrics = metrics_long[metrics_long[\"split\"] == config.primary_split].copy()\n",
    "val_metrics = metrics_long[metrics_long[\"split\"] == config.val_split].copy()\n",
    "train_metrics = metrics_long[metrics_long[\"split\"] == config.train_split].copy()\n",
    "summary_df = compute_model_summary(\n",
    "    metrics_wide, [config.primary_split, config.val_split, config.train_split]\n",
    ")\n",
    "if summary_df.empty:\n",
    "    raise RuntimeError(\"Unable to compute summary statistics from metrics.\")\n",
    "\n",
    "summary_reset = summary_df.reset_index()\n",
    "analysis_metadata[\"best_model_id\"] = summary_reset.iloc[0][\"model_id\"]\n",
    "analysis_metadata[\"best_model_display\"] = summary_reset.iloc[0][\"model_display\"]\n",
    "analysis_metadata[\"best_run_name\"] = summary_reset.iloc[0][\"run_name\"]\n",
    "\n",
    "if \"test_pearson_mean\" in summary_reset:\n",
    "    model_display_order = summary_reset.sort_values(\n",
    "        by=\"test_pearson_mean\", ascending=False\n",
    "    )[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "else:\n",
    "    model_display_order = summary_reset[\"model_display\"].tolist()\n",
    "\n",
    "display(summary_reset)\n",
    "\n",
    "analysis_state = {\n",
    "    \"run_df\": run_df,\n",
    "    \"metrics_long\": metrics_long,\n",
    "    \"metrics_wide\": metrics_wide,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"summary_df\": summary_df,\n",
    "    \"summary_reset\": summary_reset,\n",
    "    \"model_display_order\": model_display_order,\n",
    "    \"model_short_name_map\": MODEL_ID_TO_SHORT.copy(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4831c25",
   "metadata": {},
   "source": [
    "## 6. Supporting Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.868901Z",
     "iopub.status.busy": "2026-01-04T15:24:16.868688Z",
     "iopub.status.idle": "2026-01-04T15:24:16.887274Z",
     "shell.execute_reply": "2026-01-04T15:24:16.886482Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df = analysis_state[\"summary_df\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "metrics_wide = analysis_state[\"metrics_wide\"]\n",
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "\n",
    "val_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.val_split}_pearson\" in metrics_wide:\n",
    "    val_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.val_split}_pearson\",\n",
    "    )\n",
    "\n",
    "test_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "    )\n",
    "\n",
    "maybe_store_table(\n",
    "    TABLES,\n",
    "    \"metrics_per_gene_master\",\n",
    "    metrics_long.sort_values([\"split\", \"run_name\", \"model_id\", \"gene\"]),\n",
    ")\n",
    "maybe_store_table(TABLES, \"summary_metrics_all_models\", summary_reset)\n",
    "\n",
    "analysis_state.update(\n",
    "    {\n",
    "        \"val_pearson_per_gene\": val_pearson_per_gene,\n",
    "        \"test_pearson_per_gene\": test_pearson_per_gene,\n",
    "        \"split_mean_summary\": summary_reset,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ff85b",
   "metadata": {},
   "source": [
    "## 7. Heatmap: Embryonic vs Endothelial Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b0986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:16.890252Z",
     "iopub.status.busy": "2026-01-04T15:24:16.890103Z",
     "iopub.status.idle": "2026-01-04T15:24:17.513237Z",
     "shell.execute_reply": "2026-01-04T15:24:17.512468Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "\n",
    "\n",
    "def _assign_dataset(run_name: str) -> Optional[str]:\n",
    "    name = str(run_name).lower()\n",
    "    if \"embryonic\" in name:\n",
    "        return \"Embryonic\"\n",
    "    if \"endothelial\" in name:\n",
    "        return \"Endothelial\"\n",
    "    return None\n",
    "\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "subset = summary_reset[summary_reset[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])].copy()\n",
    "fig_heatmap = None\n",
    "if subset.empty or \"test_pearson_mean\" not in subset:\n",
    "    print(\"Dataset-specific test Pearson summaries unavailable; skipping heatmap.\")\n",
    "else:\n",
    "    ranked = subset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates([\"dataset\", \"model_id\"])\n",
    "    heatmap_df = best_per_model.pivot_table(\n",
    "        index=\"model_display\",\n",
    "        columns=\"dataset\",\n",
    "        values=\"test_pearson_mean\",\n",
    "    )\n",
    "    if heatmap_df.empty:\n",
    "        print(\"No values available for dataset heatmap.\")\n",
    "    else:\n",
    "        order = heatmap_df.mean(axis=1).sort_values(ascending=False).index\n",
    "        heatmap_df = heatmap_df.loc[order]\n",
    "        column_order = [col for col in [\"Embryonic\", \"Endothelial\"] if col in heatmap_df.columns]\n",
    "        heatmap_df = heatmap_df[column_order]\n",
    "        vmin, vmax = compute_heatmap_limits(heatmap_df.values)\n",
    "        fig_height = max(4, 0.4 * len(heatmap_df))\n",
    "        fig_width = max(7, 2.0 * len(column_order))\n",
    "        fig_heatmap, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            linewidths=0.4,\n",
    "            linecolor=\"#f2f2f2\",\n",
    "            cbar_kws={\"label\": \"Mean Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Mean Test Pearson by Model and Dataset\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        ax.tick_params(axis=\"x\", pad=8)\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        sns.despine(fig_heatmap, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        fig_heatmap.subplots_adjust(bottom=0.22)\n",
    "\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_by_dataset\", fig_heatmap)\n",
    "if fig_heatmap is not None:\n",
    "    display(fig_heatmap)\n",
    "    plt.close(fig_heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17172c89",
   "metadata": {},
   "source": [
    "## 8. Violin Plot: Test Pearson by Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f286cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:17.518756Z",
     "iopub.status.busy": "2026-01-04T15:24:17.518459Z",
     "iopub.status.idle": "2026-01-04T15:24:18.396688Z",
     "shell.execute_reply": "2026-01-04T15:24:18.395753Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"].copy()\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "subset = metrics_long[\n",
    "    (metrics_long[\"split\"] == config.primary_split)\n",
    "    & (metrics_long[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"]))\n",
    "].copy()\n",
    "\n",
    "fig_violin_by_dataset = None\n",
    "if subset.empty:\n",
    "    print(\"Test metrics unavailable for embryonic/endothelial comparison.\")\n",
    "else:\n",
    "    ordered_models = [m for m in model_display_order if m in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = subset[\"model_display\"].sort_values().unique().tolist()\n",
    "    dataset_order = [\"Embryonic\", \"Endothelial\"]\n",
    "    palette = {\"Embryonic\": \"#4C78A8\", \"Endothelial\": \"#F58518\"}\n",
    "    fig_width = max(10, 0.75 * max(6, len(ordered_models)))\n",
    "    fig_violin_by_dataset, ax = plt.subplots(figsize=(fig_width, 6.5))\n",
    "    sns.violinplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset\",\n",
    "        order=ordered_models,\n",
    "        hue_order=dataset_order,\n",
    "        palette=palette,\n",
    "        density_norm=\"width\",\n",
    "        inner=\"quartile\",\n",
    "        linewidth=1.0,\n",
    "        dodge=True,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = subset.sample(\n",
    "        min(len(subset), 4000),\n",
    "        random_state=config.random_seed,\n",
    "    ) if len(subset) > 4000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset\",\n",
    "        order=ordered_models,\n",
    "        hue_order=dataset_order,\n",
    "        palette=palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=2.4,\n",
    "        alpha=0.35,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.3,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(\"Test Pearson Distribution by Model and Dataset\")\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Test Pearson\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Dataset\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_violin_by_dataset, left=True, bottom=True)\n",
    "    fig_violin_by_dataset.tight_layout()\n",
    "\n",
    "register_figure(FIGURES, \"test_violin_by_dataset\", fig_violin_by_dataset)\n",
    "if fig_violin_by_dataset is not None:\n",
    "    display(fig_violin_by_dataset)\n",
    "    plt.close(fig_violin_by_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf6110",
   "metadata": {},
   "source": [
    "## 9. Generalization Gap by Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c3730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:18.407225Z",
     "iopub.status.busy": "2026-01-04T15:24:18.407023Z",
     "iopub.status.idle": "2026-01-04T15:24:18.872265Z",
     "shell.execute_reply": "2026-01-04T15:24:18.871559Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "fig_gap = None\n",
    "required_cols = {\"train_pearson_mean\", \"test_pearson_mean\"}\n",
    "subset = summary_reset[summary_reset[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])].copy()\n",
    "if subset.empty or not required_cols.issubset(subset.columns):\n",
    "    print(\"Train/test summary columns unavailable; skipping generalization gap plot.\")\n",
    "else:\n",
    "    fig_gap, axes = plt.subplots(ncols=2, figsize=(12, 6), sharex=True)\n",
    "    for ax, dataset in zip(axes, [\"Embryonic\", \"Endothelial\"]):\n",
    "        data = subset[subset[\"dataset\"] == dataset].copy()\n",
    "        if data.empty:\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"{dataset} (no data)\")\n",
    "            continue\n",
    "        ranked = data.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "        best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "        gap_df = best_per_model[[\"model_display\", \"train_pearson_mean\", \"test_pearson_mean\"]].copy()\n",
    "        gap_df[\"generalization_gap\"] = gap_df[\"train_pearson_mean\"] - gap_df[\"test_pearson_mean\"]\n",
    "        gap_df.sort_values(\"generalization_gap\", ascending=False, inplace=True)\n",
    "        colors = sns.color_palette(\"mako\", n_colors=len(gap_df))\n",
    "        bars = ax.barh(\n",
    "            gap_df[\"model_display\"],\n",
    "            gap_df[\"generalization_gap\"],\n",
    "            color=colors,\n",
    "            linewidth=0,\n",
    "        )\n",
    "        ax.axvline(0.0, color=\"#6d6d6d\", linestyle=\"--\", linewidth=1)\n",
    "        gap_min = gap_df[\"generalization_gap\"].min()\n",
    "        gap_max = gap_df[\"generalization_gap\"].max()\n",
    "        span = max(0.01, gap_max - gap_min)\n",
    "        margin = max(0.08, span * 0.12)\n",
    "        xmin = min(gap_min - margin, gap_min - 0.02)\n",
    "        xmin = min(xmin, -0.15)\n",
    "        xmax = max(gap_max + margin, 0.15)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_xlabel(\"Train minus Test Mean Pearson\")\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        ax.set_title(f\"{dataset}: Generalization Gap\")\n",
    "        label_offset = (xmax - xmin) * 0.02\n",
    "        for bar, value in zip(bars, gap_df[\"generalization_gap\"]):\n",
    "            y = bar.get_y() + bar.get_height() / 2\n",
    "            if value >= 0:\n",
    "                ax.text(value + label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"#1f1f1f\")\n",
    "            else:\n",
    "                ax.text(value - label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"right\", fontsize=9, color=\"#1f1f1f\")\n",
    "        sns.despine(ax=ax, left=True, bottom=True)\n",
    "    fig_gap.tight_layout()\n",
    "\n",
    "register_figure(FIGURES, \"generalization_gap_by_dataset\", fig_gap)\n",
    "if fig_gap is not None:\n",
    "    display(fig_gap)\n",
    "    plt.close(fig_gap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5adf89",
   "metadata": {},
   "source": [
    "## 10. Per-gene Pearson by Split and Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:18.878314Z",
     "iopub.status.busy": "2026-01-04T15:24:18.878156Z",
     "iopub.status.idle": "2026-01-04T15:24:20.042904Z",
     "shell.execute_reply": "2026-01-04T15:24:20.041622Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"].copy()\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_assign_dataset)\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "\n",
    "splits_of_interest = [config.train_split, config.val_split, config.primary_split]\n",
    "subset = metrics_long[\n",
    "    metrics_long[\"split\"].isin(splits_of_interest)\n",
    "    & metrics_long[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])\n",
    "].copy()\n",
    "\n",
    "fig_split_compare = None\n",
    "if subset.empty or \"pearson\" not in subset:\n",
    "    print(\"Pearson metrics unavailable across requested splits; skipping split comparison plot.\")\n",
    "else:\n",
    "    subset = subset[[\"model_display\", \"split\", \"pearson\", \"dataset\"]].dropna()\n",
    "    split_labels = {\n",
    "        config.train_split: \"Train\",\n",
    "        config.val_split: \"Val\",\n",
    "        config.primary_split: \"Test\",\n",
    "    }\n",
    "    subset[\"split_label\"] = subset[\"split\"].map(split_labels).fillna(subset[\"split\"].str.title())\n",
    "\n",
    "    ordered_models = [model for model in model_display_order if model in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = subset[\"model_display\"].sort_values().unique().tolist()\n",
    "\n",
    "    # Build 6 hues per model: 3 splits per dataset with shade ramps\n",
    "    hue_order = [\n",
    "        \"Embryonic | Train\",\n",
    "        \"Embryonic | Val\",\n",
    "        \"Embryonic | Test\",\n",
    "        \"Endothelial | Train\",\n",
    "        \"Endothelial | Val\",\n",
    "        \"Endothelial | Test\",\n",
    "    ]\n",
    "    palette = {\n",
    "        \"Embryonic | Train\": \"#9ecae1\",\n",
    "        \"Embryonic | Val\": \"#6baed6\",\n",
    "        \"Embryonic | Test\": \"#3182bd\",\n",
    "        \"Endothelial | Train\": \"#fdd0a2\",\n",
    "        \"Endothelial | Val\": \"#fdae6b\",\n",
    "        \"Endothelial | Test\": \"#e6550d\",\n",
    "    }\n",
    "    subset[\"dataset_split\"] = subset[\"dataset\"] + \" | \" + subset[\"split_label\"]\n",
    "\n",
    "    fig_width = max(16, 1.2 * max(8, len(ordered_models)))\n",
    "    fig_split_compare, ax = plt.subplots(figsize=(fig_width, 7.2))\n",
    "    sns.boxplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset_split\",\n",
    "        order=ordered_models,\n",
    "        hue_order=hue_order,\n",
    "        palette=palette,\n",
    "        width=0.7,\n",
    "        fliersize=0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = subset.sample(min(len(subset), 5000), random_state=config.random_seed) if len(subset) > 5000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset_split\",\n",
    "        order=ordered_models,\n",
    "        hue_order=hue_order,\n",
    "        palette=palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=2.4,\n",
    "        alpha=0.35,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.3,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    metric_min = subset[\"pearson\"].min()\n",
    "    metric_max = subset[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Pearson\")\n",
    "    ax.set_title(\"Per-gene Pearson by Split and Dataset\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Dataset | Split\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_split_compare, left=True, bottom=True)\n",
    "    fig_split_compare.tight_layout()\n",
    "\n",
    "register_figure(FIGURES, \"split_comparison_by_dataset\", fig_split_compare)\n",
    "if fig_split_compare is not None:\n",
    "    display(fig_split_compare)\n",
    "    plt.close(fig_split_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442dde0",
   "metadata": {},
   "source": [
    "## 11. Best Model Diagnostics per Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66852272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:20.055681Z",
     "iopub.status.busy": "2026-01-04T15:24:20.055503Z",
     "iopub.status.idle": "2026-01-04T15:24:20.571375Z",
     "shell.execute_reply": "2026-01-04T15:24:20.570693Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_feature_importance_table(model_dir: Path) -> tuple[pd.DataFrame, Optional[Path]]:\n",
    "    # Locate and standardise feature importances exported by the training pipeline.\n",
    "    patterns = [\n",
    "        \"feature_importance*.csv\",\n",
    "        \"feature_importances*.csv\",\n",
    "        \"feature_importance*.tsv\",\n",
    "        \"feature_importances*.tsv\",\n",
    "        \"feature_importance*.parquet\",\n",
    "        \"feature_importances*.parquet\",\n",
    "    ]\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns:\n",
    "        candidates.extend(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "    unique_candidates: list[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved in seen:\n",
    "            continue\n",
    "        if resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "    for candidate in sorted(unique_candidates):\n",
    "        try:\n",
    "            if candidate.suffix.lower() == \".parquet\":\n",
    "                df = pd.read_parquet(candidate)\n",
    "            else:\n",
    "                sep = \"\t\" if candidate.suffix.lower() in {\".tsv\", \".txt\"} else \",\"\n",
    "                df = pd.read_csv(candidate, sep=sep)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {candidate.name}: failed to load ({exc})\")\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        lower_cols = {col.lower(): col for col in df.columns}\n",
    "        feature_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"feature\", \"feature_name\", \"name\", \"variable\", \"feature_id\", \"column\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        importance_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\n",
    "                \"importance\",\n",
    "                \"importance_score\",\n",
    "                \"importance_mean\",\n",
    "                \"score\",\n",
    "                \"value\",\n",
    "                \"gain\",\n",
    "                \"weight\",\n",
    "            )\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        if feature_col is None or importance_col is None:\n",
    "            continue\n",
    "        out = df.copy()\n",
    "        out.rename(columns={feature_col: \"feature\", importance_col: \"importance\"}, inplace=True)\n",
    "        out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "        out[\"importance\"] = pd.to_numeric(out[\"importance\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"feature\", \"importance\"])\n",
    "        if out.empty:\n",
    "            continue\n",
    "        extra_cols = [col for col in out.columns if col not in {\"feature\", \"importance\"}]\n",
    "        out = out[[\"feature\", \"importance\", *extra_cols]]\n",
    "        out.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        return out.reset_index(drop=True), candidate\n",
    "    return pd.DataFrame(columns=[\"feature\", \"importance\"]), None\n",
    "\n",
    "\n",
    "def _best_model_for_dataset(summary_reset: pd.DataFrame, dataset: str) -> Optional[pd.Series]:\n",
    "    subset = summary_reset[summary_reset[\"dataset\"] == dataset].copy()\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    if \"test_pearson_mean\" in subset:\n",
    "        subset.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    return subset.iloc[0]\n",
    "\n",
    "\n",
    "def _ensure_distance_column(df: pd.DataFrame) -> tuple[pd.DataFrame, Optional[str], bool]:\n",
    "    if df is None or df.empty:\n",
    "        return df, None, False\n",
    "\n",
    "    signed_candidates = [\n",
    "        (\"delta_to_tss_kb\", True),\n",
    "        (\"distance_to_tss_kb\", True),\n",
    "        (\"delta_to_tss_bp\", False),\n",
    "        (\"distance_to_tss_bp\", False),\n",
    "    ]\n",
    "    for name, is_kb in signed_candidates:\n",
    "        if name in df.columns:\n",
    "            values = pd.to_numeric(df[name], errors=\"coerce\")\n",
    "            if (values.dropna() < 0).any():\n",
    "                return df, name, is_kb\n",
    "\n",
    "    def _infer_signed_distance(series: pd.Series) -> pd.Series:\n",
    "        import re\n",
    "        bin_pattern = re.compile(r\"bin_(-?\\d+)_to_(-?\\d+)\")\n",
    "        def _infer_distance(feature_name: str) -> Optional[float]:\n",
    "            if not isinstance(feature_name, str):\n",
    "                return None\n",
    "            token = feature_name.split(\"|\", 1)[-1]\n",
    "            match = bin_pattern.search(token)\n",
    "            if match:\n",
    "                start_bp = float(match.group(1))\n",
    "                end_bp = float(match.group(2))\n",
    "                return 0.5 * (start_bp + end_bp)\n",
    "            return None\n",
    "        return series.map(_infer_distance)\n",
    "\n",
    "    if \"feature\" in df.columns:\n",
    "        inferred = _infer_signed_distance(df[\"feature\"])\n",
    "        if inferred.notna().any():\n",
    "            df = df.copy()\n",
    "            df[\"distance_to_tss_bp_signed\"] = inferred\n",
    "            return df, \"distance_to_tss_bp_signed\", False\n",
    "\n",
    "    return df, None, False\n",
    "\n",
    "\n",
    "def _plot_feature_importance_distance(\n",
    "    df: pd.DataFrame,\n",
    "    distance_col: str,\n",
    "    distance_is_kb: bool,\n",
    "    title: str,\n",
    "    *,\n",
    "    max_distance_kb: float = 10.0,\n",
    "    y_limits: Optional[tuple[float, float]] = None,\n",
    "    show_scatter: bool = False,\n",
    ") -> Optional[plt.Figure]:\n",
    "    if df is None or df.empty or distance_col is None:\n",
    "        return None\n",
    "    valid = df.dropna(subset=[\"importance\", distance_col]).copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    distance_values = pd.to_numeric(valid[distance_col], errors=\"coerce\")\n",
    "    valid[\"distance_kb\"] = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "    valid = valid[valid[\"distance_kb\"].abs() <= max_distance_kb].copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    if not (valid[\"distance_kb\"] < 0).any():\n",
    "        print(\"Warning: no negative distance-to-TSS values detected; data may be absolute distances.\")\n",
    "    valid.sort_values(\"distance_kb\", inplace=True)\n",
    "\n",
    "    per_bin = (\n",
    "        valid.groupby(\"distance_kb\", sort=True)[\"importance\"]\n",
    "        .quantile(0.9)\n",
    "        .reset_index()\n",
    "    )\n",
    "    if per_bin.empty:\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "    if show_scatter:\n",
    "        sns.scatterplot(\n",
    "            data=valid,\n",
    "            x=\"distance_kb\",\n",
    "            y=\"importance\",\n",
    "            s=30,\n",
    "            alpha=0.45,\n",
    "            edgecolor=\"none\",\n",
    "            color=\"#4daf4a\",\n",
    "            ax=ax,\n",
    "        )\n",
    "    ax.plot(\n",
    "        per_bin[\"distance_kb\"],\n",
    "        per_bin[\"importance\"],\n",
    "        color=\"#e41a1c\",\n",
    "        linewidth=1.5,\n",
    "        label=\"90th percentile\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    ax.axvline(0.0, color=\"#999999\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlim(-max_distance_kb, max_distance_kb)\n",
    "    if y_limits is not None:\n",
    "        ax.set_ylim(y_limits)\n",
    "    ax.set_xlabel(\"Distance to TSS (kb)\")\n",
    "    ax.set_ylabel(\"Feature importance\")\n",
    "    ax.set_title(f\"{title} ({max_distance_kb:g} kb)\")\n",
    "    sns.despine(fig, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "    best_row = _best_model_for_dataset(summary_reset, dataset)\n",
    "    if best_row is None:\n",
    "        print(f\"No best model found for {dataset}.\")\n",
    "        continue\n",
    "    run_match = run_df[\n",
    "        (run_df[\"run_name\"] == best_row[\"run_name\"]) & (run_df[\"model_id\"] == best_row[\"model_id\"])\n",
    "    ]\n",
    "    if run_match.empty:\n",
    "        print(f\"Run metadata missing for {dataset} best model.\")\n",
    "        continue\n",
    "    row = run_match.iloc[0]\n",
    "    model_dir = row[\"model_path\"] if isinstance(row[\"model_path\"], Path) else Path(row[\"model_path\"])\n",
    "    scatter_path = model_dir / \"scatter_test.png\"\n",
    "    display(Markdown(\n",
    "        f\"**{dataset} best model:** `{best_row['model_display']}` (run `{best_row['run_name']}`)\"\n",
    "    ))\n",
    "    if scatter_path.exists():\n",
    "        display(Image(filename=str(scatter_path)))\n",
    "        EXTRA_EXPORT_PATHS.append((scatter_path, f\"{dataset.lower()}_scatter_test.png\"))\n",
    "    else:\n",
    "        preds_path = row.get(\"predictions_path\")\n",
    "        preds_path = preds_path if isinstance(preds_path, Path) else Path(preds_path) if preds_path else None\n",
    "        if preds_path is None or not preds_path.exists():\n",
    "            print(f\"No scatter or prediction file found for {dataset} best model.\")\n",
    "        else:\n",
    "            df = pd.read_csv(preds_path)\n",
    "            df = df[df[\"split\"] == config.primary_split]\n",
    "            if df.empty:\n",
    "                print(f\"No test predictions available for {dataset} best model.\")\n",
    "            else:\n",
    "                sample = df.sample(min(len(df), 8000), random_state=config.random_seed)\n",
    "                fig_scatter, ax = plt.subplots(figsize=(5.2, 5.2))\n",
    "                sns.scatterplot(\n",
    "                    data=sample,\n",
    "                    x=\"y_true\",\n",
    "                    y=\"y_pred\",\n",
    "                    s=14,\n",
    "                    alpha=0.4,\n",
    "                    edgecolor=\"none\",\n",
    "                    color=\"#377eb8\",\n",
    "                    ax=ax,\n",
    "                )\n",
    "                lims = [\n",
    "                    min(sample[\"y_true\"].min(), sample[\"y_pred\"].min()),\n",
    "                    max(sample[\"y_true\"].max(), sample[\"y_pred\"].max()),\n",
    "                ]\n",
    "                ax.plot(lims, lims, linestyle=\"--\", color=\"#6d6d6d\", linewidth=1)\n",
    "                ax.set_xlim(lims)\n",
    "                ax.set_ylim(lims)\n",
    "                ax.set_xlabel(\"Observed\")\n",
    "                ax.set_ylabel(\"Predicted\")\n",
    "                ax.set_title(f\"{dataset}: Test Scatter | {best_row['model_display']}\")\n",
    "                sns.despine(fig_scatter, left=True, bottom=True)\n",
    "                plt.tight_layout()\n",
    "                register_figure(FIGURES, f\"{dataset.lower()}_best_model_scatter\", fig_scatter)\n",
    "                display(fig_scatter)\n",
    "                plt.close(fig_scatter)\n",
    "\n",
    "    importance_df, importance_path = _load_feature_importance_table(model_dir)\n",
    "    if importance_df.empty:\n",
    "        print(f\"No feature importance table found for {dataset} best model.\")\n",
    "        continue\n",
    "    importance_df, distance_col, distance_is_kb = _ensure_distance_column(importance_df)\n",
    "    if distance_col is None or distance_col not in importance_df.columns:\n",
    "        print(f\"Unable to infer distance-to-TSS column for {dataset} best model.\")\n",
    "        continue\n",
    "    fig_distance = _plot_feature_importance_distance(\n",
    "        importance_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: Feature Importance vs Distance to TSS\",\n",
    "        max_distance_kb=10.0,\n",
    "        show_scatter=True,\n",
    "    )\n",
    "    zoom_mask = importance_df[distance_col].notna()\n",
    "    if zoom_mask.any():\n",
    "        distance_values = pd.to_numeric(importance_df.loc[zoom_mask, distance_col], errors=\"coerce\")\n",
    "        distance_kb = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "        zoom_range = distance_kb.abs() <= 5.0\n",
    "        zoom_importance = pd.to_numeric(importance_df.loc[zoom_mask, \"importance\"], errors=\"coerce\")\n",
    "        zoom_df = pd.DataFrame({\"distance_kb\": distance_kb[zoom_range], \"importance\": zoom_importance[zoom_range]})\n",
    "        if zoom_df.empty:\n",
    "            zoom_max = None\n",
    "        else:\n",
    "            zoom_max = zoom_df.groupby(\"distance_kb\")[\"importance\"].quantile(0.9).max()\n",
    "        zoom_y_limits = (0.0, float(zoom_max) * 1.15) if pd.notna(zoom_max) else None\n",
    "    else:\n",
    "        zoom_y_limits = None\n",
    "    fig_distance_zoomed = _plot_feature_importance_distance(\n",
    "        importance_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: Feature Importance vs Distance to TSS (zoomed)\",\n",
    "        max_distance_kb=5.0,\n",
    "        y_limits=zoom_y_limits,\n",
    "    )\n",
    "    if fig_distance is None or fig_distance_zoomed is None:\n",
    "        print(f\"No valid distance/importance data for {dataset} best model.\")\n",
    "    else:\n",
    "        model_short = MODEL_ID_TO_SHORT.get(best_row['model_id'], best_row['model_display'])\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_feature_importance_tss\", fig_distance)\n",
    "        display(fig_distance)\n",
    "        plt.close(fig_distance)\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_feature_importance_tss_zoomed\", fig_distance_zoomed)\n",
    "        display(fig_distance_zoomed)\n",
    "        plt.close(fig_distance_zoomed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120aa7e",
   "metadata": {},
   "source": [
    "## 12. Export Figures\n",
    "\n",
    "Save all generated matplotlib figures to the manuscript figure directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e735d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:24:20.574953Z",
     "iopub.status.busy": "2026-01-04T15:24:20.574786Z",
     "iopub.status.idle": "2026-01-04T15:24:24.081214Z",
     "shell.execute_reply": "2026-01-04T15:24:24.079911Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "project_root = Path(analysis_metadata.get(\"project_root\", config.project_root))\n",
    "fig_dir_raw = analysis_metadata.get(\"fig_dir\", config.fig_dir)\n",
    "fig_dir = Path(fig_dir_raw)\n",
    "if not fig_dir.is_absolute():\n",
    "    fig_dir = project_root / fig_dir\n",
    "fig_dir = fig_dir.resolve()\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exported = []\n",
    "if not FIGURES:\n",
    "    print(\"No figures registered for export.\")\n",
    "else:\n",
    "    for key, fig in FIGURES.items():\n",
    "        if fig is None:\n",
    "            continue\n",
    "        out_path = fig_dir / f\"{key}.png\"\n",
    "        fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "        exported.append(out_path)\n",
    "\n",
    "if EXTRA_EXPORT_PATHS:\n",
    "    for src_path, out_name in EXTRA_EXPORT_PATHS:\n",
    "        src_path = Path(src_path)\n",
    "        if not src_path.exists():\n",
    "            continue\n",
    "        out_path = fig_dir / out_name\n",
    "        shutil.copy2(src_path, out_path)\n",
    "        exported.append(out_path)\n",
    "\n",
    "if not exported:\n",
    "    print(\"No figures were exported.\")\n",
    "else:\n",
    "    print(f\"Exported {len(exported)} figures to {fig_dir}\")\n",
    "    for path in exported:\n",
    "        print(f\"- {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d89ceb",
   "metadata": {},
   "source": [
    "## Comparison: 100 vs 1000 Gene Runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0dc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Load data for 100 gene runs (both datasets)\n",
    "embryonic_100 = pd.read_csv('analysis/reports/embryonic_100genes/summary_metrics_all_models.csv')\n",
    "embryonic_100['dataset'] = 'Embryonic'\n",
    "embryonic_100['n_genes'] = '100'\n",
    "\n",
    "endothelial_100 = pd.read_csv('analysis/reports/endothelial_100genes/summary_metrics_all_models.csv')\n",
    "endothelial_100['dataset'] = 'Endothelial'\n",
    "endothelial_100['n_genes'] = '100'\n",
    "\n",
    "# Load data for 1000 gene runs - need to find the individual run metrics\n",
    "# Check for individual run directories\n",
    "results_base = 'output/results/spear_results'\n",
    "\n",
    "# Function to load metrics from individual runs\n",
    "def load_1000gene_runs(dataset_name):\n",
    "    \"\"\"Load all 1000 gene runs for a specific dataset\"\"\"\n",
    "    pattern = f\"{results_base}/spear_1000genes_{dataset_name.lower()}_*\"\n",
    "    run_dirs = glob.glob(pattern)\n",
    "    \n",
    "    all_metrics = []\n",
    "    for run_dir in run_dirs:\n",
    "        models_dir = f\"{run_dir}/models\"\n",
    "        if not os.path.exists(models_dir):\n",
    "            continue\n",
    "        \n",
    "        # Look for model subdirectories\n",
    "        for model_dir in glob.glob(f\"{models_dir}/*\"):\n",
    "            if not os.path.isdir(model_dir):\n",
    "                continue\n",
    "            \n",
    "            metrics_file = f\"{model_dir}/metrics_per_gene.csv\"\n",
    "            if os.path.exists(metrics_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(metrics_file)\n",
    "                    model_id = os.path.basename(model_dir)\n",
    "                    run_name = os.path.basename(run_dir)\n",
    "                    \n",
    "                    # Calculate summary statistics\n",
    "                    for split in ['test', 'val', 'train']:\n",
    "                        split_data = df[df['split'] == split]\n",
    "                        if len(split_data) > 0:\n",
    "                            all_metrics.append({\n",
    "                                'model_id': model_id,\n",
    "                                'run_name': run_name,\n",
    "                                'split': split,\n",
    "                                'pearson_mean': split_data['pearson'].mean(),\n",
    "                                'pearson_std': split_data['pearson'].std()\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {metrics_file}: {e}\")\n",
    "    \n",
    "    if not all_metrics:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot to get one row per model\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    test_data = df[df['split'] == 'test'].copy()\n",
    "    if test_data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    test_data['model'] = test_data['model_id']\n",
    "    test_data['pearson'] = test_data['pearson_mean']\n",
    "    return test_data[['model', 'pearson', 'run_name']]\n",
    "\n",
    "# Load 1000-gene data for both datasets\n",
    "embryonic_1000 = load_1000gene_runs('Embryonic')\n",
    "if not embryonic_1000.empty:\n",
    "    embryonic_1000['dataset'] = 'Embryonic'\n",
    "    embryonic_1000['n_genes'] = '1000'\n",
    "\n",
    "endothelial_1000 = load_1000gene_runs('Endothelial')\n",
    "if not endothelial_1000.empty:\n",
    "    endothelial_1000['dataset'] = 'Endothelial'\n",
    "    endothelial_1000['n_genes'] = '1000'\n",
    "\n",
    "# Combine all datasets\n",
    "all_data = []\n",
    "for df in [embryonic_100, endothelial_100, embryonic_1000, endothelial_1000]:\n",
    "    if not df.empty:\n",
    "        all_data.append(df)\n",
    "\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined_df)} total records\")\n",
    "    print(f\"Datasets: {combined_df['dataset'].unique()}\")\n",
    "    print(f\"Gene counts: {combined_df['n_genes'].unique()}\")\n",
    "else:\n",
    "    print(\"No data loaded\")\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison figure: 100 vs 1000 genes\n",
    "# Group by model and calculate differences\n",
    "\n",
    "# First, let's create a pivot to compare\n",
    "comparison_data = []\n",
    "\n",
    "for model in df_combined['model_display'].unique():\n",
    "    model_data = df_combined[df_combined['model_display'] == model]\n",
    "    \n",
    "    for dataset in ['Embryonic', 'Endothelial']:\n",
    "        data_100 = model_data[(model_data['dataset'] == dataset) & (model_data['n_genes'] == '100')]\n",
    "        data_1000 = model_data[(model_data['dataset'] == dataset) & (model_data['n_genes'] == '1000')]\n",
    "        \n",
    "        if len(data_100) > 0:\n",
    "            pearson_100 = data_100['test_pearson_mean'].values[0]\n",
    "        else:\n",
    "            pearson_100 = None\n",
    "            \n",
    "        if len(data_1000) > 0:\n",
    "            pearson_1000 = data_1000['test_pearson_mean'].values[0]\n",
    "        else:\n",
    "            pearson_1000 = None\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'model': model,\n",
    "            'dataset': dataset,\n",
    "            'pearson_100': pearson_100,\n",
    "            'pearson_1000': pearson_1000,\n",
    "            'difference': pearson_1000 - pearson_100 if (pearson_100 is not None and pearson_1000 is not None) else None\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"Comparison of Test Pearson by Model and Dataset (100 vs 1000 genes):\")\n",
    "print(\"=\"*80)\n",
    "df_comparison_display = df_comparison.copy()\n",
    "df_comparison_display = df_comparison_display.sort_values(['dataset', 'pearson_1000'], ascending=[True, False])\n",
    "print(df_comparison_display.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"-\"*80)\n",
    "for dataset in ['Embryonic', 'Endothelial']:\n",
    "    dataset_data = df_comparison[df_comparison['dataset'] == dataset]\n",
    "    valid_diffs = dataset_data['difference'].dropna()\n",
    "    if len(valid_diffs) > 0:\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"  Mean difference (1000 - 100): {valid_diffs.mean():.4f}\")\n",
    "        print(f\"  Median difference: {valid_diffs.median():.4f}\")\n",
    "        print(f\"  Models with improvement: {(valid_diffs > 0).sum()}/{len(valid_diffs)}\")\n",
    "        print(f\"  Models with decline: {(valid_diffs < 0).sum()}/{len(valid_diffs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09620b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the comparison figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Filter to only models that have both 100 and 1000 gene data for each dataset\n",
    "embryonic_comp = df_comparison[(df_comparison['dataset'] == 'Embryonic') & \n",
    "                                (df_comparison['pearson_100'].notna()) & \n",
    "                                (df_comparison['pearson_1000'].notna())]\n",
    "\n",
    "endothelial_comp = df_comparison[(df_comparison['dataset'] == 'Endothelial') & \n",
    "                                  (df_comparison['pearson_100'].notna()) &\n",
    "                                  (df_comparison['pearson_1000'].notna())]\n",
    "\n",
    "# Plot 1: Embryonic dataset comparison\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(embryonic_comp))\n",
    "width = 0.35\n",
    "\n",
    "models_embryonic = embryonic_comp.sort_values('pearson_1000', ascending=False)['model'].values\n",
    "pearson_100_embryonic = embryonic_comp.sort_values('pearson_1000', ascending=False)['pearson_100'].values\n",
    "pearson_1000_embryonic = embryonic_comp.sort_values('pearson_1000', ascending=False)['pearson_1000'].values\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, pearson_100_embryonic, width, label='100 genes', alpha=0.8, color='#3498db')\n",
    "bars2 = ax.bar(x_pos + width/2, pearson_1000_embryonic, width, label='1000 genes', alpha=0.8, color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mean Test Pearson Correlation', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Embryonic Dataset: 100 vs 1000 Genes', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_embryonic, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, max(pearson_1000_embryonic.max(), pearson_100_embryonic.max()) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    height1 = bar1.get_height()\n",
    "    height2 = bar2.get_height()\n",
    "    ax.text(bar1.get_x() + bar1.get_width()/2., height1,\n",
    "            f'{height1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    ax.text(bar2.get_x() + bar2.get_width()/2., height2,\n",
    "            f'{height2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 2: Endothelial dataset comparison\n",
    "ax = axes[1]\n",
    "\n",
    "if len(endothelial_comp) > 0:\n",
    "    x_pos_endo = np.arange(len(endothelial_comp))\n",
    "    \n",
    "    models_endothelial = endothelial_comp.sort_values('pearson_1000', ascending=False)['model'].values\n",
    "    pearson_100_endothelial = endothelial_comp.sort_values('pearson_1000', ascending=False)['pearson_100'].values\n",
    "    pearson_1000_endothelial = endothelial_comp.sort_values('pearson_1000', ascending=False)['pearson_1000'].values\n",
    "    \n",
    "    bars3 = ax.bar(x_pos_endo - width/2, pearson_100_endothelial, width, label='100 genes', alpha=0.8, color='#3498db')\n",
    "    bars4 = ax.bar(x_pos_endo + width/2, pearson_1000_endothelial, width, label='1000 genes', alpha=0.8, color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Test Pearson Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Endothelial Dataset: 100 vs 1000 Genes', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x_pos_endo)\n",
    "    ax.set_xticklabels(models_endothelial, rotation=45, ha='right', fontsize=10)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_ylim(0, max(pearson_1000_endothelial.max(), pearson_100_endothelial.max()) * 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar3, bar4) in enumerate(zip(bars3, bars4)):\n",
    "        height3 = bar3.get_height()\n",
    "        height4 = bar4.get_height()\n",
    "        ax.text(bar3.get_x() + bar3.get_width()/2., height3,\n",
    "                f'{height3:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        ax.text(bar4.get_x() + bar4.get_width()/2., height4,\n",
    "                f'{height4:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No endothelial 1000-gene data available', \n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Endothelial Dataset: 100 vs 1000 Genes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store in FIGURES dict if it exists\n",
    "if 'FIGURES' in globals():\n",
    "    FIGURES['comparison_100_vs_1000_genes'] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea559629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a difference plot showing improvement/decline from 100 to 1000 genes for both datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, dataset in enumerate(['Embryonic', 'Endothelial']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get comparison data with both 100 and 1000 gene runs\n",
    "    dataset_comp = df_comparison[(df_comparison['dataset'] == dataset) & \n",
    "                                  (df_comparison['pearson_100'].notna()) & \n",
    "                                  (df_comparison['pearson_1000'].notna())]\n",
    "    \n",
    "    if len(dataset_comp) == 0:\n",
    "        ax.text(0.5, 0.5, f'No {dataset} data with both 100 and 1000 genes', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{dataset} Dataset', fontsize=14, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    dataset_comp_sorted = dataset_comp.sort_values('difference', ascending=True)\n",
    "    \n",
    "    models = dataset_comp_sorted['model'].values\n",
    "    differences = dataset_comp_sorted['difference'].values\n",
    "    \n",
    "    # Create color map: red for negative (worse), green for positive (better)\n",
    "    colors = ['#27ae60' if d > 0 else '#e74c3c' for d in differences]\n",
    "    \n",
    "    y_pos = np.arange(len(models))\n",
    "    bars = ax.barh(y_pos, differences, color=colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(models, fontsize=10)\n",
    "    ax.set_xlabel('Difference in Mean Test Pearson (1000 - 100 genes)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{dataset} Dataset: Performance Change', fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, diff) in enumerate(zip(bars, differences)):\n",
    "        width = bar.get_width()\n",
    "        label_x = width + (0.005 if width > 0 else -0.005)\n",
    "        ha = 'left' if width > 0 else 'right'\n",
    "        ax.text(label_x, bar.get_y() + bar.get_height()/2, \n",
    "                f'{diff:+.3f}', ha=ha, va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Add legend only on the first plot\n",
    "    if idx == 0:\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='#27ae60', alpha=0.8, label='Improvement (1000 > 100)'),\n",
    "            Patch(facecolor='#e74c3c', alpha=0.8, label='Decline (1000 < 100)')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store in FIGURES dict if it exists\n",
    "if 'FIGURES' in globals():\n",
    "    FIGURES['difference_100_vs_1000_genes'] = fig\n",
    "    \n",
    "# Print summary for both datasets\n",
    "print(\"\\nKey Findings by Dataset:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset in ['Embryonic', 'Endothelial']:\n",
    "    dataset_comp = df_comparison[(df_comparison['dataset'] == dataset) & \n",
    "                                  (df_comparison['pearson_100'].notna()) & \n",
    "                                  (df_comparison['pearson_1000'].notna())]\n",
    "    \n",
    "    if len(dataset_comp) == 0:\n",
    "        print(f\"\\n{dataset}: No data with both 100 and 1000 genes\")\n",
    "        continue\n",
    "    \n",
    "    differences = dataset_comp['difference'].values\n",
    "    models = dataset_comp['model'].values\n",
    "    \n",
    "    improved = (differences > 0).sum()\n",
    "    declined = (differences < 0).sum()\n",
    "    \n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(f\"  Models that improved with 1000 genes: {improved}/{len(differences)}\")\n",
    "    print(f\"  Models that declined with 1000 genes: {declined}/{len(differences)}\")\n",
    "    print(f\"  Average change: {differences.mean():+.4f}\")\n",
    "    print(f\"  Median change: {np.median(differences):+.4f}\")\n",
    "    print(f\"  Best improvement: {differences.max():+.4f} ({models[differences.argmax()]})\")\n",
    "    print(f\"  Worst decline: {differences.min():+.4f} ({models[differences.argmin()]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grn_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
