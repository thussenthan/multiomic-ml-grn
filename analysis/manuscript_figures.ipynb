{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca00951b",
   "metadata": {},
   "source": [
    "# SPEAR Manuscript Figures\n",
    "\n",
    "Single-cell Prediction of gene Expression from ATAC-seq Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad505af",
   "metadata": {},
   "source": [
    "## Prereqs\n",
    "\n",
    "- Place run outputs under `output/results/spear_results/` (one subfolder per run with `models/` and metrics CSVs).\n",
    "\n",
    "- Ensure logs (if used) are under `output/logs/` with `spear_*` naming.\n",
    "\n",
    "- Keep `analysis/model_name_lookup.tsv` present (tracked in repo).\n",
    "\n",
    "- Use the `spear_env` kernel with required deps installed (see README).\n",
    "\n",
    "### How to run\n",
    "\n",
    "1. Open this notebook from the repo root.\n",
    "\n",
    "2. Adjust run include globs or paths if you want to target specific runs; otherwise leave defaults.\n",
    "\n",
    "3. Run all cells top-to-bottom after outputs are in place to regenerate figures/CSVs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec47a0c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d64e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:45.184668Z",
     "iopub.status.busy": "2026-01-18T00:48:45.184266Z",
     "iopub.status.idle": "2026-01-18T00:48:53.731205Z",
     "shell.execute_reply": "2026-01-18T00:48:53.730475Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {'analysis', 'scripts'}:\n",
    "    project_root = project_root.parent\n",
    "src_root = project_root / 'src'\n",
    "for candidate in (src_root, project_root):\n",
    "    if str(candidate) not in sys.path:\n",
    "        sys.path.insert(0, str(candidate))\n",
    "\n",
    "from typing import Iterable, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "# Configure plotting defaults for consistent styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "sns.set_context(\"paper\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.dpi\": 160, \"savefig.dpi\": 320})\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.display.width = 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aa874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.735897Z",
     "iopub.status.busy": "2026-01-18T00:48:53.735115Z",
     "iopub.status.idle": "2026-01-18T00:48:53.745313Z",
     "shell.execute_reply": "2026-01-18T00:48:53.744346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick toggles (edit these only)\n",
    "# - TARGET_DATASET: \"embryonic\", \"endothelial\", or \"both\" (default keeps both)\n",
    "# - USE_1000PLUS100: include 1000-gene runs with 100-gene fallback\n",
    "# - PREFER_1000_FALLBACK_100: keep most recent 1000-gene per model, fallback to 100-gene only if missing\n",
    "GENE_COUNT = 1000  # Change to 100 for 100-gene analysis\n",
    "TARGET_DATASET = \"both\"  # \"embryonic\", \"endothelial\", or \"both\"\n",
    "USE_1000PLUS100 = True\n",
    "PREFER_1000_FALLBACK_100 = True\n",
    "\n",
    "# Build run filters from the toggles above.\n",
    "if USE_1000PLUS100:\n",
    "    gene_tokens = [\"1000genes\", \"100genes\"]\n",
    "    gene_label = \"1000plus100\"\n",
    "    gene_desc = \"1000 genes (+ 100 genes backup)\"\n",
    "else:\n",
    "    gene_tokens = [f\"{GENE_COUNT}genes\"]\n",
    "    gene_label = f\"{GENE_COUNT}genes\"\n",
    "    gene_desc = f\"{GENE_COUNT} genes\"\n",
    "\n",
    "if TARGET_DATASET in (\"embryonic\", \"endothelial\"):\n",
    "    RUN_GLOB_SELECTION = [f\"*{TARGET_DATASET}*{token}*\" for token in gene_tokens]\n",
    "    RUN_SUBSET_LABEL = f\"{TARGET_DATASET}_{gene_label}\"\n",
    "    RUN_SUBSET_DESCRIPTION = f\"{TARGET_DATASET.title()} | {gene_desc}\"\n",
    "else:\n",
    "    RUN_GLOB_SELECTION = [f\"*{token}*\" for token in gene_tokens]\n",
    "    RUN_SUBSET_LABEL = f\"all_{gene_label}\"\n",
    "    RUN_SUBSET_DESCRIPTION = f\"All datasets | {gene_desc}\"\n",
    "\n",
    "REPORTS_SUFFIX = RUN_SUBSET_LABEL or f\"{GENE_COUNT}genes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979660d8",
   "metadata": {},
   "source": [
    "## 2. Analysis Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe1fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.748747Z",
     "iopub.status.busy": "2026-01-18T00:48:53.748378Z",
     "iopub.status.idle": "2026-01-18T00:48:53.764722Z",
     "shell.execute_reply": "2026-01-18T00:48:53.763577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Centralised configuration for the notebook run\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    project_root: Path\n",
    "    results_root: Path\n",
    "    lookup_path: Path\n",
    "    fig_dir: Path\n",
    "    reports_dir: Path\n",
    "    run_include_globs: tuple[str, ...] = (\"*\",)\n",
    "    run_exclude: tuple[str, ...] = tuple()\n",
    "    primary_split: str = \"test\"\n",
    "    val_split: str = \"val\"\n",
    "    train_split: str = \"train\"\n",
    "    top_gene_count: int = 15\n",
    "    top_model_count: int = 3\n",
    "    random_seed: int = 7\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # Create required directories if they are missing.\n",
    "        self.fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.lookup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {\"analysis\", \"scripts\"}:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "config = AnalysisConfig(\n",
    "    project_root=project_root,\n",
    "    results_root=project_root / \"output\" / \"results\" / \"spear_results\",\n",
    "    lookup_path=project_root / \"analysis\" / \"model_name_lookup.tsv\",\n",
    "    fig_dir=project_root / \"analysis\" / \"figs\" / \"manuscript_figures\",\n",
    "    reports_dir=project_root / \"analysis\" / \"reports\" / f\"manuscript_{REPORTS_SUFFIX}\",\n",
    "    run_include_globs=tuple(RUN_GLOB_SELECTION),\n",
    "    run_exclude=tuple(),\n",
    "    random_seed=7,\n",
    "    top_gene_count=15,\n",
    "    top_model_count=3,\n",
    ")\n",
    "\n",
    "if not config.results_root.exists():\n",
    "    raise FileNotFoundError(f\"Results directory missing: {config.results_root}\")\n",
    "if not config.lookup_path.exists():\n",
    "    # Seed the lookup table if it is missing so later steps can append to it.\n",
    "    pd.DataFrame({\n",
    "        \"model_id\": [],\n",
    "        \"model_display_name\": [],\n",
    "        \"model_short_name\": [],\n",
    "    }).to_csv(\n",
    "        config.lookup_path, sep=\"\t\", index=False\n",
    "    )\n",
    "\n",
    "np.random.seed(config.random_seed)\n",
    "rng = np.random.default_rng(config.random_seed)\n",
    "FIGURES: dict[str, plt.Figure] = {}\n",
    "TABLES: dict[str, pd.DataFrame] = {}\n",
    "EXTRA_EXPORT_PATHS: list[tuple[Path, str]] = []\n",
    "analysis_metadata: dict[str, object] = {\n",
    "    \"generated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"project_root\": config.project_root,\n",
    "    \"results_root\": config.results_root,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9dc245",
   "metadata": {},
   "source": [
    "## 3. Run Discovery Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ea3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.768232Z",
     "iopub.status.busy": "2026-01-18T00:48:53.767807Z",
     "iopub.status.idle": "2026-01-18T00:48:53.782262Z",
     "shell.execute_reply": "2026-01-18T00:48:53.781283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Canonical representation of a single trained model output folder\n",
    "@dataclass(frozen=True)\n",
    "class RunRecord:\n",
    "    run_name: str\n",
    "    model_id: str\n",
    "    run_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: Optional[Path]\n",
    "    predictions_path: Optional[Path]\n",
    "    training_history_path: Optional[Path]\n",
    "    model_display: Optional[str] = None\n",
    "\n",
    "\n",
    "LOOKUP_SPECIAL_CASES = {\n",
    "    \"cnn\": \"Convolutional Neural Network\",\n",
    "    \"rnn\": \"Recurrent Neural Network\",\n",
    "    \"lstm\": \"Long Short-Term Memory\",\n",
    "    \"mlp\": \"Multilayer Perceptron\",\n",
    "    \"svr\": \"Support Vector Regressor\",\n",
    "    \"ols\": \"Ordinary Least Squares\",\n",
    "    \"xgboost\": \"XGBoost\",\n",
    "    \"catboost\": \"CatBoost\",\n",
    "    \"hist_gradient_boosting\": \"Histogram Gradient Boosting\",\n",
    "    \"extra_trees\": \"Extra Trees\",\n",
    "    \"random_forest\": \"Random Forest\",\n",
    "    \"elastic_net\": \"Elastic Net\",\n",
    "}\n",
    "\n",
    "SHORT_NAME_FALLBACKS = {\n",
    "    \"Multilayer Perceptron\": \"MLP\",\n",
    "    \"Graph Neural Network\": \"GNN\",\n",
    "    \"Convolutional Neural Network\": \"CNN\",\n",
    "    \"Long Short-Term Memory Network\": \"LSTM\",\n",
    "    \"Recurrent Neural Network\": \"RNN\",\n",
    "    \"Transformer Encoder\": \"Transformer\",\n",
    "    \"Ordinary Least Squares\": \"OLS\",\n",
    "    \"Extra Trees\": \"Extra Trees\",\n",
    "    \"Random Forest\": \"Random Forest\",\n",
    "    \"Ridge Regression\": \"Ridge\",\n",
    "}\n",
    "\n",
    "MODEL_ID_TO_DISPLAY: dict[str, str] = {}\n",
    "MODEL_ID_TO_SHORT: dict[str, str] = {}\n",
    "MODEL_DISPLAY_TO_SHORT: dict[str, str] = {}\n",
    "\n",
    "\n",
    "def _default_short_name(display_name: str) -> str:\n",
    "    # Generate a lightweight abbreviation when none is provided.\n",
    "    if not isinstance(display_name, str) or not display_name.strip():\n",
    "        return \"\"\n",
    "    tokens = [token for token in display_name.replace(\"(\", \" \").replace(\")\", \" \").split() if token]\n",
    "    if not tokens:\n",
    "        return display_name\n",
    "    acronym = \"\".join(token[0] for token in tokens if token and token[0].isalnum()).upper()\n",
    "    if 1 < len(acronym) <= 5:\n",
    "        return acronym\n",
    "    return display_name\n",
    "\n",
    "\n",
    "def _matches_any(value: str, patterns: Iterable[str]) -> bool:\n",
    "    \"\"\"Check if value matches any of the fnmatch patterns.\"\"\"\n",
    "    return any(fnmatch(value, pattern) for pattern in patterns)\n",
    "\n",
    "\n",
    "def _first_existing(base_dir: Path, candidates: Sequence[str]) -> Optional[Path]:\n",
    "    \"\"\"Return the first candidate path that exists, or None.\"\"\"\n",
    "    for candidate in candidates:\n",
    "        path = base_dir / candidate\n",
    "        if path.exists():\n",
    "            return path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e69628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.784931Z",
     "iopub.status.busy": "2026-01-18T00:48:53.784765Z",
     "iopub.status.idle": "2026-01-18T00:48:53.791334Z",
     "shell.execute_reply": "2026-01-18T00:48:53.790427Z"
    }
   },
   "outputs": [],
   "source": [
    "def discover_model_runs(\n",
    "    results_root: Path,\n",
    "    include_globs: Iterable[str],\n",
    "    exclude_patterns: Iterable[str],\n",
    ") -> list[RunRecord]:\n",
    "    results_root = Path(results_root)\n",
    "    if not results_root.exists():\n",
    "        raise FileNotFoundError(f\"Results root missing: {results_root}\")\n",
    "\n",
    "    records_by_key: dict[tuple[str, str], RunRecord] = {}\n",
    "    include = tuple(include_globs) if include_globs else (\"*\",)\n",
    "    exclude = tuple(exclude_patterns) if exclude_patterns else tuple()\n",
    "\n",
    "    for models_dir in sorted(results_root.rglob(\"models\")):\n",
    "        if not models_dir.is_dir():\n",
    "            continue\n",
    "        run_dir = models_dir.parent\n",
    "        run_name = run_dir.name\n",
    "        if not _matches_any(run_name, include):\n",
    "            continue\n",
    "        if exclude and _matches_any(run_name, exclude):\n",
    "            continue\n",
    "\n",
    "        for model_dir in sorted(models_dir.iterdir()):\n",
    "            if not model_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            model_id = model_dir.name\n",
    "            metrics_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"metrics_per_gene.csv\",\n",
    "                    \"metrics_by_gene.csv\",\n",
    "                    \"metrics_cv.csv\",\n",
    "                ),\n",
    "            )\n",
    "            predictions_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"predictions_raw.csv\",\n",
    "                    \"predictions.csv\",\n",
    "                ),\n",
    "            )\n",
    "            history_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"training_history.csv\",\n",
    "                    \"training_history_loss.csv\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            rec = RunRecord(\n",
    "                run_name=run_name,\n",
    "                model_id=model_id,\n",
    "                run_path=run_dir,\n",
    "                model_path=model_dir,\n",
    "                metrics_path=metrics_path,\n",
    "                predictions_path=predictions_path,\n",
    "                training_history_path=history_path,\n",
    "            )\n",
    "\n",
    "            key = (run_name, model_id)\n",
    "            existing = records_by_key.get(key)\n",
    "            if existing is None:\n",
    "                records_by_key[key] = rec\n",
    "                continue\n",
    "\n",
    "            existing_has_metrics = existing.metrics_path is not None and Path(existing.metrics_path).exists()\n",
    "            current_has_metrics = metrics_path is not None and Path(metrics_path).exists()\n",
    "\n",
    "            if existing_has_metrics and not current_has_metrics:\n",
    "                # Keep the version that already has metrics\n",
    "                continue\n",
    "            if current_has_metrics and not existing_has_metrics:\n",
    "                # Prefer the version with metrics when the existing one is missing\n",
    "                records_by_key[key] = rec\n",
    "                continue\n",
    "            # If both have or both lack metrics, keep the first-seen (sorted iteration is stable)\n",
    "\n",
    "    return list(records_by_key.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9be831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.794316Z",
     "iopub.status.busy": "2026-01-18T00:48:53.794157Z",
     "iopub.status.idle": "2026-01-18T00:48:53.803291Z",
     "shell.execute_reply": "2026-01-18T00:48:53.802415Z"
    }
   },
   "outputs": [],
   "source": [
    "def _read_lookup_table(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame({\n",
    "            \"model_id\": pd.Series(dtype=\"string\"),\n",
    "            \"model_display_name\": pd.Series(dtype=\"string\"),\n",
    "            \"model_short_name\": pd.Series(dtype=\"string\"),\n",
    "        })\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    expected = {\"model_id\", \"model_display_name\"}\n",
    "    missing_cols = expected.difference(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Lookup table missing required columns: {sorted(missing_cols)}\")\n",
    "    if \"model_short_name\" not in df.columns:\n",
    "        df[\"model_short_name\"] = df[\"model_display_name\"].map(_default_short_name)\n",
    "    else:\n",
    "        df[\"model_short_name\"] = df[\"model_short_name\"].fillna(\"\")\n",
    "        missing_short = df[\"model_short_name\"].str.strip() == \"\"\n",
    "        df.loc[missing_short, \"model_short_name\"] = df.loc[missing_short, \"model_display_name\"].map(_default_short_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _update_model_name_maps(df: pd.DataFrame) -> None:\n",
    "    # Cache name lookups for downstream plotting helpers.\n",
    "    global MODEL_ID_TO_DISPLAY, MODEL_ID_TO_SHORT, MODEL_DISPLAY_TO_SHORT\n",
    "    if df.empty:\n",
    "        MODEL_ID_TO_DISPLAY = {}\n",
    "        MODEL_ID_TO_SHORT = {}\n",
    "        MODEL_DISPLAY_TO_SHORT = {}\n",
    "        return\n",
    "    standardised = df.fillna(\"\")\n",
    "    MODEL_ID_TO_DISPLAY = {\n",
    "        row.model_id: row.model_display_name or _guess_display_name(row.model_id)\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_ID_TO_SHORT = {\n",
    "        row.model_id: (row.model_short_name or MODEL_ID_TO_DISPLAY[row.model_id])\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_DISPLAY_TO_SHORT = {\n",
    "        MODEL_ID_TO_DISPLAY[row.model_id]: MODEL_ID_TO_SHORT[row.model_id]\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "\n",
    "def _guess_display_name(model_id: str) -> str:\n",
    "    if model_id in LOOKUP_SPECIAL_CASES:\n",
    "        return LOOKUP_SPECIAL_CASES[model_id]\n",
    "    parts = [part for part in model_id.replace(\"-\", \" \").replace(\"_\", \" \").split(\" \") if part]\n",
    "    if not parts:\n",
    "        return model_id\n",
    "    formatted = []\n",
    "    for token in parts:\n",
    "        if len(token) <= 3:\n",
    "            formatted.append(token.upper())\n",
    "        else:\n",
    "            formatted.append(token.capitalize())\n",
    "    return \" \".join(formatted)\n",
    "\n",
    "\n",
    "def ensure_model_lookup(path: Path, model_ids: Iterable[str]) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    df = _read_lookup_table(path)\n",
    "    existing = set(df[\"model_id\"]) if not df.empty else set()\n",
    "    new_rows = []\n",
    "    for model_id in sorted(set(model_ids).difference(existing)):\n",
    "        display_name = _guess_display_name(model_id)\n",
    "        short_name = SHORT_NAME_FALLBACKS.get(display_name, _default_short_name(display_name))\n",
    "        new_rows.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"model_display_name\": display_name,\n",
    "                \"model_short_name\": short_name,\n",
    "            }\n",
    "        )\n",
    "    if new_rows:\n",
    "        additions = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, additions], ignore_index=True) if not df.empty else additions\n",
    "        df.sort_values(\"model_id\", inplace=True)\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def attach_lookup(records: Sequence[RunRecord], model_lookup: pd.DataFrame) -> list[RunRecord]:\n",
    "    if model_lookup.empty:\n",
    "        return list(records)\n",
    "    display_map = dict(zip(model_lookup[\"model_id\"], model_lookup[\"model_display_name\"]))\n",
    "    resolved: list[RunRecord] = []\n",
    "    for record in records:\n",
    "        display = display_map.get(record.model_id, _guess_display_name(record.model_id))\n",
    "        resolved.append(replace(record, model_display=display))\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3292f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.806239Z",
     "iopub.status.busy": "2026-01-18T00:48:53.806081Z",
     "iopub.status.idle": "2026-01-18T00:48:53.810913Z",
     "shell.execute_reply": "2026-01-18T00:48:53.810007Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_records_to_frame(records: Sequence[RunRecord]) -> pd.DataFrame:\n",
    "    if not records:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"run_name\",\n",
    "                \"model_id\",\n",
    "                \"model_display\",\n",
    "                \"run_path\",\n",
    "                \"model_path\",\n",
    "                \"metrics_path\",\n",
    "                \"predictions_path\",\n",
    "                \"training_history_path\",\n",
    "            ]\n",
    "        )\n",
    "    data = [\n",
    "        {\n",
    "            \"run_name\": r.run_name,\n",
    "            \"model_id\": r.model_id,\n",
    "            \"model_display\": r.model_display or _guess_display_name(r.model_id),\n",
    "            \"run_path\": r.run_path,\n",
    "            \"model_path\": r.model_path,\n",
    "            \"metrics_path\": r.metrics_path,\n",
    "            \"predictions_path\": r.predictions_path,\n",
    "            \"training_history_path\": r.training_history_path,\n",
    "        }\n",
    "        for r in records\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def to_relative_path(path_like: Optional[Path], root: Path) -> Optional[str]:\n",
    "    if path_like is None:\n",
    "        return None\n",
    "    path = Path(path_like)\n",
    "    try:\n",
    "        return str(path.resolve().relative_to(root))\n",
    "    except Exception:\n",
    "        return str(path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771b03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.813756Z",
     "iopub.status.busy": "2026-01-18T00:48:53.813603Z",
     "iopub.status.idle": "2026-01-18T00:48:53.821343Z",
     "shell.execute_reply": "2026-01-18T00:48:53.820451Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_metrics(records: Sequence[RunRecord]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames: list[pd.DataFrame] = []\n",
    "    for record in records:\n",
    "        metrics_path = record.metrics_path\n",
    "        if metrics_path is None or not metrics_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to load metrics from {metrics_path}: {exc}\")\n",
    "            continue\n",
    "        required_cols = {\"gene\", \"split\", \"pearson\"}\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            continue\n",
    "        df = df.copy()\n",
    "        df[\"run_name\"] = record.run_name\n",
    "        df[\"model_id\"] = record.model_id\n",
    "        df[\"model_display\"] = record.model_display or _guess_display_name(record.model_id)\n",
    "        long_frames.append(df)\n",
    "    if not long_frames:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    metrics_long = pd.concat(long_frames, ignore_index=True)\n",
    "    base_cols = [col for col in [\"run_name\", \"model_id\", \"model_display\", \"gene\", \"split\"] if col in metrics_long.columns]\n",
    "    metric_cols = [col for col in metrics_long.columns if col not in base_cols]\n",
    "    metrics_long = metrics_long[base_cols + metric_cols]\n",
    "\n",
    "    wide = metrics_long.pivot_table(\n",
    "        index=[\"run_name\", \"model_id\", \"model_display\", \"gene\"],\n",
    "        columns=\"split\",\n",
    "        values=\"pearson\",\n",
    "    )\n",
    "    wide.columns = [f\"{str(col).lower()}_pearson\" for col in wide.columns]\n",
    "    metrics_wide = wide.reset_index()\n",
    "\n",
    "    return metrics_long, metrics_wide\n",
    "\n",
    "\n",
    "def compute_model_summary(\n",
    "    metrics_wide: pd.DataFrame,\n",
    "    splits: Sequence[str] = (\"train\", \"val\", \"test\"),\n",
    ") -> pd.DataFrame:\n",
    "    if metrics_wide.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model_display\",\n",
    "                \"model_id\",\n",
    "                \"run_name\",\n",
    "                *[f\"{split}_pearson_mean\" for split in splits],\n",
    "                *[f\"{split}_pearson_std\" for split in splits],\n",
    "            ]\n",
    "        )\n",
    "    summaries = []\n",
    "    lower_splits = [split.lower() for split in splits]\n",
    "    for (run_name, model_id, model_display), group in metrics_wide.groupby([\"run_name\", \"model_id\", \"model_display\"], dropna=False):\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"model_display\": model_display,\n",
    "        }\n",
    "        for split, lower in zip(splits, lower_splits):\n",
    "            column = f\"{lower}_pearson\"\n",
    "            if column in group:\n",
    "                values = group[column].dropna()\n",
    "                if not values.empty:\n",
    "                    row[f\"{split}_pearson_mean\"] = values.mean()\n",
    "                    row[f\"{split}_pearson_std\"] = values.std(ddof=1) if len(values) > 1 else float(\"nan\")\n",
    "        summaries.append(row)\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    if \"test_pearson_mean\" in summary_df:\n",
    "        summary_df.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    summary_df.set_index([\"model_display\", \"model_id\", \"run_name\"], inplace=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f540ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.824129Z",
     "iopub.status.busy": "2026-01-18T00:48:53.823976Z",
     "iopub.status.idle": "2026-01-18T00:48:53.829764Z",
     "shell.execute_reply": "2026-01-18T00:48:53.828888Z"
    }
   },
   "outputs": [],
   "source": [
    "def maybe_store_table(store: dict[str, pd.DataFrame], key: str, table: pd.DataFrame) -> None:\n",
    "    if table is None or table.empty:\n",
    "        return\n",
    "    store[key] = table\n",
    "\n",
    "\n",
    "def register_figure(store: dict[str, object], key: str, fig: Optional[plt.Figure]) -> None:\n",
    "    # Track generated matplotlib figures for later export.\n",
    "    if fig is None:\n",
    "        store.pop(key, None)\n",
    "        return\n",
    "    store[key] = fig\n",
    "\n",
    "\n",
    "def compute_heatmap_limits(\n",
    "    values: pd.DataFrame | np.ndarray,\n",
    "    lower_percentile: float = 5.0,\n",
    "    upper_percentile: float = 95.0,\n",
    "    clip: tuple[float, float] = (0.0, 1.0),\n",
    "    min_buffer: float = 0.01,\n",
    ") -> tuple[float, float]:\n",
    "    # Derive consistent vmin/vmax bounds so heatmaps emphasise the dense value range.\n",
    "    data = np.asarray(values, dtype=float)\n",
    "    data = data[np.isfinite(data)]\n",
    "    if data.size == 0:\n",
    "        return clip\n",
    "    lower = np.percentile(data, lower_percentile)\n",
    "    upper = np.percentile(data, upper_percentile)\n",
    "    buffer = max(min_buffer, (upper - lower) * 0.05)\n",
    "    vmin = max(clip[0], lower - buffer)\n",
    "    vmax = min(clip[1], upper + buffer)\n",
    "    if vmin > vmax:\n",
    "        if clip[0] <= clip[1]:\n",
    "            vmin, vmax = clip\n",
    "        else:\n",
    "            vmin, vmax = float(data.min()), float(data.max())\n",
    "    if np.isclose(vmin, vmax):\n",
    "        spread = max(min_buffer, abs(vmin) * 0.1 or min_buffer)\n",
    "        vmin -= spread\n",
    "        vmax += spread\n",
    "        vmin = max(clip[0], vmin)\n",
    "        vmax = min(clip[1], vmax)\n",
    "        if vmin > vmax:\n",
    "            vmin, vmax = float(data.min()), float(data.max())\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "def to_short_name(name: str | None) -> str:\n",
    "    # Return a concise display name for figure titles and filenames.\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    if name in MODEL_DISPLAY_TO_SHORT:\n",
    "        return MODEL_DISPLAY_TO_SHORT[name]\n",
    "    return SHORT_NAME_FALLBACKS.get(name, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146fb895",
   "metadata": {},
   "source": [
    "## 4. Discover and Inspect Run Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c59869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:53.833388Z",
     "iopub.status.busy": "2026-01-18T00:48:53.833087Z",
     "iopub.status.idle": "2026-01-18T00:48:54.111073Z",
     "shell.execute_reply": "2026-01-18T00:48:54.109965Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_run_records = discover_model_runs(\n",
    "    config.results_root,\n",
    "    config.run_include_globs,\n",
    "    config.run_exclude,\n",
    ")\n",
    "model_lookup = ensure_model_lookup(\n",
    "    config.lookup_path,\n",
    "    [record.model_id for record in raw_run_records],\n",
    ")\n",
    "_update_model_name_maps(model_lookup)\n",
    "run_records = attach_lookup(raw_run_records, model_lookup)\n",
    "\n",
    "# Helper to identify dataset from a run name\n",
    "\n",
    "def _dataset_from_run_name(name: str) -> Optional[str]:\n",
    "    name = str(name).lower()\n",
    "    if \"embryonic\" in name:\n",
    "        return \"Embryonic\"\n",
    "    if \"endothelial\" in name:\n",
    "        return \"Endothelial\"\n",
    "    return None\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def _gene_count_from_run_name(name: str) -> Optional[int]:\n",
    "    match = re.search(r\"(\\d+)genes\", str(name).lower())\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return int(match.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _run_timestamp_key(name: str) -> tuple[int, int]:\n",
    "    match = re.search(r\"_(\\d{8})_(\\d{6})\", str(name))\n",
    "    if not match:\n",
    "        return (0, 0)\n",
    "    return (int(match.group(1)), int(match.group(2)))\n",
    "\n",
    "\n",
    "def _has_metrics(rec: RunRecord) -> bool:\n",
    "    path = rec.metrics_path\n",
    "    if not path:\n",
    "        return False\n",
    "    path = path if isinstance(path, Path) else Path(path)\n",
    "    return path.exists()\n",
    "\n",
    "def _model_dir_has_files(rec: RunRecord) -> bool:\n",
    "    path = rec.model_path\n",
    "    if not path:\n",
    "        return False\n",
    "    path = path if isinstance(path, Path) else Path(path)\n",
    "    try:\n",
    "        return path.exists() and any(path.iterdir())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _is_nonempty_record(rec: RunRecord) -> bool:\n",
    "    return _has_metrics(rec) or _model_dir_has_files(rec)\n",
    "\n",
    "\n",
    "\n",
    "def _select_latest_runs(records: list[RunRecord]) -> list[RunRecord]:\n",
    "    if not records:\n",
    "        return []\n",
    "    grouped: dict[tuple[str, int, str], list[RunRecord]] = {}\n",
    "    passthrough: list[RunRecord] = []\n",
    "    for rec in records:\n",
    "        dataset = _dataset_from_run_name(rec.run_name)\n",
    "        gene_count = _gene_count_from_run_name(rec.run_name)\n",
    "        if not dataset or gene_count is None:\n",
    "            passthrough.append(rec)\n",
    "            continue\n",
    "        key = (dataset, gene_count, rec.model_id)\n",
    "        grouped.setdefault(key, []).append(rec)\n",
    "\n",
    "    selected: list[RunRecord] = []\n",
    "    for group in grouped.values():\n",
    "        with_metrics = [rec for rec in group if _has_metrics(rec)]\n",
    "        if not with_metrics:\n",
    "            continue\n",
    "        with_metrics.sort(key=lambda rec: _run_timestamp_key(rec.run_name), reverse=True)\n",
    "        selected.append(with_metrics[0])\n",
    "\n",
    "    return selected + passthrough\n",
    "\n",
    "\n",
    "run_records = _select_latest_runs(list(run_records))\n",
    "\n",
    "# Fallback: if analyzing 1000 genes and a specific (run, model) combo has missing metrics,\n",
    "# try the 100-gene variant from archive. Only use fallback when needed.\n",
    "FALLBACK_USED: set[tuple[str, str]] = set()\n",
    "FALLBACK_DISPLAY_RENAME: dict[tuple[str, str], str] = {}\n",
    "FALLBACK_COMBOS: set[tuple[str, str]] = set()  # (Dataset, model_id)\n",
    "IGNORED_MISSING: set[tuple[str, str]] = set()\n",
    "if GENE_COUNT == 1000:\n",
    "    archive_root = config.results_root / \"archive\"\n",
    "    if archive_root.exists():\n",
    "        successful_1000: set[tuple[str, str]] = set()\n",
    "        for record in raw_run_records:\n",
    "            if \"1000genes\" not in record.run_name:\n",
    "                continue\n",
    "            metrics_path = record.metrics_path\n",
    "            if metrics_path is None or not Path(metrics_path).exists():\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(record.run_name)\n",
    "            if ds:\n",
    "                successful_1000.add((ds, record.model_id))\n",
    "\n",
    "        for run_dir in archive_root.iterdir():\n",
    "            if not run_dir.is_dir() or \"1000genes\" not in run_dir.name:\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(run_dir.name)\n",
    "            if not ds:\n",
    "                continue\n",
    "            models_dir = run_dir / \"models\"\n",
    "            if not models_dir.exists():\n",
    "                continue\n",
    "            for model_dir in models_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                metrics_path = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if metrics_path and metrics_path.exists():\n",
    "                    successful_1000.add((ds, model_dir.name))\n",
    "\n",
    "        updated_records: list[RunRecord | None] = list(run_records)\n",
    "\n",
    "        # Only replace missing 1000-gene records with 100-gene variants\n",
    "        for idx, rec in enumerate(list(run_records)):\n",
    "            metrics_path = rec.metrics_path\n",
    "            metrics_missing = metrics_path is None or not Path(metrics_path).exists()\n",
    "            if not metrics_missing:\n",
    "                continue\n",
    "\n",
    "            dataset_label = _dataset_from_run_name(rec.run_name)\n",
    "\n",
    "            # Prefer another 1000-gene run in archive for the same dataset/model.\n",
    "            alt_model_dir = None\n",
    "            alt_metrics = None\n",
    "            alt_run_dir = None\n",
    "            candidates = [d for d in archive_root.iterdir() if d.is_dir() and \"1000genes\" in d.name]\n",
    "            if dataset_label:\n",
    "                candidates = [d for d in candidates if dataset_label.lower() in d.name.lower()]\n",
    "            for cand in sorted(candidates, key=lambda d: _run_timestamp_key(d.name), reverse=True):\n",
    "                model_dir = cand / \"models\" / rec.model_id\n",
    "                if not model_dir.exists():\n",
    "                    continue\n",
    "                alt_metrics = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if alt_metrics and alt_metrics.exists():\n",
    "                    alt_run_dir = cand\n",
    "                    alt_model_dir = model_dir\n",
    "                    break\n",
    "\n",
    "            if alt_run_dir is None and (dataset_label, rec.model_id) not in successful_1000:\n",
    "                # Candidate alt run name by replacing gene count portion\n",
    "                alt_run_name = rec.run_name.replace(f\"{GENE_COUNT}genes\", \"100genes\")\n",
    "                alt_run_dir = archive_root / alt_run_name\n",
    "\n",
    "                # If exact archive path not present, search for any archive run that contains\n",
    "                # '100genes' and a matching model id under its models/ directory, filtered by dataset\n",
    "                if not alt_run_dir.exists():\n",
    "                    candidates = [d for d in archive_root.iterdir() if d.is_dir() and \"100genes\" in d.name]\n",
    "                    if dataset_label:\n",
    "                        candidates = [d for d in candidates if dataset_label.lower() in d.name.lower()]\n",
    "                    found = None\n",
    "                    for cand in sorted(candidates, key=lambda d: _run_timestamp_key(d.name), reverse=True):\n",
    "                        model_dir = cand / \"models\" / rec.model_id\n",
    "                        if not model_dir.exists():\n",
    "                            continue\n",
    "                        alt_metrics = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                        if alt_metrics and alt_metrics.exists():\n",
    "                            found = (cand, model_dir, alt_metrics)\n",
    "                            break\n",
    "                    if found:\n",
    "                        alt_run_dir, alt_model_dir, alt_metrics = found\n",
    "                    else:\n",
    "                        alt_model_dir = None\n",
    "                        alt_metrics = None\n",
    "                else:\n",
    "                    alt_model_dir = alt_run_dir / \"models\" / rec.model_id\n",
    "                    if alt_model_dir.exists():\n",
    "                        alt_metrics = _first_existing(alt_model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "\n",
    "            if alt_metrics and alt_metrics.exists() and alt_model_dir and alt_model_dir.exists():\n",
    "                alt_preds = _first_existing(\n",
    "                    alt_model_dir,\n",
    "                    (\n",
    "                        \"predictions_raw.csv\",\n",
    "                        \"predictions.csv\",\n",
    "                    ),\n",
    "                )\n",
    "                alt_history = _first_existing(\n",
    "                    alt_model_dir,\n",
    "                    (\n",
    "                        \"training_history.csv\",\n",
    "                        \"training_history_loss.csv\",\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                # Keep actual run_path/model_path pointing to archive files, but prepare\n",
    "                # a user-friendly display name that normalises 'grn' -> 'spear' when present\n",
    "                alt_run_dir_name = alt_run_dir.name\n",
    "                display_run_name = alt_run_dir_name.replace(\"grn\", \"spear\") if \"grn\" in alt_run_dir_name else alt_run_dir_name\n",
    "\n",
    "                new_rec = RunRecord(\n",
    "                    run_name=alt_run_dir_name,\n",
    "                    model_id=rec.model_id,\n",
    "                    run_path=alt_run_dir,\n",
    "                    model_path=alt_model_dir,\n",
    "                    metrics_path=alt_metrics,\n",
    "                    predictions_path=alt_preds,\n",
    "                    training_history_path=alt_history,\n",
    "                    model_display=rec.model_display,\n",
    "                )\n",
    "                # Replace the entry and record that a fallback was used for this run+model\n",
    "                updated_records[idx] = new_rec\n",
    "                FALLBACK_USED.add((new_rec.run_name, new_rec.model_id))\n",
    "                FALLBACK_DISPLAY_RENAME[(new_rec.run_name, new_rec.model_id)] = display_run_name\n",
    "                dataset_label = dataset_label or _dataset_from_run_name(alt_run_dir_name)\n",
    "                if dataset_label:\n",
    "                    FALLBACK_COMBOS.add((dataset_label, rec.model_id))\n",
    "\n",
    "        run_records = [r for r in updated_records if r is not None]\n",
    "\n",
    "# Ensure model_display is attached for any replaced records.\n",
    "run_records = attach_lookup(run_records, model_lookup)\n",
    "\n",
    "# Add archive 100-gene runs when 1000-gene models are missing (per dataset)\n",
    "if USE_1000PLUS100:\n",
    "    archive_root = config.results_root / \"archive\"\n",
    "    if archive_root.exists():\n",
    "        dataset_models_1000 = {}\n",
    "        for rec in run_records:\n",
    "            if \"1000genes\" not in rec.run_name:\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(rec.run_name) or \"\"\n",
    "            dataset_models_1000.setdefault(ds, set()).add(rec.model_id)\n",
    "\n",
    "        archive_candidates = {}\n",
    "        for run_dir in archive_root.iterdir():\n",
    "            if not run_dir.is_dir() or \"100genes\" not in run_dir.name:\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(run_dir.name) or \"\"\n",
    "            if not ds:\n",
    "                continue\n",
    "            models_dir = run_dir / \"models\"\n",
    "            if not models_dir.exists():\n",
    "                continue\n",
    "            for model_dir in models_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                metrics_path = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if not metrics_path or not metrics_path.exists():\n",
    "                    continue\n",
    "                key = (ds, model_dir.name)\n",
    "                archive_candidates.setdefault(key, []).append((run_dir, model_dir, metrics_path))\n",
    "\n",
    "        def _timestamp_key(name: str) -> tuple[int, int]:\n",
    "            import re\n",
    "            match = re.search(r\"_(\\d{8})_(\\d{6})\", name)\n",
    "            if not match:\n",
    "                return (0, 0)\n",
    "            return (int(match.group(1)), int(match.group(2)))\n",
    "\n",
    "        added_records = []\n",
    "        for (ds, model_id), candidates in archive_candidates.items():\n",
    "            if model_id in dataset_models_1000.get(ds, set()):\n",
    "                continue\n",
    "            candidates.sort(key=lambda item: _timestamp_key(item[0].name), reverse=True)\n",
    "            run_dir, model_dir, metrics_path = candidates[0]\n",
    "            preds_path = _first_existing(model_dir, (\"predictions_raw.csv\", \"predictions.csv\"))\n",
    "            history_path = _first_existing(model_dir, (\"training_history.csv\", \"training_history_loss.csv\"))\n",
    "            new_rec = RunRecord(\n",
    "                run_name=run_dir.name,\n",
    "                model_id=model_id,\n",
    "                run_path=run_dir,\n",
    "                model_path=model_dir,\n",
    "                metrics_path=metrics_path,\n",
    "                predictions_path=preds_path,\n",
    "                training_history_path=history_path,\n",
    "                model_display=MODEL_ID_TO_DISPLAY.get(model_id, _guess_display_name(model_id)),\n",
    "            )\n",
    "            added_records.append(new_rec)\n",
    "            FALLBACK_USED.add((new_rec.run_name, new_rec.model_id))\n",
    "            if ds:\n",
    "                FALLBACK_COMBOS.add((ds, model_id))\n",
    "        if added_records:\n",
    "            run_records = list(run_records) + added_records\n",
    "\n",
    "# Prefer most recent 1000-gene runs, fallback to 100-gene when missing.\n",
    "\n",
    "def _select_preferred_runs(records: list[RunRecord], prefer_1000: bool) -> list[RunRecord]:\n",
    "    if not records:\n",
    "        return []\n",
    "    if not prefer_1000:\n",
    "        return list(records)\n",
    "\n",
    "    def _timestamp_key(name: str) -> tuple[int, int]:\n",
    "        match = re.search(r\"_(\\d{8})_(\\d{6})\", name)\n",
    "        if not match:\n",
    "            return (0, 0)\n",
    "        return (int(match.group(1)), int(match.group(2)))\n",
    "\n",
    "    def _has_metrics(rec: RunRecord) -> bool:\n",
    "        path = rec.metrics_path\n",
    "        if not path:\n",
    "            return False\n",
    "        path = path if isinstance(path, Path) else Path(path)\n",
    "        return path.exists()\n",
    "\n",
    "    def _dataset_key(rec: RunRecord) -> str:\n",
    "        return _dataset_from_run_name(rec.run_name) or \"\"\n",
    "\n",
    "    selected: list[RunRecord] = []\n",
    "    by_model_dataset: dict[tuple[str, str], list[RunRecord]] = {}\n",
    "    for record in records:\n",
    "        key = (record.model_id, _dataset_key(record))\n",
    "        by_model_dataset.setdefault(key, []).append(record)\n",
    "\n",
    "    for (model_id, dataset_label), group in by_model_dataset.items():\n",
    "        group_1000 = [rec for rec in group if \"1000genes\" in rec.run_name and _has_metrics(rec)]\n",
    "        group_100 = [rec for rec in group if \"100genes\" in rec.run_name and _has_metrics(rec)]\n",
    "        group_1000_all = [rec for rec in group if \"1000genes\" in rec.run_name]\n",
    "        group_100_all = [rec for rec in group if \"100genes\" in rec.run_name]\n",
    "        candidates = group_1000 or group_100 or group_1000_all or group_100_all or group\n",
    "        candidates.sort(key=lambda rec: _timestamp_key(rec.run_name), reverse=True)\n",
    "        selected.append(candidates[0])\n",
    "\n",
    "    return selected\n",
    "\n",
    "run_records = _select_preferred_runs(list(run_records), PREFER_1000_FALLBACK_100)\n",
    "\n",
    "run_df = run_records_to_frame(run_records)\n",
    "run_df.sort_values([\"run_name\", \"model_id\"], inplace=True)\n",
    "run_df_display = run_df.copy()\n",
    "for column in (\"run_path\", \"model_path\", \"metrics_path\", \"predictions_path\", \"training_history_path\"):\n",
    "    run_df_display[column] = run_df_display[column].map(lambda value: to_relative_path(value, config.project_root))\n",
    "\n",
    "# Optionally rename display run names from 'grn' -> 'spear' when applicable for clarity.\n",
    "if FALLBACK_DISPLAY_RENAME:\n",
    "    mask = run_df_display.apply(lambda r: (r['run_name'], r['model_id']) in FALLBACK_DISPLAY_RENAME, axis=1)\n",
    "    if mask.any():\n",
    "        def _maybe_rename_row(r):\n",
    "            key = (r['run_name'], r['model_id'])\n",
    "            return FALLBACK_DISPLAY_RENAME.get(key, r['run_name'])\n",
    "        run_df_display.loc[mask, 'run_name'] = run_df_display.loc[mask].apply(_maybe_rename_row, axis=1)\n",
    "\n",
    "analysis_metadata.update({\n",
    "    \"results_root\": to_relative_path(config.results_root, config.project_root),\n",
    "    \"fig_dir\": to_relative_path(config.fig_dir, config.project_root),\n",
    "    \"reports_dir\": to_relative_path(config.reports_dir, config.project_root),\n",
    "    \"run_count\": run_df[\"run_name\"].nunique(),\n",
    "    \"model_count\": len(run_df),\n",
    "    \"model_lookup_path\": to_relative_path(config.lookup_path, config.project_root),\n",
    "})\n",
    "analysis_metadata.setdefault(\"include_globs\", config.run_include_globs)\n",
    "analysis_metadata.setdefault(\"subset_descriptor\", RUN_SUBSET_DESCRIPTION)\n",
    "analysis_metadata.setdefault(\"subset_label\", RUN_SUBSET_LABEL)\n",
    "\n",
    "display(Markdown(f\"**Scanning results root:** `{analysis_metadata['results_root']}`\"))\n",
    "include_filters = analysis_metadata.get(\"include_globs\")\n",
    "if include_filters:\n",
    "    include_text = \", \".join(str(item) for item in include_filters)\n",
    "    display(Markdown(f\"**Include filters:** `{include_text}`\"))\n",
    "subset_descriptor = analysis_metadata.get(\"subset_descriptor\")\n",
    "if subset_descriptor:\n",
    "    display(Markdown(f\"**Subset criteria:** {subset_descriptor}\"))\n",
    "\n",
    "display(Markdown(\n",
    "    f\"**Figure output:** `{analysis_metadata['fig_dir']}` | **Reports:** `{analysis_metadata['reports_dir']}`\"\n",
    "))\n",
    "display(run_df_display)\n",
    "print(\n",
    "    \"Discovered\",\n",
    "    analysis_metadata[\"model_count\"],\n",
    "    \"model folders across\",\n",
    "    analysis_metadata[\"run_count\"],\n",
    "    \"runs.\",\n",
    ")\n",
    "print(f\"Fallback replacements: {len(FALLBACK_USED)} model+dataset combos using 100-gene from archive\")\n",
    "if IGNORED_MISSING:\n",
    "    print(f\"Ignored missing runs (no fallback): {sorted(IGNORED_MISSING)}\")\n",
    "\n",
    "\n",
    "# Export fallback usage list for transparency (covers archive + in-run 100-gene fallbacks).\n",
    "\n",
    "# Build availability lookup from raw records to detect missing 1000-gene metrics.\n",
    "model_dataset_key = []\n",
    "for record in raw_run_records:\n",
    "    ds = _dataset_from_run_name(record.run_name) or \"\"\n",
    "    model_dataset_key.append((record.model_id, ds, record.run_name, record.metrics_path))\n",
    "\n",
    "fallback_rows = []\n",
    "for record in run_records:\n",
    "    ds = _dataset_from_run_name(record.run_name) or \"\"\n",
    "    if \"100genes\" not in record.run_name:\n",
    "        continue\n",
    "    # Check if any 1000-gene run exists with metrics for the same model/dataset.\n",
    "    has_1000_metrics = False\n",
    "    for model_id, dataset_label, run_name, metrics_path in model_dataset_key:\n",
    "        if model_id != record.model_id or dataset_label != ds:\n",
    "            continue\n",
    "        if \"1000genes\" not in run_name:\n",
    "            continue\n",
    "        if metrics_path is None:\n",
    "            continue\n",
    "        try:\n",
    "            if Path(metrics_path).exists():\n",
    "                has_1000_metrics = True\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not has_1000_metrics:\n",
    "        fallback_rows.append({\n",
    "            \"dataset\": ds,\n",
    "            \"model_id\": record.model_id,\n",
    "            \"run_name\": record.run_name,\n",
    "            \"source_run\": record.run_name,\n",
    "        })\n",
    "\n",
    "# Include explicit archive replacements, if any.\n",
    "for run_name, model_id in sorted(FALLBACK_USED):\n",
    "    dataset_label = _dataset_from_run_name(run_name) or \"\"\n",
    "    source = FALLBACK_DISPLAY_RENAME.get((run_name, model_id), run_name)\n",
    "    fallback_rows.append({\n",
    "        \"dataset\": dataset_label,\n",
    "        \"model_id\": model_id,\n",
    "        \"run_name\": run_name,\n",
    "        \"source_run\": source,\n",
    "    })\n",
    "\n",
    "fallback_report = Path(config.reports_dir) / \"fallback_models_used.csv\"\n",
    "if not fallback_rows:\n",
    "    fallback_df = pd.DataFrame(columns=[\"dataset\", \"model_id\", \"run_name\", \"source_run\"])\n",
    "else:\n",
    "    fallback_df = pd.DataFrame(fallback_rows).drop_duplicates()\n",
    "fallback_df.to_csv(fallback_report, index=False)\n",
    "analysis_metadata[\"fallback_report\"] = to_relative_path(fallback_report, config.project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07624b37",
   "metadata": {},
   "source": [
    "## 5. Load Metrics and Compute Summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8d977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:54.114316Z",
     "iopub.status.busy": "2026-01-18T00:48:54.114129Z",
     "iopub.status.idle": "2026-01-18T00:48:54.466465Z",
     "shell.execute_reply": "2026-01-18T00:48:54.465742Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long, metrics_wide = load_metrics(run_records)\n",
    "\n",
    "# Table: mean test Pearson for all runs with metrics\n",
    "all_records = attach_lookup(raw_run_records, model_lookup)\n",
    "all_metrics_long, all_metrics_wide = load_metrics(all_records)\n",
    "if all_metrics_wide.empty or \"test_pearson\" not in all_metrics_wide:\n",
    "    print(\"No test-set metrics available for full run table.\")\n",
    "else:\n",
    "    mean_df = (\n",
    "        all_metrics_wide.groupby([\"run_name\", \"model_id\"])\n",
    "        [\"test_pearson\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"test_pearson\": \"test_pearson_mean\"})\n",
    "    )\n",
    "    path_df = run_records_to_frame(all_records)[[\"run_name\", \"model_id\", \"model_path\"]]\n",
    "    all_run_mean_test_pearson = mean_df.merge(path_df, on=[\"run_name\", \"model_id\"], how=\"left\")\n",
    "    all_run_mean_test_pearson[\"dataset\"] = all_run_mean_test_pearson[\"run_name\"].map(_dataset_from_run_name)\n",
    "    all_run_mean_test_pearson[\"model_path\"] = all_run_mean_test_pearson[\"model_path\"].map(\n",
    "        lambda value: to_relative_path(value, config.project_root)\n",
    "    )\n",
    "    display(all_run_mean_test_pearson.sort_values([\"run_name\", \"model_id\"]))\n",
    "    TABLES[\"all_run_mean_test_pearson\"] = all_run_mean_test_pearson\n",
    "if metrics_long.empty:\n",
    "    raise RuntimeError(\"No metrics available for plotting.\")\n",
    "\n",
    "\n",
    "# Helper reused across sections\n",
    "\n",
    "def _dataset_from_run_name(name: str) -> Optional[str]:\n",
    "    name = str(name).lower()\n",
    "    if \"embryonic\" in name:\n",
    "        return \"Embryonic\"\n",
    "    if \"endothelial\" in name:\n",
    "        return \"Endothelial\"\n",
    "    return None\n",
    "\n",
    "split_filter = {config.primary_split, config.val_split, config.train_split}\n",
    "available_splits = sorted(metrics_long[\"split\"].unique())\n",
    "missing_splits = split_filter.difference(available_splits)\n",
    "if missing_splits:\n",
    "    print(\"Warning: the following splits are missing from metrics files:\", sorted(missing_splits))\n",
    "\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_dataset_from_run_name)\n",
    "if not metrics_wide.empty and \"dataset\" not in metrics_wide:\n",
    "    metrics_wide[\"dataset\"] = metrics_wide[\"run_name\"].map(_dataset_from_run_name)\n",
    "\n",
    "test_metrics = metrics_long[metrics_long[\"split\"] == config.primary_split].copy()\n",
    "val_metrics = metrics_long[metrics_long[\"split\"] == config.val_split].copy()\n",
    "train_metrics = metrics_long[metrics_long[\"split\"] == config.train_split].copy()\n",
    "summary_df = compute_model_summary(\n",
    "    metrics_wide, [config.primary_split, config.val_split, config.train_split]\n",
    ")\n",
    "if summary_df.empty:\n",
    "    raise RuntimeError(\"Unable to compute summary statistics from metrics.\")\n",
    "\n",
    "summary_reset = summary_df.reset_index()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_dataset_from_run_name)\n",
    "analysis_metadata[\"best_model_id\"] = summary_reset.iloc[0][\"model_id\"]\n",
    "analysis_metadata[\"best_model_display\"] = summary_reset.iloc[0][\"model_display\"]\n",
    "analysis_metadata[\"best_run_name\"] = summary_reset.iloc[0][\"run_name\"]\n",
    "\n",
    "# Keep model_display untouched; fallback markers are added in plots instead of renaming.\n",
    "fallback_pairs = set(globals().get(\"FALLBACK_COMBOS\", set()))\n",
    "\n",
    "if \"test_pearson_mean\" in summary_reset:\n",
    "    model_display_order = summary_reset.sort_values(\n",
    "        by=\"test_pearson_mean\", ascending=False\n",
    "    )[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "else:\n",
    "    model_display_order = summary_reset[\"model_display\"].tolist()\n",
    "\n",
    "display(summary_reset)\n",
    "\n",
    "# Print fallback details for user reference\n",
    "if fallback_pairs:\n",
    "    print(f\"\\n=== FALLBACK MODELS ({len(fallback_pairs)} combos using 100-gene archive) ===\")\n",
    "    fallback_sources = list(globals().get(\"FALLBACK_USED\", set()))\n",
    "    for dataset_name, model_id in sorted(fallback_pairs):\n",
    "        source = next(\n",
    "            (run for run, mid in fallback_sources if mid == model_id and dataset_name.lower() in run.lower()),\n",
    "            None,\n",
    "        )\n",
    "        source_text = source or \"archive\"\n",
    "        print(f\"  {model_id:20s} + {dataset_name:12s} (from: {source_text})\")\n",
    "\n",
    "analysis_state = {\n",
    "    \"run_df\": run_df,\n",
    "    \"metrics_long\": metrics_long,\n",
    "    \"metrics_wide\": metrics_wide,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"summary_df\": summary_df,\n",
    "    \"summary_reset\": summary_reset,\n",
    "    \"model_display_order\": model_display_order,\n",
    "    \"model_short_name_map\": MODEL_ID_TO_SHORT.copy(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4831c25",
   "metadata": {},
   "source": [
    "## 6. Supporting Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:54.469788Z",
     "iopub.status.busy": "2026-01-18T00:48:54.469597Z",
     "iopub.status.idle": "2026-01-18T00:48:54.525236Z",
     "shell.execute_reply": "2026-01-18T00:48:54.524204Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df = analysis_state[\"summary_df\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "metrics_wide = analysis_state[\"metrics_wide\"]\n",
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "\n",
    "if \"dataset\" not in metrics_long:\n",
    "    def _dataset_from_run_name(name: str) -> Optional[str]:\n",
    "        name = str(name).lower()\n",
    "        if \"embryonic\" in name:\n",
    "            return \"Embryonic\"\n",
    "        if \"endothelial\" in name:\n",
    "            return \"Endothelial\"\n",
    "        return None\n",
    "    metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_dataset_from_run_name)\n",
    "\n",
    "val_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.val_split}_pearson\" in metrics_wide:\n",
    "    val_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.val_split}_pearson\",\n",
    "    )\n",
    "\n",
    "test_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "    )\n",
    "\n",
    "# Order genes from highest to lowest test Pearson using Embryonic scores when available\n",
    "_gene_order = pd.Index([])\n",
    "emb_test = metrics_long[\n",
    "    (metrics_long[\"split\"] == config.primary_split)\n",
    "    & (metrics_long[\"dataset\"] == \"Embryonic\")\n",
    "]\n",
    "if not emb_test.empty and \"pearson\" in emb_test:\n",
    "    _gene_order = (\n",
    "        emb_test.groupby(\"gene\")[\"pearson\"].mean().sort_values(ascending=False).index\n",
    "    )\n",
    "if not test_pearson_per_gene.empty and len(_gene_order) > 0:\n",
    "    aligned = test_pearson_per_gene.index.intersection(_gene_order)\n",
    "    remainder = test_pearson_per_gene.index.difference(_gene_order)\n",
    "    test_pearson_per_gene = test_pearson_per_gene.loc[list(aligned) + list(remainder)]\n",
    "    if not val_pearson_per_gene.empty:\n",
    "        val_pearson_per_gene = val_pearson_per_gene.reindex(test_pearson_per_gene.index)\n",
    "\n",
    "maybe_store_table(\n",
    "    TABLES,\n",
    "    \"metrics_per_gene_master\",\n",
    "    metrics_long.sort_values([\"split\", \"run_name\", \"model_id\", \"gene\"]),\n",
    ")\n",
    "maybe_store_table(TABLES, \"summary_metrics_all_models\", summary_reset)\n",
    "\n",
    "# Persist key summaries to disk\n",
    "reports_dir = Path(config.reports_dir)\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_csv = reports_dir / \"summary_metrics_all_models.csv\"\n",
    "summary_reset.to_csv(summary_csv, index=False)\n",
    "\n",
    "# Write a compact markdown summary including fallbacks\n",
    "results_md = reports_dir / \"RESULTS_SUMMARY.md\"\n",
    "fallback_pairs = set(globals().get(\"FALLBACK_COMBOS\", set()))\n",
    "fallback_sources = list(globals().get(\"FALLBACK_USED\", set()))\n",
    "if fallback_pairs:\n",
    "    fb_lines = [\"- {} | {} | source: {}\".format(\n",
    "        dataset,\n",
    "        model,\n",
    "        next((run for run, mid in fallback_sources if mid == model and dataset.lower() in run.lower()), \"archive\"),\n",
    "    ) for dataset, model in sorted(fallback_pairs)]\n",
    "else:\n",
    "    fb_lines = [\"- none\"]\n",
    "\n",
    "_top = summary_reset.head(10)\n",
    "md_table = \"| \" + \" | \".join(_top.columns) + \" |\\n\"\n",
    "md_table += \"| \" + \" | \".join([\"---\"] * len(_top.columns)) + \" |\\n\"\n",
    "for _, row in _top.iterrows():\n",
    "    md_table += \"| \" + \" | \".join(str(row[col]) for col in _top.columns) + \" |\\n\"\n",
    "\n",
    "md_text = \"\\n\".join(\n",
    "    [\"# Results Summary\",\n",
    "     f\"- Generated at: {analysis_metadata.get('generated_at', '')}\",\n",
    "     f\"- Results root: {analysis_metadata.get('results_root', config.results_root)}\",\n",
    "     f\"- Reports dir: {reports_dir}\",\n",
    "     f\"- Run count: {analysis_metadata.get('run_count', len(summary_reset))}\",\n",
    "     f\"- Model rows: {len(summary_reset)}\",\n",
    "     \"- Fallbacks (dataset | model | source):\",\n",
    "     *fb_lines,\n",
    "     \"\",\n",
    "     \"## Top models (test pearson)\",\n",
    "     md_table\n",
    "    ]\n",
    ")\n",
    "results_md.write_text(md_text)\n",
    "\n",
    "analysis_state.update(\n",
    "    {\n",
    "        \"val_pearson_per_gene\": val_pearson_per_gene,\n",
    "        \"test_pearson_per_gene\": test_pearson_per_gene,\n",
    "        \"split_mean_summary\": summary_reset,\n",
    "        \"summary_csv_path\": summary_csv,\n",
    "        \"results_md_path\": results_md,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ff85b",
   "metadata": {},
   "source": [
    "## 7. Heatmap: Embryonic vs Endothelial Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b0986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:54.528509Z",
     "iopub.status.busy": "2026-01-18T00:48:54.528334Z",
     "iopub.status.idle": "2026-01-18T00:48:55.087704Z",
     "shell.execute_reply": "2026-01-18T00:48:55.086649Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "\n",
    "def _assign_dataset(run_name: str) -> Optional[str]:\n",
    "    name = str(run_name).lower()\n",
    "    if \"embryonic\" in name:\n",
    "        return \"Embryonic\"\n",
    "    if \"endothelial\" in name:\n",
    "        return \"Endothelial\"\n",
    "    return None\n",
    "\n",
    "if \"dataset\" not in summary_reset:\n",
    "    summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "else:\n",
    "    summary_reset[\"dataset\"] = summary_reset[\"dataset\"].fillna(summary_reset[\"run_name\"].map(_assign_dataset))\n",
    "\n",
    "subset = summary_reset[summary_reset[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])].copy()\n",
    "fig_heatmap = None\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "if subset.empty or \"test_pearson_mean\" not in subset:\n",
    "    print(\"Dataset-specific test Pearson summaries unavailable; skipping heatmap.\")\n",
    "else:\n",
    "    ranked = subset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates([\"dataset\", \"model_id\"])\n",
    "    heatmap_df = best_per_model.pivot_table(\n",
    "        index=\"model_display\",\n",
    "        columns=\"dataset\",\n",
    "        values=\"test_pearson_mean\",\n",
    "    )\n",
    "    if heatmap_df.empty:\n",
    "        print(\"No values available for dataset heatmap.\")\n",
    "    else:\n",
    "        if \"Embryonic\" in heatmap_df:\n",
    "            order = heatmap_df[\"Embryonic\"].sort_values(ascending=False).index\n",
    "        else:\n",
    "            order = heatmap_df.mean(axis=1).sort_values(ascending=False).index\n",
    "        heatmap_df = heatmap_df.loc[order]\n",
    "        column_order = [col for col in [\"Embryonic\", \"Endothelial\"] if col in heatmap_df.columns]\n",
    "        heatmap_df = heatmap_df[column_order]\n",
    "        vmin, vmax = compute_heatmap_limits(heatmap_df.values)\n",
    "        fig_height = max(4, 0.4 * len(heatmap_df))\n",
    "        fig_width = max(7, 2.0 * len(column_order))\n",
    "        fig_heatmap, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            linewidths=0.4,\n",
    "            linecolor=\"#f2f2f2\",\n",
    "            cbar_kws={\"label\": \"Mean Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"Mean Test Pearson by Model and Dataset | {title_suffix}\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        ax.tick_params(axis=\"x\", pad=8)\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        sns.despine(fig_heatmap, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        fig_heatmap.subplots_adjust(bottom=0.22)\n",
    "\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_by_dataset\", fig_heatmap)\n",
    "if fig_heatmap is not None:\n",
    "    display(fig_heatmap)\n",
    "    plt.close(fig_heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17172c89",
   "metadata": {},
   "source": [
    "## 8. Violin Plot: Test Pearson by Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f286cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:55.094303Z",
     "iopub.status.busy": "2026-01-18T00:48:55.093882Z",
     "iopub.status.idle": "2026-01-18T00:48:55.769049Z",
     "shell.execute_reply": "2026-01-18T00:48:55.768172Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"].copy()\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "summary_reset = analysis_state.get(\"summary_reset\", pd.DataFrame()).copy()\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "\n",
    "subset = metrics_long[\n",
    "    (metrics_long[\"split\"] == config.primary_split)\n",
    "    & (metrics_long[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"]))\n",
    "].copy()\n",
    "\n",
    "fig_violin_by_dataset = None\n",
    "if subset.empty:\n",
    "    print(\"Test metrics unavailable for embryonic/endothelial comparison.\")\n",
    "else:\n",
    "    emb_order = []\n",
    "    if not summary_reset.empty and \"test_pearson_mean\" in summary_reset:\n",
    "        emb_order = (\n",
    "            summary_reset[summary_reset[\"dataset\"] == \"Embryonic\"]\n",
    "            .sort_values(\"test_pearson_mean\", ascending=False)[\"model_display\"].tolist()\n",
    "        )\n",
    "    ordered_models = [m for m in emb_order if m in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = [m for m in model_display_order if m in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = subset[\"model_display\"].sort_values().unique().tolist()\n",
    "    dataset_order = [\"Embryonic\", \"Endothelial\"]\n",
    "    palette = {\"Embryonic\": \"#4C78A8\", \"Endothelial\": \"#F58518\"}\n",
    "    fig_width = max(10, 0.75 * max(6, len(ordered_models)))\n",
    "    fig_violin_by_dataset, ax = plt.subplots(figsize=(fig_width, 6.5))\n",
    "    sns.violinplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset\",\n",
    "        order=ordered_models,\n",
    "        hue_order=dataset_order,\n",
    "        palette=palette,\n",
    "        density_norm=\"width\",\n",
    "        inner=\"quartile\",\n",
    "        linewidth=1.2,\n",
    "        dodge=True,\n",
    "        ax=ax,\n",
    "    )\n",
    "    # Emphasize quartile/median lines.\n",
    "    for line in ax.lines:\n",
    "        line.set_linewidth(2.0)\n",
    "        line.set_color(\"#111111\")\n",
    "\n",
    "    jitter_sample = subset.sample(\n",
    "        min(len(subset), 4000),\n",
    "        random_state=config.random_seed,\n",
    "    ) if len(subset) > 4000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset\",\n",
    "        order=ordered_models,\n",
    "        hue_order=dataset_order,\n",
    "        palette=palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=2.4,\n",
    "        alpha=0.35,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.3,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    ax.set_xticklabels(ordered_models, rotation=35, ha=\"right\")\n",
    "    ax.set_title(f\"Test Pearson Distribution by Model and Dataset | {title_suffix}\")\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Test Pearson\")\n",
    "    ax.legend(title=\"Dataset\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_violin_by_dataset, left=True, bottom=True)\n",
    "    fig_violin_by_dataset.tight_layout()\n",
    "\n",
    "register_figure(FIGURES, \"test_violin_by_dataset\", fig_violin_by_dataset)\n",
    "if fig_violin_by_dataset is not None:\n",
    "    display(fig_violin_by_dataset)\n",
    "    plt.close(fig_violin_by_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81104b58",
   "metadata": {},
   "source": [
    "## 9. Dataset-Level Summary Panels\n",
    "\n",
    "Dataset-scoped metrics panels and generalization gap summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10d5ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:48:55.779227Z",
     "iopub.status.busy": "2026-01-18T00:48:55.779043Z",
     "iopub.status.idle": "2026-01-18T00:49:03.100254Z",
     "shell.execute_reply": "2026-01-18T00:49:03.099640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset-specific manuscript plots\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "metrics_long = analysis_state[\"metrics_long\"].copy()\n",
    "metrics_wide = analysis_state[\"metrics_wide\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_assign_dataset)\n",
    "if not metrics_wide.empty and \"dataset\" not in metrics_wide:\n",
    "    metrics_wide[\"dataset\"] = metrics_wide[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "metrics_to_plot = {\n",
    "    \"pearson\": \"Pearson\",\n",
    "    \"spearman\": \"Spearman\",\n",
    "    \"r2\": \"R2\",\n",
    "    \"rmse\": \"RMSE\",\n",
    "}\n",
    "\n",
    "for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "    ds_summary = summary_reset[summary_reset[\"dataset\"] == dataset].copy()\n",
    "    ds_test = metrics_long[\n",
    "        (metrics_long[\"dataset\"] == dataset)\n",
    "        & (metrics_long[\"split\"] == config.primary_split)\n",
    "    ].copy()\n",
    "    ds_splits = metrics_long[\n",
    "        (metrics_long[\"dataset\"] == dataset)\n",
    "        & (metrics_long[\"split\"].isin([config.train_split, config.val_split, config.primary_split]))\n",
    "    ].copy()\n",
    "\n",
    "    if ds_summary.empty or ds_test.empty:\n",
    "        print(f\"Skipping {dataset} dataset-specific plots (no data).\")\n",
    "        continue\n",
    "\n",
    "    model_order = (\n",
    "        ds_summary.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "        [[\"model_display\", \"model_id\"]]\n",
    "        .drop_duplicates(\"model_id\")[\"model_display\"]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Split comparison overview (per dataset)\n",
    "    split_labels = {\n",
    "        config.train_split: \"Train\",\n",
    "        config.val_split: \"Val\",\n",
    "        config.primary_split: \"Test\",\n",
    "    }\n",
    "    ds_splits = ds_splits[[\"model_display\", \"split\", \"pearson\"]].dropna()\n",
    "    ds_splits[\"split_label\"] = ds_splits[\"split\"].map(split_labels).fillna(ds_splits[\"split\"].str.title())\n",
    "    split_order = [split_labels[split] for split in [config.train_split, config.val_split, config.primary_split]]\n",
    "    fig_width = max(12, 0.75 * max(6, len(model_order)))\n",
    "    fig_split, ax = plt.subplots(figsize=(fig_width, 6.5))\n",
    "    sns.boxplot(\n",
    "        data=ds_splits,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=model_order,\n",
    "        hue_order=split_order,\n",
    "        palette=\"Set2\",\n",
    "        width=0.65,\n",
    "        fliersize=0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = ds_splits.sample(min(len(ds_splits), 4000), random_state=config.random_seed) if len(ds_splits) > 4000 else ds_splits\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=model_order,\n",
    "        hue_order=split_order,\n",
    "        palette=\"Set2\",\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=3.0,\n",
    "        alpha=0.5,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.4,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Pearson\")\n",
    "    ax.set_title(f\"{dataset} | Split Comparison Overview | {title_suffix}\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Split\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_split, left=True, bottom=True)\n",
    "    fig_split.tight_layout()\n",
    "    register_figure(FIGURES, f\"split_comparison_overview_{dataset.lower()}\", fig_split)\n",
    "    display(fig_split)\n",
    "    plt.close(fig_split)\n",
    "\n",
    "    # Boxplots (half violin + half box + scatter) ordered by test Pearson\n",
    "    def _half_violin_box(metric_key: str, title_label: str) -> None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.violinplot(\n",
    "            data=ds_test,\n",
    "            x=\"model_display\",\n",
    "            y=metric_key,\n",
    "            order=model_order,\n",
    "            ax=ax,\n",
    "            palette=\"Set2\",\n",
    "            inner=None,\n",
    "            cut=0,\n",
    "            linewidth=1.0,\n",
    "        )\n",
    "        for idx, poly in enumerate(ax.collections[:len(model_order)]):\n",
    "            for path in poly.get_paths():\n",
    "                verts = path.vertices\n",
    "                verts[:, 0] = np.clip(verts[:, 0], idx, np.inf)\n",
    "\n",
    "        data_by_model = [\n",
    "            ds_test.loc[ds_test[\"model_display\"] == name, metric_key].dropna().values\n",
    "            for name in model_order\n",
    "        ]\n",
    "        positions = np.arange(len(model_order)) - 0.2\n",
    "        box = ax.boxplot(\n",
    "            data_by_model,\n",
    "            positions=positions,\n",
    "            widths=0.2,\n",
    "            patch_artist=True,\n",
    "            showfliers=False,\n",
    "            medianprops={\"color\": \"#222222\", \"linewidth\": 1.0},\n",
    "        )\n",
    "        for patch in box[\"boxes\"]:\n",
    "            patch.set(facecolor=\"#ffffff\", edgecolor=\"#222222\", linewidth=1.0)\n",
    "        for whisker in box[\"whiskers\"]:\n",
    "            whisker.set(color=\"#222222\", linewidth=1.0)\n",
    "        for cap in box[\"caps\"]:\n",
    "            cap.set(color=\"#222222\", linewidth=1.0)\n",
    "\n",
    "        rng_local = np.random.default_rng(config.random_seed)\n",
    "        for idx, vals in enumerate(data_by_model):\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            jitter = rng_local.normal(0, 0.03, size=len(vals))\n",
    "            x_vals = np.full(len(vals), positions[idx]) + jitter\n",
    "            ax.scatter(x_vals, vals, s=10, alpha=0.35, color=\"#2b2b2b\", linewidth=0)\n",
    "\n",
    "        ax.set_title(f\"{dataset} | Test {title_label} by Model | {title_suffix}\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(title_label)\n",
    "        ax.set_xticks(np.arange(len(model_order)))\n",
    "        ax.set_xticklabels(model_order, rotation=45, ha=\"right\")\n",
    "        ax.set_xlim(-0.6, len(model_order) - 0.4)\n",
    "        sns.despine(ax=ax, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        register_figure(FIGURES, f\"test_{metric_key}_boxplot_{dataset.lower()}\", fig)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    for metric_key, title_label in metrics_to_plot.items():\n",
    "        if metric_key in ds_test.columns:\n",
    "            _half_violin_box(metric_key, title_label)\n",
    "\n",
    "    # Combined mean-metric heatmap (per dataset)\n",
    "    metric_means = {}\n",
    "    for metric_key, title_label in metrics_to_plot.items():\n",
    "        if metric_key not in ds_test.columns:\n",
    "            continue\n",
    "        series = ds_test.groupby(\"model_display\")[metric_key].mean().dropna()\n",
    "        if not series.empty:\n",
    "            metric_means[title_label] = series\n",
    "\n",
    "    if metric_means:\n",
    "        combined_df = pd.DataFrame(metric_means).reindex(index=model_order)\n",
    "        fig_height = max(4, 0.4 * len(combined_df.index))\n",
    "        metric_cols = list(combined_df.columns)\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=len(metric_cols),\n",
    "            figsize=(1.6 * len(metric_cols), fig_height),\n",
    "            squeeze=False,\n",
    "        )\n",
    "        for idx, metric in enumerate(metric_cols):\n",
    "            ax = axes[0][idx]\n",
    "            col_data = combined_df[[metric]]\n",
    "            vmin_col, vmax_col = compute_heatmap_limits(col_data.values, lower_percentile=10.0, upper_percentile=95.0)\n",
    "            sns.heatmap(\n",
    "                col_data,\n",
    "                cmap=\"viridis\",\n",
    "                vmin=vmin_col,\n",
    "                vmax=vmax_col,\n",
    "                annot=True,\n",
    "                fmt=\".3f\",\n",
    "                linewidths=0.2,\n",
    "                linecolor=\"#f5f5f5\",\n",
    "                cbar=False,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(\"\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"Model\" if idx == 0 else \"\")\n",
    "            if idx == 0:\n",
    "                ax.yaxis.labelpad = 20\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "            if idx != 0:\n",
    "                ax.set_yticklabels([])\n",
    "        fig.suptitle(f\"{dataset} | Summary metrics (test means) | {title_suffix}\", fontsize=12)\n",
    "        sns.despine(fig, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        register_figure(FIGURES, f\"test_metric_mean_heatmap_combined_{dataset.lower()}\", fig)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Top-20 per-model gene blocks (per dataset)\n",
    "    if \"pearson\" in ds_test.columns:\n",
    "        test_pivot = ds_test.pivot_table(\n",
    "            index=\"gene\",\n",
    "            columns=\"model_display\",\n",
    "            values=\"pearson\",\n",
    "            aggfunc=\"mean\",\n",
    "        )\n",
    "        values = {}\n",
    "        annot = {}\n",
    "        for model_name in model_order:\n",
    "            if model_name not in test_pivot.columns:\n",
    "                continue\n",
    "            series = test_pivot[model_name].dropna().sort_values(ascending=False).head(20)\n",
    "            padded_values = series.tolist() + [float(\"nan\")] * (20 - len(series))\n",
    "            padded_genes = series.index.tolist() + [\"\"] * (20 - len(series))\n",
    "            values[model_name] = padded_values\n",
    "            annot[model_name] = [\"{}\\n{:.2f}\".format(g, v) if g else \"\" for g, v in zip(padded_genes, padded_values)]\n",
    "\n",
    "        if values:\n",
    "            heatmap_df = pd.DataFrame(values, index=[f\"rank {i}\" for i in range(1, 21)])\n",
    "            annot_df = pd.DataFrame(annot, index=heatmap_df.index)\n",
    "            fig_width = max(8, 1.2 * len(heatmap_df.columns))\n",
    "            fig_height = 10\n",
    "            fig_blocks, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "            sns.heatmap(\n",
    "                heatmap_df,\n",
    "                cmap=\"crest\",\n",
    "                annot=annot_df,\n",
    "                fmt=\"\",\n",
    "                linewidths=0.3,\n",
    "                linecolor=\"#f5f5f5\",\n",
    "                cbar_kws={\"label\": \"Test Pearson\"},\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(f\"{dataset} | Test Pearson (top 20 genes per model) | {title_suffix}\")\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Rank (per model)\")\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "            sns.despine(fig_blocks, left=True, bottom=True)\n",
    "            plt.tight_layout()\n",
    "            register_figure(FIGURES, f\"test_pearson_heatmap_top20_per_model_blocks_{dataset.lower()}\", fig_blocks)\n",
    "            display(fig_blocks)\n",
    "            plt.close(fig_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c3730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:03.113423Z",
     "iopub.status.busy": "2026-01-18T00:49:03.113201Z",
     "iopub.status.idle": "2026-01-18T00:49:03.482066Z",
     "shell.execute_reply": "2026-01-18T00:49:03.481456Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "\n",
    "fig_gap = None\n",
    "required_cols = {\"train_pearson_mean\", \"test_pearson_mean\"}\n",
    "subset = summary_reset[summary_reset[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])].copy()\n",
    "if subset.empty or not required_cols.issubset(subset.columns):\n",
    "    print(\"Train/test summary columns unavailable; skipping generalization gap plot.\")\n",
    "else:\n",
    "    fig_gap, axes = plt.subplots(ncols=2, figsize=(12, 6), sharex=True)\n",
    "    for ax, dataset in zip(axes, [\"Embryonic\", \"Endothelial\"]):\n",
    "        data = subset[subset[\"dataset\"] == dataset].copy()\n",
    "        if data.empty:\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"{dataset} (no data)\")\n",
    "            continue\n",
    "        ranked = data.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "        best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "        gap_df = best_per_model[[\"model_display\", \"model_id\", \"train_pearson_mean\", \"test_pearson_mean\"]].copy()\n",
    "        gap_df[\"generalization_gap\"] = gap_df[\"train_pearson_mean\"] - gap_df[\"test_pearson_mean\"]\n",
    "        # Sort ascending=True so low gaps are at top, high gaps at bottom\n",
    "        gap_df.sort_values(\"generalization_gap\", ascending=True, inplace=True)\n",
    "        colors = sns.color_palette(\"mako\", n_colors=len(gap_df))\n",
    "        bars = ax.barh(\n",
    "            gap_df[\"model_display\"],\n",
    "            gap_df[\"generalization_gap\"],\n",
    "            color=colors,\n",
    "            linewidth=0,\n",
    "        )\n",
    "        ax.axvline(0.0, color=\"#6d6d6d\", linestyle=\"--\", linewidth=1)\n",
    "        gap_min = gap_df[\"generalization_gap\"].min()\n",
    "        gap_max = gap_df[\"generalization_gap\"].max()\n",
    "        span = max(0.01, gap_max - gap_min)\n",
    "        margin = max(0.08, span * 0.12)\n",
    "        xmin = min(gap_min - margin, gap_min - 0.02)\n",
    "        xmin = min(xmin, -0.15)\n",
    "        xmax = max(gap_max + margin, 0.15)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_xlabel(\"Train - Test Mean Pearson\")\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        ax.set_title(dataset)\n",
    "        ax.invert_yaxis()\n",
    "        label_offset = (xmax - xmin) * 0.02\n",
    "        for bar, (_, row) in zip(bars, gap_df.iterrows()):\n",
    "            y = bar.get_y() + bar.get_height() / 2\n",
    "            value = row[\"generalization_gap\"]\n",
    "            if value >= 0:\n",
    "                ax.text(value + label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"#1f1f1f\")\n",
    "            else:\n",
    "                ax.text(value - label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"right\", fontsize=9, color=\"#1f1f1f\")\n",
    "        sns.despine(ax=ax, left=True, bottom=True)\n",
    "    fig_gap.suptitle(f\"Generalization Gap (Train - Test) | {title_suffix}\", fontsize=12)\n",
    "    fig_gap.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "\n",
    "register_figure(FIGURES, \"generalization_gap_by_dataset\", fig_gap)\n",
    "if fig_gap is not None:\n",
    "    display(fig_gap)\n",
    "    plt.close(fig_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8c038",
   "metadata": {},
   "source": [
    "## 10. Split Comparison by Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:03.487815Z",
     "iopub.status.busy": "2026-01-18T00:49:03.487637Z",
     "iopub.status.idle": "2026-01-18T00:49:04.628125Z",
     "shell.execute_reply": "2026-01-18T00:49:04.627402Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"].copy()\n",
    "metrics_long[\"dataset\"] = metrics_long[\"run_name\"].map(_assign_dataset)\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "\n",
    "splits_of_interest = [config.train_split, config.val_split, config.primary_split]\n",
    "subset = metrics_long[\n",
    "    metrics_long[\"split\"].isin(splits_of_interest)\n",
    "    & metrics_long[\"dataset\"].isin([\"Embryonic\", \"Endothelial\"])\n",
    "].copy()\n",
    "\n",
    "fig_split_compare = None\n",
    "if subset.empty or \"pearson\" not in subset:\n",
    "    print(\"Pearson metrics unavailable across requested splits; skipping split comparison plot.\")\n",
    "else:\n",
    "    subset = subset[[\"model_display\", \"split\", \"pearson\", \"dataset\"]].dropna()\n",
    "    split_labels = {\n",
    "        config.train_split: \"Train\",\n",
    "        config.val_split: \"Val\",\n",
    "        config.primary_split: \"Test\",\n",
    "    }\n",
    "    subset[\"split_label\"] = subset[\"split\"].map(split_labels).fillna(subset[\"split\"].str.title())\n",
    "\n",
    "    ordered_models = [model for model in model_display_order if model in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = subset[\"model_display\"].sort_values().unique().tolist()\n",
    "\n",
    "    # Build 6 hues per model: 3 splits per dataset with shade ramps\n",
    "    hue_order = [\n",
    "        \"Embryonic | Train\",\n",
    "        \"Embryonic | Val\",\n",
    "        \"Embryonic | Test\",\n",
    "        \"Endothelial | Train\",\n",
    "        \"Endothelial | Val\",\n",
    "        \"Endothelial | Test\",\n",
    "    ]\n",
    "    palette = {\n",
    "        \"Embryonic | Train\": \"#9ecae1\",\n",
    "        \"Embryonic | Val\": \"#6baed6\",\n",
    "        \"Embryonic | Test\": \"#3182bd\",\n",
    "        \"Endothelial | Train\": \"#fdd0a2\",\n",
    "        \"Endothelial | Val\": \"#fdae6b\",\n",
    "        \"Endothelial | Test\": \"#e6550d\",\n",
    "    }\n",
    "    subset[\"dataset_split\"] = subset[\"dataset\"] + \" | \" + subset[\"split_label\"]\n",
    "\n",
    "    fig_width = max(16, 1.2 * max(8, len(ordered_models)))\n",
    "    fig_split_compare, ax = plt.subplots(figsize=(fig_width, 7.2))\n",
    "    sns.boxplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset_split\",\n",
    "        order=ordered_models,\n",
    "        hue_order=hue_order,\n",
    "        palette=palette,\n",
    "        width=0.7,\n",
    "        fliersize=0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = subset.sample(min(len(subset), 5000), random_state=config.random_seed) if len(subset) > 5000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"dataset_split\",\n",
    "        order=ordered_models,\n",
    "        hue_order=hue_order,\n",
    "        palette=palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=2.4,\n",
    "        alpha=0.35,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.3,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    metric_min = subset[\"pearson\"].min()\n",
    "    metric_max = subset[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Pearson\")\n",
    "    ax.set_title(\"Per-gene Pearson by Split and Dataset\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Dataset | Split\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_split_compare, left=True, bottom=True)\n",
    "    fig_split_compare.tight_layout()\n",
    "\n",
    "register_figure(FIGURES, \"split_comparison_by_dataset\", fig_split_compare)\n",
    "if fig_split_compare is not None:\n",
    "    display(fig_split_compare)\n",
    "    plt.close(fig_split_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442dde0",
   "metadata": {},
   "source": [
    "## 11. Best Model Diagnostics per Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66852272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:04.643353Z",
     "iopub.status.busy": "2026-01-18T00:49:04.643107Z",
     "iopub.status.idle": "2026-01-18T00:49:05.580086Z",
     "shell.execute_reply": "2026-01-18T00:49:05.579474Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_feature_importance_table(model_dir: Path) -> tuple[pd.DataFrame, Optional[Path]]:\n",
    "    # Locate and standardise feature importances exported by the training pipeline.\n",
    "    patterns = [\n",
    "        \"feature_importance*.csv\",\n",
    "        \"feature_importances*.csv\",\n",
    "        \"feature_importance*.tsv\",\n",
    "        \"feature_importances*.tsv\",\n",
    "        \"feature_importance*.parquet\",\n",
    "        \"feature_importances*.parquet\",\n",
    "    ]\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns:\n",
    "        candidates.extend(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "    unique_candidates: list[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved in seen:\n",
    "            continue\n",
    "        if resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "    for candidate in sorted(unique_candidates):\n",
    "        try:\n",
    "            if candidate.suffix.lower() == \".parquet\":\n",
    "                df = pd.read_parquet(candidate)\n",
    "            else:\n",
    "                sep = \"\t\" if candidate.suffix.lower() in {\".tsv\", \".txt\"} else \",\"\n",
    "                df = pd.read_csv(candidate, sep=sep)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {candidate.name}: failed to load ({exc})\")\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        lower_cols = {col.lower(): col for col in df.columns}\n",
    "        feature_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"feature\", \"feature_name\", \"name\", \"variable\", \"feature_id\", \"column\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        importance_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\n",
    "                \"importance\",\n",
    "                \"importance_score\",\n",
    "                \"importance_mean\",\n",
    "                \"score\",\n",
    "                \"value\",\n",
    "                \"gain\",\n",
    "                \"weight\",\n",
    "            )\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        if feature_col is None or importance_col is None:\n",
    "            continue\n",
    "        out = df.copy()\n",
    "        out.rename(columns={feature_col: \"feature\", importance_col: \"importance\"}, inplace=True)\n",
    "        out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "        out[\"importance\"] = pd.to_numeric(out[\"importance\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"feature\", \"importance\"])\n",
    "        if out.empty:\n",
    "            continue\n",
    "        extra_cols = [col for col in out.columns if col not in {\"feature\", \"importance\"}]\n",
    "        out = out[[\"feature\", \"importance\", *extra_cols]]\n",
    "        out.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        return out.reset_index(drop=True), candidate\n",
    "    return pd.DataFrame(columns=[\"feature\", \"importance\"]), None\n",
    "\n",
    "\n",
    "def _best_model_for_dataset(summary_reset: pd.DataFrame, dataset: str) -> Optional[pd.Series]:\n",
    "    subset = summary_reset[summary_reset[\"dataset\"] == dataset].copy()\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    if \"test_pearson_mean\" in subset:\n",
    "        subset.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    return subset.iloc[0]\n",
    "\n",
    "\n",
    "def _ensure_distance_column(df: pd.DataFrame) -> tuple[pd.DataFrame, Optional[str], bool]:\n",
    "    if df is None or df.empty:\n",
    "        return df, None, False\n",
    "\n",
    "    signed_candidates = [\n",
    "        (\"delta_to_tss_kb\", True),\n",
    "        (\"distance_to_tss_kb\", True),\n",
    "        (\"delta_to_tss_bp\", False),\n",
    "        (\"distance_to_tss_bp\", False),\n",
    "    ]\n",
    "    for name, is_kb in signed_candidates:\n",
    "        if name in df.columns:\n",
    "            values = pd.to_numeric(df[name], errors=\"coerce\")\n",
    "            if (values.dropna() < 0).any():\n",
    "                return df, name, is_kb\n",
    "\n",
    "    def _infer_signed_distance(series: pd.Series) -> pd.Series:\n",
    "        import re\n",
    "        bin_pattern = re.compile(r\"bin_(-?\\d+)_to_(-?\\d+)\")\n",
    "        def _infer_distance(feature_name: str) -> Optional[float]:\n",
    "            if not isinstance(feature_name, str):\n",
    "                return None\n",
    "            token = feature_name.split(\"|\", 1)[-1]\n",
    "            match = bin_pattern.search(token)\n",
    "            if match:\n",
    "                start_bp = float(match.group(1))\n",
    "                end_bp = float(match.group(2))\n",
    "                return 0.5 * (start_bp + end_bp)\n",
    "            return None\n",
    "        return series.map(_infer_distance)\n",
    "\n",
    "    if \"feature\" in df.columns:\n",
    "        inferred = _infer_signed_distance(df[\"feature\"])\n",
    "        if inferred.notna().any():\n",
    "            df = df.copy()\n",
    "            df[\"distance_to_tss_bp_signed\"] = inferred\n",
    "            return df, \"distance_to_tss_bp_signed\", False\n",
    "\n",
    "    return df, None, False\n",
    "\n",
    "\n",
    "def _plot_feature_importance_distance(\n",
    "    df: pd.DataFrame,\n",
    "    distance_col: str,\n",
    "    distance_is_kb: bool,\n",
    "    title: str,\n",
    "    *,\n",
    "    max_distance_kb: float = 10.0,\n",
    "    y_limits: Optional[tuple[float, float]] = None,\n",
    "    show_scatter: bool = False,\n",
    ") -> Optional[plt.Figure]:\n",
    "    if df is None or df.empty or distance_col is None:\n",
    "        return None\n",
    "    valid = df.dropna(subset=[\"importance\", distance_col]).copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    distance_values = pd.to_numeric(valid[distance_col], errors=\"coerce\")\n",
    "    valid[\"distance_kb\"] = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "    valid = valid[valid[\"distance_kb\"].abs() <= max_distance_kb].copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    if not (valid[\"distance_kb\"] < 0).any():\n",
    "        print(\"Warning: no negative distance-to-TSS values detected; data may be absolute distances.\")\n",
    "    valid.sort_values(\"distance_kb\", inplace=True)\n",
    "\n",
    "    per_bin = (\n",
    "        valid.groupby(\"distance_kb\", sort=True)[\"importance\"]\n",
    "        .quantile(0.9)\n",
    "        .reset_index()\n",
    "    )\n",
    "    if per_bin.empty:\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "    if show_scatter:\n",
    "        sns.scatterplot(\n",
    "            data=valid,\n",
    "            x=\"distance_kb\",\n",
    "            y=\"importance\",\n",
    "            s=30,\n",
    "            alpha=0.45,\n",
    "            edgecolor=\"none\",\n",
    "            color=\"#4daf4a\",\n",
    "            ax=ax,\n",
    "        )\n",
    "    ax.plot(\n",
    "        per_bin[\"distance_kb\"],\n",
    "        per_bin[\"importance\"],\n",
    "        color=\"#e41a1c\",\n",
    "        linewidth=1.5,\n",
    "        label=\"90th percentile\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    ax.axvline(0.0, color=\"#999999\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlim(-max_distance_kb, max_distance_kb)\n",
    "    if y_limits is not None:\n",
    "        ax.set_ylim(y_limits)\n",
    "    ax.set_xlabel(\"Distance to TSS (kb)\")\n",
    "    ax.set_ylabel(\"Feature importance\")\n",
    "    ax.set_title(f\"{title} ({max_distance_kb:g} kb)\")\n",
    "    sns.despine(fig, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "    best_row = _best_model_for_dataset(summary_reset, dataset)\n",
    "    if best_row is None:\n",
    "        print(f\"No best model found for {dataset}.\")\n",
    "        continue\n",
    "    run_match = run_df[\n",
    "        (run_df[\"run_name\"] == best_row[\"run_name\"]) & (run_df[\"model_id\"] == best_row[\"model_id\"])\n",
    "    ]\n",
    "    if run_match.empty:\n",
    "        print(f\"Run metadata missing for {dataset} best model.\")\n",
    "        continue\n",
    "    row = run_match.iloc[0]\n",
    "    model_dir = row[\"model_path\"] if isinstance(row[\"model_path\"], Path) else Path(row[\"model_path\"])\n",
    "    scatter_path = model_dir / \"scatter_test.png\"\n",
    "    display(Markdown(\n",
    "        f\"**{dataset} best model:** `{best_row['model_display']}` (run `{best_row['run_name']}`)\"\n",
    "    ))\n",
    "    if scatter_path.exists():\n",
    "        display(Image(filename=str(scatter_path)))\n",
    "        EXTRA_EXPORT_PATHS.append((scatter_path, f\"{dataset.lower()}_scatter_test.png\"))\n",
    "    else:\n",
    "        preds_path = row.get(\"predictions_path\")\n",
    "        preds_path = preds_path if isinstance(preds_path, Path) else Path(preds_path) if preds_path else None\n",
    "        if preds_path is None or not preds_path.exists():\n",
    "            print(f\"No scatter or prediction file found for {dataset} best model.\")\n",
    "        else:\n",
    "            df = pd.read_csv(preds_path)\n",
    "            df = df[df[\"split\"] == config.primary_split]\n",
    "            if df.empty:\n",
    "                print(f\"No test predictions available for {dataset} best model.\")\n",
    "            else:\n",
    "                sample = df.sample(min(len(df), 8000), random_state=config.random_seed)\n",
    "                fig_scatter, ax = plt.subplots(figsize=(5.2, 5.2))\n",
    "                sns.scatterplot(\n",
    "                    data=sample,\n",
    "                    x=\"y_true\",\n",
    "                    y=\"y_pred\",\n",
    "                    s=14,\n",
    "                    alpha=0.4,\n",
    "                    edgecolor=\"none\",\n",
    "                    color=\"#377eb8\",\n",
    "                    ax=ax,\n",
    "                )\n",
    "                lims = [\n",
    "                    min(sample[\"y_true\"].min(), sample[\"y_pred\"].min()),\n",
    "                    max(sample[\"y_true\"].max(), sample[\"y_pred\"].max()),\n",
    "                ]\n",
    "                ax.plot(lims, lims, linestyle=\"--\", color=\"#6d6d6d\", linewidth=1)\n",
    "                ax.set_xlim(lims)\n",
    "                ax.set_ylim(lims)\n",
    "                ax.set_xlabel(\"Observed\")\n",
    "                ax.set_ylabel(\"Predicted\")\n",
    "                ax.set_title(f\"{dataset}: Test Scatter | {best_row['model_display']} | {title_suffix}\")\n",
    "                sns.despine(fig_scatter, left=True, bottom=True)\n",
    "                plt.tight_layout()\n",
    "                register_figure(FIGURES, f\"{dataset.lower()}_best_model_scatter\", fig_scatter)\n",
    "                display(fig_scatter)\n",
    "                plt.close(fig_scatter)\n",
    "\n",
    "    importance_df, importance_path = _load_feature_importance_table(model_dir)\n",
    "    if importance_df.empty:\n",
    "        print(f\"No feature importance table found for {dataset} best model.\")\n",
    "        continue\n",
    "    importance_df, distance_col, distance_is_kb = _ensure_distance_column(importance_df)\n",
    "    if distance_col is None or distance_col not in importance_df.columns:\n",
    "        print(f\"Unable to infer distance-to-TSS column for {dataset} best model.\")\n",
    "        continue\n",
    "    fig_distance = _plot_feature_importance_distance(\n",
    "        importance_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: Feature Importance vs Distance to TSS | {title_suffix}\",\n",
    "        max_distance_kb=10.0,\n",
    "        show_scatter=True,\n",
    "    )\n",
    "    zoom_mask = importance_df[distance_col].notna()\n",
    "    if zoom_mask.any():\n",
    "        distance_values = pd.to_numeric(importance_df.loc[zoom_mask, distance_col], errors=\"coerce\")\n",
    "        distance_kb = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "        zoom_range = distance_kb.abs() <= 5.0\n",
    "        zoom_importance = pd.to_numeric(importance_df.loc[zoom_mask, \"importance\"], errors=\"coerce\")\n",
    "        zoom_df = pd.DataFrame({\"distance_kb\": distance_kb[zoom_range], \"importance\": zoom_importance[zoom_range]})\n",
    "        if zoom_df.empty:\n",
    "            zoom_max = None\n",
    "        else:\n",
    "            zoom_max = zoom_df.groupby(\"distance_kb\")[\"importance\"].quantile(0.9).max()\n",
    "        zoom_y_limits = (0.0, float(zoom_max) * 1.15) if pd.notna(zoom_max) else None\n",
    "    else:\n",
    "        zoom_y_limits = None\n",
    "    fig_distance_zoomed = _plot_feature_importance_distance(\n",
    "        importance_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: Feature Importance vs Distance to TSS (zoomed) | {title_suffix}\",\n",
    "        max_distance_kb=5.0,\n",
    "        y_limits=zoom_y_limits,\n",
    "    )\n",
    "    if fig_distance is None or fig_distance_zoomed is None:\n",
    "        print(f\"No valid distance/importance data for {dataset} best model.\")\n",
    "    else:\n",
    "        model_short = MODEL_ID_TO_SHORT.get(best_row['model_id'], best_row['model_display'])\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_feature_importance_tss\", fig_distance)\n",
    "        display(fig_distance)\n",
    "        plt.close(fig_distance)\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_feature_importance_tss_zoomed\", fig_distance_zoomed)\n",
    "        display(fig_distance_zoomed)\n",
    "        plt.close(fig_distance_zoomed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72858cfb",
   "metadata": {},
   "source": [
    "## 12. Best Model SHAP Values per Dataset\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide model-agnostic feature importance.\n",
    "If SHAP values were computed during training (enable_shap=true), plot them vs TSS distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444e620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:05.584296Z",
     "iopub.status.busy": "2026-01-18T00:49:05.584132Z",
     "iopub.status.idle": "2026-01-18T00:49:05.604847Z",
     "shell.execute_reply": "2026-01-18T00:49:05.604262Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_shap_table(model_dir: Path) -> tuple[pd.DataFrame, Optional[Path]]:\n",
    "    \"\"\"Load SHAP importance table from model directory.\"\"\"\n",
    "    patterns = [\n",
    "        \"shap_importance*.csv\",\n",
    "        \"shap_importances*.csv\",\n",
    "    ]\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns:\n",
    "        candidates.extend(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "    \n",
    "    unique_candidates: list[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved in seen or resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "    \n",
    "    for candidate in sorted(unique_candidates):\n",
    "        try:\n",
    "            df = pd.read_csv(candidate)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {candidate.name}: failed to load ({exc})\")\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Check for required columns\n",
    "        lower_cols = {col.lower(): col for col in df.columns}\n",
    "        feature_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"feature\", \"feature_name\", \"name\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        shap_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"shap_mean_abs\", \"shap_value\", \"shap\", \"shap_importance\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        \n",
    "        if feature_col is None or shap_col is None:\n",
    "            continue\n",
    "        \n",
    "        out = df.copy()\n",
    "        out.rename(columns={feature_col: \"feature\", shap_col: \"shap_value\"}, inplace=True)\n",
    "        out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "        out[\"shap_value\"] = pd.to_numeric(out[\"shap_value\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"feature\", \"shap_value\"])\n",
    "        if out.empty:\n",
    "            continue\n",
    "        \n",
    "        extra_cols = [col for col in out.columns if col not in {\"feature\", \"shap_value\"}]\n",
    "        out = out[[\"feature\", \"shap_value\", *extra_cols]]\n",
    "        out.sort_values(\"shap_value\", ascending=False, inplace=True)\n",
    "        return out.reset_index(drop=True), candidate\n",
    "    \n",
    "    return pd.DataFrame(columns=[\"feature\", \"shap_value\"]), None\n",
    "\n",
    "\n",
    "def _plot_shap_vs_tss_distance(\n",
    "    df: pd.DataFrame,\n",
    "    distance_col: str,\n",
    "    distance_is_kb: bool,\n",
    "    title: str,\n",
    "    *,\n",
    "    max_distance_kb: float = 10.0,\n",
    "    y_limits: Optional[tuple[float, float]] = None,\n",
    "    show_scatter: bool = False,\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"Plot SHAP values vs TSS distance in manuscript style.\"\"\"\n",
    "    if df is None or df.empty or distance_col is None:\n",
    "        return None\n",
    "    valid = df.dropna(subset=[\"shap_value\", distance_col]).copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    distance_values = pd.to_numeric(valid[distance_col], errors=\"coerce\")\n",
    "    valid[\"distance_kb\"] = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "    valid = valid[valid[\"distance_kb\"].abs() <= max_distance_kb].copy()\n",
    "    if valid.empty:\n",
    "        return None\n",
    "    if not (valid[\"distance_kb\"] < 0).any():\n",
    "        print(\"Warning: no negative distance-to-TSS values detected; data may be absolute distances.\")\n",
    "    valid.sort_values(\"distance_kb\", inplace=True)\n",
    "\n",
    "    per_bin = (\n",
    "        valid.groupby(\"distance_kb\", sort=True)[\"shap_value\"]\n",
    "        .quantile(0.9)\n",
    "        .reset_index()\n",
    "    )\n",
    "    if per_bin.empty:\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "    if show_scatter:\n",
    "        sns.scatterplot(\n",
    "            data=valid,\n",
    "            x=\"distance_kb\",\n",
    "            y=\"shap_value\",\n",
    "            s=30,\n",
    "            alpha=0.45,\n",
    "            edgecolor=\"none\",\n",
    "            color=\"#4daf4a\",\n",
    "            ax=ax,\n",
    "        )\n",
    "    ax.plot(\n",
    "        per_bin[\"distance_kb\"],\n",
    "        per_bin[\"shap_value\"],\n",
    "        color=\"#e41a1c\",\n",
    "        linewidth=1.5,\n",
    "        label=\"90th percentile\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    ax.axvline(0.0, color=\"#999999\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlim(-max_distance_kb, max_distance_kb)\n",
    "    if y_limits is not None:\n",
    "        ax.set_ylim(y_limits)\n",
    "    ax.set_xlabel(\"Distance to TSS (kb)\")\n",
    "    ax.set_ylabel(\"SHAP value\")\n",
    "    ax.set_title(f\"{title} ({max_distance_kb:g} kb)\")\n",
    "    sns.despine(fig, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Plot SHAP values vs TSS distance for best models\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "shap_plots_created = False\n",
    "for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "    best_row = _best_model_for_dataset(summary_reset, dataset)\n",
    "    if best_row is None:\n",
    "        continue\n",
    "    run_match = run_df[\n",
    "        (run_df[\"run_name\"] == best_row[\"run_name\"]) & (run_df[\"model_id\"] == best_row[\"model_id\"])\n",
    "    ]\n",
    "    if run_match.empty:\n",
    "        continue\n",
    "    row = run_match.iloc[0]\n",
    "    model_dir = row[\"model_path\"] if isinstance(row[\"model_path\"], Path) else Path(row[\"model_path\"])\n",
    "    \n",
    "    shap_df, shap_path = _load_shap_table(model_dir)\n",
    "    if shap_df.empty:\n",
    "        continue\n",
    "    \n",
    "    display(Markdown(\n",
    "        f\"**{dataset} SHAP values:** `{best_row['model_display']}` (run `{best_row['run_name']}`)\"\n",
    "    ))\n",
    "    \n",
    "    shap_df, distance_col, distance_is_kb = _ensure_distance_column(shap_df)\n",
    "    if distance_col is None or distance_col not in shap_df.columns:\n",
    "        print(f\"Unable to infer distance-to-TSS column for {dataset} SHAP values.\")\n",
    "        continue\n",
    "    \n",
    "    fig_shap = _plot_shap_vs_tss_distance(\n",
    "        shap_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: SHAP Values vs Distance to TSS | {title_suffix}\",\n",
    "        max_distance_kb=10.0,\n",
    "        show_scatter=True,\n",
    "    )\n",
    "    \n",
    "    # Zoomed version (5kb)\n",
    "    zoom_mask = shap_df[distance_col].notna()\n",
    "    if zoom_mask.any():\n",
    "        distance_values = pd.to_numeric(shap_df.loc[zoom_mask, distance_col], errors=\"coerce\")\n",
    "        distance_kb = distance_values if distance_is_kb else distance_values / 1_000.0\n",
    "        zoom_range = distance_kb.abs() <= 5.0\n",
    "        zoom_shap = pd.to_numeric(shap_df.loc[zoom_mask, \"shap_value\"], errors=\"coerce\")\n",
    "        zoom_df = pd.DataFrame({\"distance_kb\": distance_kb[zoom_range], \"shap_value\": zoom_shap[zoom_range]})\n",
    "        if zoom_df.empty:\n",
    "            zoom_max = None\n",
    "        else:\n",
    "            zoom_max = zoom_df.groupby(\"distance_kb\")[\"shap_value\"].quantile(0.9).max()\n",
    "        zoom_y_limits = (0.0, float(zoom_max) * 1.15) if pd.notna(zoom_max) else None\n",
    "    else:\n",
    "        zoom_y_limits = None\n",
    "    \n",
    "    fig_shap_zoomed = _plot_shap_vs_tss_distance(\n",
    "        shap_df,\n",
    "        distance_col,\n",
    "        distance_is_kb,\n",
    "        f\"{dataset}: SHAP Values vs Distance to TSS (zoomed) | {title_suffix}\",\n",
    "        max_distance_kb=5.0,\n",
    "        y_limits=zoom_y_limits,\n",
    "    )\n",
    "    \n",
    "    if fig_shap is None or fig_shap_zoomed is None:\n",
    "        print(f\"No valid distance/SHAP data for {dataset} best model.\")\n",
    "    else:\n",
    "        model_short = MODEL_ID_TO_SHORT.get(best_row['model_id'], best_row['model_display'])\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_shap_tss\", fig_shap)\n",
    "        display(fig_shap)\n",
    "        plt.close(fig_shap)\n",
    "        register_figure(FIGURES, f\"{dataset.lower()}_{model_short}_shap_tss_zoomed\", fig_shap_zoomed)\n",
    "        display(fig_shap_zoomed)\n",
    "        plt.close(fig_shap_zoomed)\n",
    "        shap_plots_created = True\n",
    "\n",
    "if not shap_plots_created:\n",
    "    print(\"No SHAP values found. Ensure models were trained with enable_shap=true.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20610fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:05.607378Z",
     "iopub.status.busy": "2026-01-18T00:49:05.607230Z",
     "iopub.status.idle": "2026-01-18T00:49:05.646193Z",
     "shell.execute_reply": "2026-01-18T00:49:05.645122Z"
    }
   },
   "outputs": [],
   "source": [
    "run_df = analysis_state[\"run_df\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "# Plot training curves for best models\n",
    "fig_training_curves = None\n",
    "training_data_found = False\n",
    "\n",
    "for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "    best_row = _best_model_for_dataset(summary_reset, dataset)\n",
    "    if best_row is None:\n",
    "        continue\n",
    "    run_match = run_df[\n",
    "        (run_df[\"run_name\"] == best_row[\"run_name\"]) & (run_df[\"model_id\"] == best_row[\"model_id\"])\n",
    "    ]\n",
    "    if run_match.empty:\n",
    "        continue\n",
    "    row = run_match.iloc[0]\n",
    "    history_path = row.get(\"training_history_path\")\n",
    "    history_path = history_path if isinstance(history_path, Path) else Path(history_path) if history_path else None\n",
    "    \n",
    "    if history_path is None or not history_path.exists():\n",
    "        continue\n",
    "    \n",
    "    training_data_found = True\n",
    "    try:\n",
    "        history_df = pd.read_csv(history_path)\n",
    "        if history_df.empty or not {\"loss\", \"epoch\"}.issubset(history_df.columns):\n",
    "            continue\n",
    "        \n",
    "        if fig_training_curves is None:\n",
    "            fig_training_curves, axes = plt.subplots(ncols=2, figsize=(13, 5))\n",
    "        \n",
    "        ax = axes[0] if dataset == \"Embryonic\" else axes[1]\n",
    "        \n",
    "        # Plot loss curves\n",
    "        if \"loss\" in history_df:\n",
    "            ax.plot(history_df[\"epoch\"], history_df[\"loss\"], label=\"Train Loss\", linewidth=2, color=\"#1f77b4\")\n",
    "        if \"val_loss\" in history_df:\n",
    "            ax.plot(history_df[\"epoch\"], history_df[\"val_loss\"], label=\"Val Loss\", linewidth=2, color=\"#ff7f0e\")\n",
    "        \n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(f\"{dataset}: {best_row['model_display']} Training\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        sns.despine(ax=ax, left=True, bottom=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load training history for {dataset}: {e}\")\n",
    "\n",
    "if fig_training_curves is not None:\n",
    "    fig_training_curves.tight_layout()\n",
    "    register_figure(FIGURES, \"training_curves\", fig_training_curves)\n",
    "    display(fig_training_curves)\n",
    "    plt.close(fig_training_curves)\n",
    "elif training_data_found:\n",
    "    print(\"Training history available but could not generate combined plot.\")\n",
    "else:\n",
    "    print(\"No training history files found in model outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d110c6",
   "metadata": {},
   "source": [
    "## 13. Top/Bottom Genes per Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c006d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:05.649880Z",
     "iopub.status.busy": "2026-01-18T00:49:05.649469Z",
     "iopub.status.idle": "2026-01-18T00:49:17.022413Z",
     "shell.execute_reply": "2026-01-18T00:49:17.021687Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_wide = analysis_state[\"metrics_wide\"].copy()\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "# Top and bottom performing genes - SPLIT BY DATASET\n",
    "if not metrics_wide.empty and f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    metrics_wide_with_dataset = metrics_wide.copy()\n",
    "    metrics_wide_with_dataset[\"dataset\"] = metrics_wide_with_dataset[\"run_name\"].map(_assign_dataset)\n",
    "    \n",
    "    for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "        dataset_metrics = metrics_wide_with_dataset[\n",
    "            metrics_wide_with_dataset[\"dataset\"] == dataset\n",
    "        ].copy()\n",
    "        \n",
    "        if dataset_metrics.empty:\n",
    "            print(f\"No data available for {dataset} gene extremes.\")\n",
    "            continue\n",
    "        \n",
    "        gene_stats = dataset_metrics.groupby(\"gene\")[f\"{config.primary_split}_pearson\"].agg([\"mean\", \"std\", \"count\"])\n",
    "        gene_stats = gene_stats.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "        n_top = min(20, len(gene_stats) // 2)\n",
    "        if n_top > 0:\n",
    "            top_genes = gene_stats.head(n_top)\n",
    "            bottom_genes = gene_stats.tail(n_top)\n",
    "            extremes = pd.concat([top_genes, bottom_genes])\n",
    "            extremes[\"category\"] = [\"Top\"] * len(top_genes) + [\"Bottom\"] * len(bottom_genes)\n",
    "\n",
    "            fig_gene_extremes, ax = plt.subplots(figsize=(11, 8))\n",
    "            colors = [\"#2ecc71\" if c == \"Top\" else \"#e74c3c\" for c in extremes[\"category\"]]\n",
    "\n",
    "            y_pos = np.arange(len(extremes))\n",
    "            ax.barh(y_pos, extremes[\"mean\"], xerr=extremes[\"std\"], color=colors, capsize=4, alpha=0.85)\n",
    "\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(extremes.index, fontsize=9)\n",
    "            ax.set_xlabel(\"Mean Test Pearson (std)\")\n",
    "            ax.set_title(f\"{dataset}: Top and Bottom 20 Genes Across All Models\")\n",
    "            ax.invert_yaxis()\n",
    "            sns.despine(fig_gene_extremes, left=True, bottom=True)\n",
    "            fig_gene_extremes.tight_layout()\n",
    "            register_figure(FIGURES, f\"gene_extremes_{dataset.lower()}\", fig_gene_extremes)\n",
    "            display(fig_gene_extremes)\n",
    "            plt.close(fig_gene_extremes)\n",
    "        else:\n",
    "            print(f\"Insufficient genes for {dataset} extremes plot.\")\n",
    "else:\n",
    "    print(\"Test Pearson per-gene data unavailable for gene extremes.\")\n",
    "\n",
    "# Prediction error distribution by gene (best model per dataset) - SPLIT BY DATASET\n",
    "if not metrics_wide.empty and f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "        subset = summary_reset[summary_reset[\"dataset\"] == dataset].copy()\n",
    "        if subset.empty:\n",
    "            print(f\"{dataset}: no data available for gene pearson distribution\")\n",
    "            continue\n",
    "        subset = subset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "        best_row = subset.iloc[0]\n",
    "        model_mask = (\n",
    "            (metrics_wide[\"run_name\"] == best_row[\"run_name\"])\n",
    "            & (metrics_wide[\"model_id\"] == best_row[\"model_id\"])\n",
    "        )\n",
    "        best_metrics = metrics_wide[model_mask]\n",
    "        if best_metrics.empty:\n",
    "            print(f\"{dataset}: missing metrics for best model\")\n",
    "            continue\n",
    "        gene_pearson = best_metrics.groupby(\"gene\")[f\"{config.primary_split}_pearson\"].mean()\n",
    "        \n",
    "        fig_pred_error, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.hist(gene_pearson, bins=max(10, len(gene_pearson) // 3), color=\"#3498db\", alpha=0.7, edgecolor=\"black\")\n",
    "        ax.axvline(gene_pearson.mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {gene_pearson.mean():.3f}\")\n",
    "        ax.axvline(gene_pearson.median(), color=\"orange\", linestyle=\"--\", linewidth=2, label=f\"Median: {gene_pearson.median():.3f}\")\n",
    "        model_label = best_row[\"model_display\"]\n",
    "        ax.set_title(f\"{dataset} Best Model: Per-Gene Test Pearson Distribution\\n({model_label})\")\n",
    "        ax.set_xlabel(\"Mean Test Pearson per Gene\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend(fontsize=9)\n",
    "        sns.despine(fig_pred_error, left=True, bottom=True)\n",
    "        fig_pred_error.tight_layout()\n",
    "        register_figure(FIGURES, f\"gene_pearson_distribution_{dataset.lower()}\", fig_pred_error)\n",
    "        display(fig_pred_error)\n",
    "        plt.close(fig_pred_error)\n",
    "else:\n",
    "    print(\"Test Pearson per-gene data unavailable for gene pearson distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959aa5f0",
   "metadata": {},
   "source": [
    "## 14. Cross-Dataset Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c75e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:17.387161Z",
     "iopub.status.busy": "2026-01-18T00:49:17.386987Z",
     "iopub.status.idle": "2026-01-18T00:49:17.402552Z",
     "shell.execute_reply": "2026-01-18T00:49:17.401971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-dataset performance comparison\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "fig_cross_dataset = None\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "if not summary_reset.empty and \"test_pearson_mean\" in summary_reset:\n",
    "    cross_perf_mean = summary_reset.pivot_table(\n",
    "        index=\"model_display\",\n",
    "        columns=\"dataset\",\n",
    "        values=\"test_pearson_mean\",\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "    cross_perf_std = summary_reset.pivot_table(\n",
    "        index=\"model_display\",\n",
    "        columns=\"dataset\",\n",
    "        values=\"test_pearson_mean\",\n",
    "        aggfunc=\"std\",\n",
    "    )\n",
    "\n",
    "    if not cross_perf_mean.empty and len(cross_perf_mean.columns) > 1:\n",
    "        cross_perf_mean = cross_perf_mean.dropna(how=\"any\")\n",
    "        cross_perf_std = cross_perf_std.reindex(cross_perf_mean.index)\n",
    "\n",
    "        if not cross_perf_mean.empty and len(cross_perf_mean) > 1:\n",
    "            cross_perf_mean[\"mean\"] = cross_perf_mean.mean(axis=1)\n",
    "            cross_perf_mean = cross_perf_mean.sort_values(\"mean\", ascending=False).drop(\"mean\", axis=1)\n",
    "            cross_perf_std = cross_perf_std.reindex(cross_perf_mean.index)\n",
    "\n",
    "            fig_cross_dataset, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "            x = np.arange(len(cross_perf_mean))\n",
    "            width = 0.35\n",
    "\n",
    "            colors = [\"#3498db\", \"#e74c3c\"]\n",
    "            for i, col in enumerate(cross_perf_mean.columns):\n",
    "                yerr = cross_perf_std[col] if col in cross_perf_std.columns else None\n",
    "                ax.bar(\n",
    "                    x + i * width,\n",
    "                    cross_perf_mean[col],\n",
    "                    width,\n",
    "                    label=col,\n",
    "                    color=colors[i],\n",
    "                    alpha=0.8,\n",
    "                    yerr=yerr,\n",
    "                    capsize=4,\n",
    "                    error_kw={\"elinewidth\": 1, \"ecolor\": \"#333333\"},\n",
    "                )\n",
    "\n",
    "            ax.set_ylabel(\"Mean Test Pearson\")\n",
    "            ax.set_title(f\"Cross-Dataset Model Performance Comparison | {title_suffix}\")\n",
    "            ax.set_xticks(x + width / 2)\n",
    "            ax.set_xticklabels([to_short_name(mid) for mid in cross_perf_mean.index], rotation=45, ha=\"right\")\n",
    "            ax.legend()\n",
    "            ax.grid(axis=\"y\", alpha=0.3)\n",
    "            sns.despine(fig_cross_dataset, left=True, bottom=True)\n",
    "            fig_cross_dataset.tight_layout()\n",
    "            register_figure(FIGURES, \"cross_dataset_performance\", fig_cross_dataset)\n",
    "            display(fig_cross_dataset)\n",
    "            plt.close(fig_cross_dataset)\n",
    "        else:\n",
    "            print(\"Insufficient models present in both datasets for cross-dataset analysis.\")\n",
    "    else:\n",
    "        print(\"Cross-dataset comparison unavailable (single dataset or incomplete data).\")\n",
    "else:\n",
    "    print(\"Summary data unavailable for cross-dataset analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bdc40",
   "metadata": {},
   "source": [
    "## 15. Model Ranking Stability Across Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb8a2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:49:17.405132Z",
     "iopub.status.busy": "2026-01-18T00:49:17.404977Z",
     "iopub.status.idle": "2026-01-18T00:51:56.919183Z",
     "shell.execute_reply": "2026-01-18T00:51:56.918261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model ranking stability across splits (per dataset)\n",
    "summary_reset = analysis_state[\"summary_reset\"].copy()\n",
    "summary_reset[\"dataset\"] = summary_reset[\"run_name\"].map(_assign_dataset)\n",
    "\n",
    "title_suffix = RUN_SUBSET_DESCRIPTION or f\"{GENE_COUNT} genes\"\n",
    "\n",
    "if not summary_reset.empty and \"test_pearson_mean\" in summary_reset and \"val_pearson_mean\" in summary_reset:\n",
    "    for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "        rank_data = summary_reset[summary_reset[\"dataset\"] == dataset][[\"model_display\", \"test_pearson_mean\", \"val_pearson_mean\"]].copy()\n",
    "        rank_data = rank_data.dropna()\n",
    "        if rank_data.empty or len(rank_data) <= 2:\n",
    "            continue\n",
    "        rank_data[\"test_rank\"] = rank_data[\"test_pearson_mean\"].rank(ascending=False)\n",
    "        rank_data[\"val_rank\"] = rank_data[\"val_pearson_mean\"].rank(ascending=False)\n",
    "        rank_data = rank_data.sort_values(\"test_rank\")\n",
    "\n",
    "        fig_ranking, ax = plt.subplots(figsize=(10, 6))\n",
    "        x = np.arange(len(rank_data))\n",
    "        width = 0.35\n",
    "        ax.bar(x - width/2, rank_data[\"test_rank\"], width, label=\"Test Set Rank\", color=\"#2ecc71\", alpha=0.8)\n",
    "        ax.bar(x + width/2, rank_data[\"val_rank\"], width, label=\"Val Set Rank\", color=\"#f39c12\", alpha=0.8)\n",
    "        ax.set_ylabel(\"Rank (lower is better)\")\n",
    "        ax.set_title(f\"{dataset} | Model Ranking Stability (Test vs Val) | {title_suffix}\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([to_short_name(name) for name in rank_data[\"model_display\"]], rotation=45, ha=\"right\")\n",
    "        ax.invert_yaxis()\n",
    "        ax.legend()\n",
    "        sns.despine(fig_ranking, left=True, bottom=True)\n",
    "        fig_ranking.tight_layout()\n",
    "        register_figure(FIGURES, f\"model_ranking_stability_{dataset.lower()}\", fig_ranking)\n",
    "        display(fig_ranking)\n",
    "        plt.close(fig_ranking)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029d540",
   "metadata": {},
   "source": [
    "## 16. Model Complexity vs Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity vs performance (simple plot)\n",
    "fig_complexity = None\n",
    "if not summary_reset.empty and \"test_pearson_mean\" in summary_reset:\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    complexity_data = summary_reset.copy()\n",
    "\n",
    "    if \"param_count\" not in complexity_data.columns:\n",
    "        param_count_path = config.project_root / \"analysis\" / \"param_count.csv\"\n",
    "        if param_count_path.exists():\n",
    "            try:\n",
    "                param_count_df = pd.read_csv(param_count_path)\n",
    "            except Exception:\n",
    "                param_count_df = None\n",
    "            if isinstance(param_count_df, pd.DataFrame) and not param_count_df.empty:\n",
    "                expected = {\"model_display\", \"model_id\", \"dataset\", \"param_count\"}\n",
    "                if expected.issubset(param_count_df.columns):\n",
    "                    param_count_df = param_count_df.drop_duplicates([\"model_id\", \"dataset\"])\n",
    "                    complexity_data = complexity_data.merge(\n",
    "                        param_count_df[[\"model_id\", \"dataset\", \"param_count\"]],\n",
    "                        on=[\"model_id\", \"dataset\"],\n",
    "                        how=\"left\",\n",
    "                    )\n",
    "        if \"param_count\" not in complexity_data.columns:\n",
    "            complexity_data[\"param_count\"] = 1\n",
    "\n",
    "    if complexity_data[\"param_count\"].isna().any() and isinstance(run_df, pd.DataFrame) and not run_df.empty:\n",
    "        def _select_model_path(model_dir: Path) -> Path | None:\n",
    "            if not model_dir or not model_dir.exists():\n",
    "                return None\n",
    "            candidates = [\n",
    "                p for p in model_dir.glob(\"model.*\")\n",
    "                if p.is_file() and \"scaler\" not in p.name.lower()\n",
    "            ]\n",
    "            if not candidates:\n",
    "                patterns = (\"*.pkl\", \"*.joblib\", \"*.sav\", \"*.pt\", \"*.pth\", \"*.cbm\", \"*.json\", \"*.bst\")\n",
    "                for pattern in patterns:\n",
    "                    candidates.extend(\n",
    "                        p for p in model_dir.glob(pattern)\n",
    "                        if p.is_file() and \"scaler\" not in p.name.lower()\n",
    "                    )\n",
    "            if not candidates:\n",
    "                return None\n",
    "            candidates.sort(key=lambda p: (p.name != \"model.pkl\", p.name))\n",
    "            return candidates[0]\n",
    "\n",
    "        def _approx_param_count(model_path: Path | None) -> int:\n",
    "            if not model_path or not model_path.exists():\n",
    "                return 1\n",
    "            try:\n",
    "                return max(1, int(model_path.stat().st_size))\n",
    "            except Exception:\n",
    "                return 1\n",
    "\n",
    "        approx_counts = []\n",
    "        for _, row in complexity_data.iterrows():\n",
    "            count = row.get(\"param_count\")\n",
    "            if pd.notna(count):\n",
    "                approx_counts.append(int(count))\n",
    "                continue\n",
    "            match = run_df[(run_df[\"run_name\"] == row[\"run_name\"]) & (run_df[\"model_id\"] == row[\"model_id\"])]\n",
    "            if match.empty:\n",
    "                approx_counts.append(1)\n",
    "                continue\n",
    "            model_dir = match.iloc[0][\"model_path\"]\n",
    "            model_dir = model_dir if isinstance(model_dir, Path) else Path(model_dir)\n",
    "            model_path = _select_model_path(model_dir)\n",
    "            approx_counts.append(_approx_param_count(model_path))\n",
    "        complexity_data[\"param_count\"] = approx_counts\n",
    "\n",
    "    param_table = (\n",
    "        complexity_data[[\"model_display\", \"model_id\", \"dataset\", \"param_count\", \"test_pearson_mean\"]]\n",
    "        .drop_duplicates([\"model_id\", \"dataset\"])\n",
    "        .sort_values([\"dataset\", \"param_count\"], ascending=[True, False])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    display(param_table)\n",
    "\n",
    "    fig_complexity, ax = plt.subplots(figsize=(22, 12))\n",
    "    colors = {\"Embryonic\": \"#4C78A8\", \"Endothelial\": \"#F58518\"}\n",
    "\n",
    "    label_nudges = {\n",
    "        # (model_id, dataset): (dx, dy) in points\n",
    "        (\"elastic_net\", \"Embryonic\"): (9, 9),\n",
    "        (\"lasso\", \"Embryonic\"): (9, -9),\n",
    "        (\"extra_trees\", \"Endothelial\"): (-55, 0),\n",
    "        (\"hist_gradient_boosting\", \"Embryonic\"): (6, 6),\n",
    "        (\"cnn\", \"Endothelial\"): (9, -9),\n",
    "        (\"random_forest\", \"Embryonic\"): (12, 0),\n",
    "        (\"xgboost\", \"Embryonic\"): (9, 7),\n",
    "        (\"ridge\", \"Embryonic\"): (9, 9),\n",
    "        (\"ols\", \"Embryonic\"): (9, -9),\n",
    "        (\"ols\", \"Endothelial\"): (9, 3),\n",
    "        (\"ridge\", \"Endothelial\"): (9, -9),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for dataset in [\"Embryonic\", \"Endothelial\"]:\n",
    "        subset = complexity_data[complexity_data[\"dataset\"] == dataset].copy()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        ax.scatter(\n",
    "            subset[\"param_count\"],\n",
    "            subset[\"test_pearson_mean\"],\n",
    "            s=110,\n",
    "            alpha=0.8,\n",
    "            color=colors.get(dataset, \"#666666\"),\n",
    "            edgecolors=\"black\",\n",
    "            linewidth=0.6,\n",
    "            label=dataset,\n",
    "        )\n",
    "        default_dx, default_dy = 6, 6\n",
    "        for row in subset.itertuples(index=False):\n",
    "            label_key = (getattr(row, \"model_id\"), dataset)\n",
    "            dx, dy = label_nudges.get(label_key, (default_dx, default_dy))\n",
    "            ax.annotate(\n",
    "                to_short_name(getattr(row, \"model_display\")),\n",
    "                (getattr(row, \"param_count\"), getattr(row, \"test_pearson_mean\")),\n",
    "                xytext=(dx, dy),\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,\n",
    "                alpha=0.9,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.15\", facecolor=\"white\", edgecolor=\"none\", alpha=0.55),\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Parameter count (approx; log scale)\")\n",
    "    ax.set_ylabel(\"Mean Test Pearson\")\n",
    "    ax.set_title(f\"Model Complexity vs Performance | {title_suffix}\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"both\")\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    sns.despine(fig_complexity, left=True, bottom=True)\n",
    "    fig_complexity.subplots_adjust(left=0.08, right=0.96, top=0.90, bottom=0.15)\n",
    "    register_figure(FIGURES, \"complexity_vs_performance\", fig_complexity)\n",
    "    display(fig_complexity)\n",
    "    plt.close(fig_complexity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01110590",
   "metadata": {},
   "source": [
    "## 17. Export Figures\n",
    "\n",
    "Save all generated matplotlib figures to the manuscript figure directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e735d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T00:51:56.929089Z",
     "iopub.status.busy": "2026-01-18T00:51:56.928941Z",
     "iopub.status.idle": "2026-01-18T00:52:25.274393Z",
     "shell.execute_reply": "2026-01-18T00:52:25.273021Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "project_root = Path(analysis_metadata.get(\"project_root\", config.project_root))\n",
    "fig_dir_raw = analysis_metadata.get(\"fig_dir\", config.fig_dir)\n",
    "fig_dir = Path(fig_dir_raw)\n",
    "if not fig_dir.is_absolute():\n",
    "    fig_dir = project_root / fig_dir\n",
    "fig_dir = fig_dir.resolve()\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exported = []\n",
    "if not FIGURES:\n",
    "    print(\"No figures registered for export.\")\n",
    "else:\n",
    "    for key, fig in FIGURES.items():\n",
    "        if fig is None:\n",
    "            continue\n",
    "        out_path = fig_dir / f\"{key}.png\"\n",
    "        fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "        exported.append(out_path)\n",
    "\n",
    "if EXTRA_EXPORT_PATHS:\n",
    "    for src_path, out_name in EXTRA_EXPORT_PATHS:\n",
    "        src_path = Path(src_path)\n",
    "        if not src_path.exists():\n",
    "            continue\n",
    "        out_path = fig_dir / out_name\n",
    "        shutil.copy2(src_path, out_path)\n",
    "        exported.append(out_path)\n",
    "\n",
    "if not exported:\n",
    "    print(\"No figures were exported.\")\n",
    "else:\n",
    "    print(f\"Exported {len(exported)} figures to {fig_dir}\")\n",
    "    for path in exported:\n",
    "        print(f\"- {path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grn_ml_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
