{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186d0309",
   "metadata": {},
   "source": [
    "# SPEAR Results Analysis\n",
    "\n",
    "Single-cell Prediction of gene Expression from ATAC-seq Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeff9a1",
   "metadata": {},
   "source": [
    "- Summary figures: split comparison, test heatmaps, distribution profiles, train+val dataset comparison, generalization gap.\n",
    "- Best-model feature importance diagnostics and export helpers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a06e0",
   "metadata": {},
   "source": [
    "## Prereqs\n",
    "\n",
    "- Place run outputs under `output/results/spear_results/` (one subfolder per run with `models/` and metrics CSVs).\n",
    "\n",
    "- Ensure logs exist under `output/logs/` with `spear_*` naming if you want resource plots.\n",
    "\n",
    "- Keep `analysis/model_name_lookup.tsv` present (already tracked in repo).\n",
    "\n",
    "- Install dependencies (see README) and select the `spear_env` kernel.\n",
    "\n",
    "### How to run\n",
    "\n",
    "1. Open this notebook inside the repo root.\n",
    "\n",
    "2. Adjust the run include globs if you want to filter; otherwise leave defaults.\n",
    "\n",
    "3. Run all cells top-to-bottom after outputs are in place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e751842",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Imports, plotting defaults, and global configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91ba28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:41.114507Z",
     "iopub.status.busy": "2025-12-30T16:09:41.114352Z",
     "iopub.status.idle": "2025-12-30T16:09:55.262698Z",
     "shell.execute_reply": "2025-12-30T16:09:55.262204Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "# Configure plotting defaults for consistent styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "sns.set_context(\"paper\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.dpi\": 160, \"savefig.dpi\": 320})\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.display.width = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983bfe1",
   "metadata": {},
   "source": [
    "## 2. Analysis Configuration\n",
    "\n",
    "Project paths, figure/report locations, and default analysis parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c7ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:55.264770Z",
     "iopub.status.busy": "2025-12-30T16:09:55.264521Z",
     "iopub.status.idle": "2025-12-30T16:09:55.273428Z",
     "shell.execute_reply": "2025-12-30T16:09:55.273012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Centralised configuration for the notebook run\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    project_root: Path\n",
    "    results_root: Path\n",
    "    lookup_path: Path\n",
    "    fig_dir: Path\n",
    "    reports_dir: Path\n",
    "    run_include_globs: tuple[str, ...] = (\"*\",)\n",
    "    run_exclude: tuple[str, ...] = tuple()\n",
    "    primary_split: str = \"test\"\n",
    "    val_split: str = \"val\"\n",
    "    train_split: str = \"train\"\n",
    "    top_gene_count: int = 15\n",
    "    top_model_count: int = 3\n",
    "    random_seed: int = 7\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Create required directories if they are missing.\"\"\"\n",
    "        self.fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.lookup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {\"analysis\", \"scripts\"}:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "config = AnalysisConfig(\n",
    "    project_root=project_root,\n",
    "    results_root=project_root / \"output\" / \"results\" / \"spear_results\",\n",
    "    lookup_path=project_root / \"analysis\" / \"model_name_lookup.tsv\",\n",
    "    fig_dir=project_root / \"analysis\" / \"figs\",\n",
    "    reports_dir=project_root / \"analysis\" / \"reports\",\n",
    "    run_include_globs=(\"*\",),\n",
    "    run_exclude=tuple(),\n",
    "    random_seed=7,\n",
    "    top_gene_count=15,\n",
    "    top_model_count=3,\n",
    " )\n",
    "\n",
    "if not config.results_root.exists():\n",
    "    raise FileNotFoundError(f\"Results directory missing: {config.results_root}\")\n",
    "if not config.lookup_path.exists():\n",
    "    # Seed the lookup table if it is missing so later steps can append to it.\n",
    "    pd.DataFrame({\n",
    "        \"model_id\": [],\n",
    "        \"model_display_name\": [],\n",
    "        \"model_short_name\": [],\n",
    "    }).to_csv(\n",
    "        config.lookup_path, sep=\"\\t\", index=False\n",
    "    )\n",
    "\n",
    "np.random.seed(config.random_seed)\n",
    "rng = np.random.default_rng(config.random_seed)\n",
    "FIGURES: dict[str, plt.Figure] = {}\n",
    "TABLES: dict[str, pd.DataFrame] = {}\n",
    "analysis_metadata: dict[str, object] = {\n",
    "    \"generated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"project_root\": config.project_root,\n",
    "    \"results_root\": config.results_root,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed0e8d",
   "metadata": {},
   "source": [
    "## 2a. Optional Run Selection Overrides\n",
    "\n",
    "Use these lists to focus the notebook on a subset of runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e70d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:55.274877Z",
     "iopub.status.busy": "2025-12-30T16:09:55.274727Z",
     "iopub.status.idle": "2025-12-30T16:09:55.279779Z",
     "shell.execute_reply": "2025-12-30T16:09:55.279389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Populate one or both of the lists below to narrow the analysis scope.\n",
    "RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_1000genes_k5_pg20_20251108_xgboost\",\n",
    "    # Path(\"/gpfs/.../output/results/spear_results/spear_1000genes_k5_pg20_20251108_random_forest\"),\n",
    "]\n",
    "RUN_GLOB_SELECTION: list[str] = [\n",
    "    \"spear_100genes_endothelial_compute*\",\n",
    "]\n",
    "RUN_SUBSET_LABEL: Optional[str] = \"endothelial_100genes\"  # e.g. \"1000genes_k5_pg20_window200kb\"\n",
    "RUN_SUBSET_DESCRIPTION: Optional[str] = \"Endothelial | 100 genes\"\n",
    "\n",
    "\n",
    "# --- Do not edit below unless you want to change the filtering logic itself. ---\n",
    "\n",
    "def _resolve_run_name(entry: str | Path | None) -> Optional[str]:\n",
    "    if entry is None:\n",
    "        return None\n",
    "    if isinstance(entry, Path):\n",
    "        candidate = entry\n",
    "    else:\n",
    "        text = str(entry).strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        candidate = Path(text)\n",
    "    candidate = candidate.expanduser()\n",
    "    if not candidate.is_absolute():\n",
    "        candidate = (config.results_root / candidate).resolve()\n",
    "    if candidate.is_file():\n",
    "        candidate = candidate.parent\n",
    "    if candidate.name == \"models\":\n",
    "        candidate = candidate.parent\n",
    "    return candidate.name or None\n",
    "\n",
    "\n",
    "manual_run_names = [\n",
    "    name for name in (_resolve_run_name(entry) for entry in RUN_DIRECTORY_SELECTION) if name\n",
    "]\n",
    "glob_patterns = [pattern.strip() for pattern in RUN_GLOB_SELECTION if pattern and pattern.strip()]\n",
    "combined_patterns = tuple(dict.fromkeys(glob_patterns + manual_run_names))\n",
    "\n",
    "if combined_patterns:\n",
    "    config = replace(\n",
    "        config,\n",
    "        run_include_globs=combined_patterns,\n",
    "    )\n",
    "    subset_label = (RUN_SUBSET_LABEL or \"\").strip() or None\n",
    "    subset_description = (RUN_SUBSET_DESCRIPTION or \"\").strip() or \", \".join(combined_patterns)\n",
    "    if subset_label:\n",
    "        config = replace(\n",
    "            config,\n",
    "            fig_dir=config.fig_dir / subset_label,\n",
    "            reports_dir=config.reports_dir / subset_label,\n",
    "        )\n",
    "    analysis_metadata.update({\n",
    "        \"subset_label\": subset_label,\n",
    "        \"subset_descriptor\": subset_description,\n",
    "        \"include_globs\": combined_patterns,\n",
    "    })\n",
    "else:\n",
    "    analysis_metadata.pop(\"subset_label\", None)\n",
    "    analysis_metadata.pop(\"subset_descriptor\", None)\n",
    "    analysis_metadata.pop(\"include_globs\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba28943",
   "metadata": {},
   "source": [
    "## 3. Run Discovery Utilities\n",
    "\n",
    "Helpers for locating model outputs and attaching display metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5345a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:55.281280Z",
     "iopub.status.busy": "2025-12-30T16:09:55.281135Z",
     "iopub.status.idle": "2025-12-30T16:09:55.308612Z",
     "shell.execute_reply": "2025-12-30T16:09:55.308170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Canonical representation of a single trained model output folder\n",
    "@dataclass(frozen=True)\n",
    "class RunRecord:\n",
    "    run_name: str\n",
    "    model_id: str\n",
    "    run_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: Optional[Path]\n",
    "    predictions_path: Optional[Path]\n",
    "    training_history_path: Optional[Path]\n",
    "    model_display: Optional[str] = None\n",
    "\n",
    "\n",
    "LOOKUP_SPECIAL_CASES = {\n",
    "    \"cnn\": \"Convolutional Neural Network\",\n",
    "    \"rnn\": \"Recurrent Neural Network\",\n",
    "    \"lstm\": \"Long Short-Term Memory\",\n",
    "    \"mlp\": \"Multilayer Perceptron\",\n",
    "    \"svr\": \"Support Vector Regressor\",\n",
    "    \"ols\": \"Ordinary Least Squares\",\n",
    "    \"xgboost\": \"XGBoost\",\n",
    "    \"catboost\": \"CatBoost\",\n",
    "    \"hist_gradient_boosting\": \"Histogram Gradient Boosting\",\n",
    "    \"extra_trees\": \"Extra Trees\",\n",
    "    \"random_forest\": \"Random Forest\",\n",
    "    \"elastic_net\": \"Elastic Net\",\n",
    "}\n",
    "\n",
    "SHORT_NAME_FALLBACKS = {\n",
    "    \"Multilayer Perceptron\": \"MLP\",\n",
    "    \"Graph Neural Network\": \"GNN\",\n",
    "    \"Convolutional Neural Network\": \"CNN\",\n",
    "    \"Long Short-Term Memory Network\": \"LSTM\",\n",
    "    \"Recurrent Neural Network\": \"RNN\",\n",
    "    \"Transformer Encoder\": \"Transformer\",\n",
    "    \"Ordinary Least Squares\": \"OLS\",\n",
    "    \"Extra Trees\": \"Extra Trees\",\n",
    "    \"Random Forest\": \"Random Forest\",\n",
    "    \"Ridge Regression\": \"Ridge\",\n",
    "}\n",
    "\n",
    "MODEL_ID_TO_DISPLAY: dict[str, str] = {}\n",
    "MODEL_ID_TO_SHORT: dict[str, str] = {}\n",
    "MODEL_DISPLAY_TO_SHORT: dict[str, str] = {}\n",
    "\n",
    "\n",
    "def _default_short_name(display_name: str) -> str:\n",
    "    \"\"\"Generate a lightweight abbreviation when none is provided.\"\"\"\n",
    "    if not isinstance(display_name, str) or not display_name.strip():\n",
    "        return \"\"\n",
    "    tokens = [token for token in display_name.replace(\"(\", \" \").replace(\")\", \" \").split() if token]\n",
    "    if not tokens:\n",
    "        return display_name\n",
    "    acronym = \"\".join(token[0] for token in tokens if token and token[0].isalnum()).upper()\n",
    "    if 1 < len(acronym) <= 5:\n",
    "        return acronym\n",
    "    return display_name\n",
    "\n",
    "\n",
    "def compute_heatmap_limits(\n",
    "    values: pd.DataFrame | np.ndarray,\n",
    "    lower_percentile: float = 5.0,\n",
    "    upper_percentile: float = 95.0,\n",
    "    clip: tuple[float, float] = (0.0, 1.0),\n",
    "    min_buffer: float = 0.01,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Derive consistent vmin/vmax bounds so heatmaps emphasise the dense value range.\"\"\"\n",
    "    data = np.asarray(values, dtype=float)\n",
    "    data = data[np.isfinite(data)]\n",
    "    if data.size == 0:\n",
    "        return clip\n",
    "    lower = np.percentile(data, lower_percentile)\n",
    "    upper = np.percentile(data, upper_percentile)\n",
    "    buffer = max(min_buffer, (upper - lower) * 0.05)\n",
    "    vmin = max(clip[0], lower - buffer)\n",
    "    vmax = min(clip[1], upper + buffer)\n",
    "    if np.isclose(vmin, vmax):\n",
    "        spread = max(min_buffer, abs(vmin) * 0.1 or min_buffer)\n",
    "        vmin = max(clip[0], vmin - spread)\n",
    "        vmax = min(clip[1], vmax + spread)\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "def to_short_name(name: str | None) -> str:\n",
    "    \"\"\"Return a concise display name for figure titles and filenames.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    if name in MODEL_DISPLAY_TO_SHORT:\n",
    "        return MODEL_DISPLAY_TO_SHORT[name]\n",
    "    return SHORT_NAME_FALLBACKS.get(name, name)\n",
    "\n",
    "\n",
    "def _read_lookup_table(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame({\n",
    "            \"model_id\": pd.Series(dtype=\"string\"),\n",
    "            \"model_display_name\": pd.Series(dtype=\"string\"),\n",
    "            \"model_short_name\": pd.Series(dtype=\"string\"),\n",
    "        })\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    expected = {\"model_id\", \"model_display_name\"}\n",
    "    missing_cols = expected.difference(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Lookup table missing required columns: {sorted(missing_cols)}\")\n",
    "    if \"model_short_name\" not in df.columns:\n",
    "        df[\"model_short_name\"] = df[\"model_display_name\"].map(_default_short_name)\n",
    "    else:\n",
    "        df[\"model_short_name\"] = df[\"model_short_name\"].fillna(\"\")\n",
    "        missing_short = df[\"model_short_name\"].str.strip() == \"\"\n",
    "        df.loc[missing_short, \"model_short_name\"] = df.loc[missing_short, \"model_display_name\"].map(_default_short_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _update_model_name_maps(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Cache name lookups for downstream plotting helpers.\"\"\"\n",
    "    global MODEL_ID_TO_DISPLAY, MODEL_ID_TO_SHORT, MODEL_DISPLAY_TO_SHORT\n",
    "    if df.empty:\n",
    "        MODEL_ID_TO_DISPLAY = {}\n",
    "        MODEL_ID_TO_SHORT = {}\n",
    "        MODEL_DISPLAY_TO_SHORT = {}\n",
    "        return\n",
    "    standardised = df.fillna(\"\")\n",
    "    MODEL_ID_TO_DISPLAY = {\n",
    "        row.model_id: row.model_display_name or _guess_display_name(row.model_id)\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_ID_TO_SHORT = {\n",
    "        row.model_id: (row.model_short_name or MODEL_ID_TO_DISPLAY[row.model_id])\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_DISPLAY_TO_SHORT = {\n",
    "        MODEL_ID_TO_DISPLAY[row.model_id]: MODEL_ID_TO_SHORT[row.model_id]\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "\n",
    "def _guess_display_name(model_id: str) -> str:\n",
    "    if model_id in LOOKUP_SPECIAL_CASES:\n",
    "        return LOOKUP_SPECIAL_CASES[model_id]\n",
    "    parts = [part for part in model_id.replace(\"-\", \" \").replace(\"_\", \" \").split(\" \") if part]\n",
    "    if not parts:\n",
    "        return model_id\n",
    "    formatted = []\n",
    "    for token in parts:\n",
    "        if len(token) <= 3:\n",
    "            formatted.append(token.upper())\n",
    "        else:\n",
    "            formatted.append(token.capitalize())\n",
    "    return \" \".join(formatted)\n",
    "\n",
    "\n",
    "def _matches_any(value: str, patterns: Iterable[str]) -> bool:\n",
    "    return any(fnmatch(value, pattern) for pattern in patterns) if patterns else False\n",
    "\n",
    "\n",
    "def _first_existing(path: Path, candidates: Sequence[str]) -> Optional[Path]:\n",
    "    for name in candidates:\n",
    "        candidate = path / name\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def discover_model_runs(\n",
    "    results_root: Path,\n",
    "    include_globs: Iterable[str],\n",
    "    exclude_patterns: Iterable[str],\n",
    ") -> list[RunRecord]:\n",
    "    results_root = Path(results_root)\n",
    "    if not results_root.exists():\n",
    "        raise FileNotFoundError(f\"Results root missing: {results_root}\")\n",
    "    records: list[RunRecord] = []\n",
    "    include = tuple(include_globs) if include_globs else (\"*\",)\n",
    "    exclude = tuple(exclude_patterns) if exclude_patterns else tuple()\n",
    "    for run_dir in sorted(results_root.iterdir()):\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "        run_name = run_dir.name\n",
    "        if not _matches_any(run_name, include):\n",
    "            continue\n",
    "        if exclude and _matches_any(run_name, exclude):\n",
    "            continue\n",
    "        models_dir = run_dir / \"models\"\n",
    "        if not models_dir.exists():\n",
    "            continue\n",
    "        for model_dir in sorted(models_dir.iterdir()):\n",
    "            if not model_dir.is_dir():\n",
    "                continue\n",
    "            model_id = model_dir.name\n",
    "            metrics_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"metrics_per_gene.csv\",\n",
    "                    \"metrics_by_gene.csv\",\n",
    "                    \"metrics_cv.csv\",\n",
    "                ),\n",
    "            )\n",
    "            predictions_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"predictions_raw.csv\",\n",
    "                    \"predictions.csv\",\n",
    "                ),\n",
    "            )\n",
    "            history_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"training_history.csv\",\n",
    "                    \"training_history_loss.csv\",\n",
    "                ),\n",
    "            )\n",
    "            records.append(\n",
    "                RunRecord(\n",
    "                    run_name=run_name,\n",
    "                    model_id=model_id,\n",
    "                    run_path=run_dir,\n",
    "                    model_path=model_dir,\n",
    "                    metrics_path=metrics_path,\n",
    "                    predictions_path=predictions_path,\n",
    "                    training_history_path=history_path,\n",
    "                )\n",
    "            )\n",
    "    return records\n",
    "\n",
    "\n",
    "def ensure_model_lookup(path: Path, model_ids: Iterable[str]) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    df = _read_lookup_table(path)\n",
    "    existing = set(df[\"model_id\"]) if not df.empty else set()\n",
    "    new_rows = []\n",
    "    for model_id in sorted(set(model_ids).difference(existing)):\n",
    "        display_name = _guess_display_name(model_id)\n",
    "        short_name = SHORT_NAME_FALLBACKS.get(display_name, _default_short_name(display_name))\n",
    "        new_rows.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"model_display_name\": display_name,\n",
    "                \"model_short_name\": short_name,\n",
    "            }\n",
    "        )\n",
    "    if new_rows:\n",
    "        additions = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, additions], ignore_index=True) if not df.empty else additions\n",
    "        df.sort_values(\"model_id\", inplace=True)\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def attach_lookup(records: Sequence[RunRecord], model_lookup: pd.DataFrame) -> list[RunRecord]:\n",
    "    if model_lookup.empty:\n",
    "        return list(records)\n",
    "    display_map = dict(zip(model_lookup[\"model_id\"], model_lookup[\"model_display_name\"]))\n",
    "    resolved: list[RunRecord] = []\n",
    "    for record in records:\n",
    "        display = display_map.get(record.model_id, _guess_display_name(record.model_id))\n",
    "        resolved.append(replace(record, model_display=display))\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def run_records_to_frame(records: Sequence[RunRecord]) -> pd.DataFrame:\n",
    "    if not records:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"run_name\",\n",
    "                \"model_id\",\n",
    "                \"model_display\",\n",
    "                \"run_path\",\n",
    "                \"model_path\",\n",
    "                \"metrics_path\",\n",
    "                \"predictions_path\",\n",
    "                \"training_history_path\",\n",
    "            ]\n",
    "        )\n",
    "    data = [\n",
    "        {\n",
    "            \"run_name\": r.run_name,\n",
    "            \"model_id\": r.model_id,\n",
    "            \"model_display\": r.model_display or _guess_display_name(r.model_id),\n",
    "            \"run_path\": r.run_path,\n",
    "            \"model_path\": r.model_path,\n",
    "            \"metrics_path\": r.metrics_path,\n",
    "            \"predictions_path\": r.predictions_path,\n",
    "            \"training_history_path\": r.training_history_path,\n",
    "        }\n",
    "        for r in records\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def to_relative_path(path_like: Optional[Path], root: Path) -> Optional[str]:\n",
    "    if path_like is None:\n",
    "        return None\n",
    "    path = Path(path_like)\n",
    "    try:\n",
    "        return str(path.resolve().relative_to(root))\n",
    "    except Exception:\n",
    "        return str(path.resolve())\n",
    "\n",
    "\n",
    "def maybe_store_table(store: dict[str, pd.DataFrame], key: str, table: pd.DataFrame) -> None:\n",
    "    if table is None or table.empty:\n",
    "        return\n",
    "    store[key] = table\n",
    "\n",
    "\n",
    "def register_figure(store: dict[str, object], key: str, fig: Optional[plt.Figure]) -> None:\n",
    "    \"\"\"Track generated matplotlib figures for later export.\"\"\"\n",
    "    if fig is None:\n",
    "        store.pop(key, None)\n",
    "        return\n",
    "    store[key] = fig\n",
    "\n",
    "\n",
    "def load_metrics(records: Sequence[RunRecord]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames: list[pd.DataFrame] = []\n",
    "    for record in records:\n",
    "        metrics_path = record.metrics_path\n",
    "        if metrics_path is None or not metrics_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to load metrics from {metrics_path}: {exc}\")\n",
    "            continue\n",
    "        required_cols = {\"gene\", \"split\", \"pearson\"}\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            continue\n",
    "        df = df.copy()\n",
    "        df[\"run_name\"] = record.run_name\n",
    "        df[\"model_id\"] = record.model_id\n",
    "        df[\"model_display\"] = record.model_display or _guess_display_name(record.model_id)\n",
    "        long_frames.append(df)\n",
    "    if not long_frames:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    metrics_long = pd.concat(long_frames, ignore_index=True)\n",
    "    base_cols = [col for col in [\"run_name\", \"model_id\", \"model_display\", \"gene\", \"split\"] if col in metrics_long.columns]\n",
    "    metric_cols = [col for col in metrics_long.columns if col not in base_cols]\n",
    "    metrics_long = metrics_long[base_cols + metric_cols]\n",
    "\n",
    "    wide = metrics_long.pivot_table(\n",
    "        index=[\"run_name\", \"model_id\", \"model_display\", \"gene\"],\n",
    "        columns=\"split\",\n",
    "        values=\"pearson\",\n",
    "    )\n",
    "    wide.columns = [f\"{str(col).lower()}_pearson\" for col in wide.columns]\n",
    "    metrics_wide = wide.reset_index()\n",
    "\n",
    "    return metrics_long, metrics_wide\n",
    "\n",
    "\n",
    "def compute_model_summary(\n",
    "    metrics_wide: pd.DataFrame,\n",
    "    splits: Sequence[str],\n",
    ") -> pd.DataFrame:\n",
    "    if metrics_wide.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model_display\",\n",
    "                \"model_id\",\n",
    "                \"run_name\",\n",
    "                *[f\"{split}_pearson_mean\" for split in splits],\n",
    "                *[f\"{split}_pearson_std\" for split in splits],\n",
    "            ]\n",
    "        )\n",
    "    summaries = []\n",
    "    lower_splits = [split.lower() for split in splits]\n",
    "    for (run_name, model_id, model_display), group in metrics_wide.groupby([\"run_name\", \"model_id\", \"model_display\"], dropna=False):\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"model_display\": model_display,\n",
    "        }\n",
    "        for split, lower in zip(splits, lower_splits):\n",
    "            column = f\"{lower}_pearson\"\n",
    "            if column in group:\n",
    "                values = group[column].dropna()\n",
    "                if not values.empty:\n",
    "                    row[f\"{split}_pearson_mean\"] = values.mean()\n",
    "                    row[f\"{split}_pearson_std\"] = values.std(ddof=1) if len(values) > 1 else float(\"nan\")\n",
    "        summaries.append(row)\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    if \"test_pearson_mean\" in summary_df:\n",
    "        summary_df.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    summary_df.set_index([\"model_display\", \"model_id\", \"run_name\"], inplace=True)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c900cf",
   "metadata": {},
   "source": [
    "## 4. Discover and Inspect Run Metadata\n",
    "\n",
    "Enumerate available runs and assemble the run metadata table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118f456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:55.310010Z",
     "iopub.status.busy": "2025-12-30T16:09:55.309860Z",
     "iopub.status.idle": "2025-12-30T16:09:55.452001Z",
     "shell.execute_reply": "2025-12-30T16:09:55.451548Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_run_records = discover_model_runs(\n",
    "    config.results_root,\n",
    "    config.run_include_globs,\n",
    "    config.run_exclude\n",
    ")\n",
    "model_lookup = ensure_model_lookup(\n",
    "    config.lookup_path,\n",
    "    [record.model_id for record in raw_run_records]\n",
    ")\n",
    "_update_model_name_maps(model_lookup)\n",
    "run_records = attach_lookup(raw_run_records, model_lookup)\n",
    "run_df = run_records_to_frame(run_records)\n",
    "run_df.sort_values([\"run_name\", \"model_id\"], inplace=True)\n",
    "run_df_display = run_df.copy()\n",
    "for column in (\"run_path\", \"model_path\", \"metrics_path\", \"predictions_path\", \"training_history_path\"):\n",
    "    run_df_display[column] = run_df_display[column].map(lambda value: to_relative_path(value, config.project_root))\n",
    "analysis_metadata.update({\n",
    "    \"results_root\": to_relative_path(config.results_root, config.project_root),\n",
    "    \"fig_dir\": to_relative_path(config.fig_dir, config.project_root),\n",
    "    \"reports_dir\": to_relative_path(config.reports_dir, config.project_root),\n",
    "    \"run_count\": run_df[\"run_name\"].nunique(),\n",
    "    \"model_count\": len(run_df),\n",
    "    \"model_lookup_path\": to_relative_path(config.lookup_path, config.project_root),\n",
    "})\n",
    "analysis_metadata.setdefault(\"include_globs\", config.run_include_globs)\n",
    "display(Markdown(f\"**Scanning results root:** `{analysis_metadata['results_root']}`\"))\n",
    "subset_descriptor = analysis_metadata.get(\"subset_descriptor\")\n",
    "if subset_descriptor:\n",
    "    display(Markdown(f\"**Subset criteria:** {subset_descriptor}\"))\n",
    "include_filters = analysis_metadata.get(\"include_globs\")\n",
    "if include_filters:\n",
    "    include_text = ', '.join(str(item) for item in include_filters)\n",
    "    display(Markdown(f\"**Include filters:** `{include_text}`\"))\n",
    "display(Markdown(\n",
    "    f\"**Figure output:** `{analysis_metadata['fig_dir']}` | **Reports:** `{analysis_metadata['reports_dir']}`\"\n",
    "))\n",
    "display(run_df_display)\n",
    "print(\n",
    "    \"Discovered\",\n",
    "    analysis_metadata[\"model_count\"],\n",
    "    \"model folders across\",\n",
    "    analysis_metadata[\"run_count\"],\n",
    "    \"runs.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af85674",
   "metadata": {},
   "source": [
    "## 5. Load Metrics and Compute Summaries\n",
    "\n",
    "Load per-gene metrics and compute split-level summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c97df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:55.453512Z",
     "iopub.status.busy": "2025-12-30T16:09:55.453351Z",
     "iopub.status.idle": "2025-12-30T16:09:56.023496Z",
     "shell.execute_reply": "2025-12-30T16:09:56.023068Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long, metrics_wide = load_metrics(run_records)\n",
    "if metrics_long.empty:\n",
    "    raise RuntimeError(\"No metrics available for plotting.\")\n",
    "\n",
    "split_filter = {config.primary_split, config.val_split, config.train_split}\n",
    "available_splits = sorted(metrics_long[\"split\"].unique())\n",
    "missing_splits = split_filter.difference(available_splits)\n",
    "if missing_splits:\n",
    "    print(\"Warning: the following splits are missing from metrics files:\", sorted(missing_splits))\n",
    "\n",
    "test_metrics = metrics_long[metrics_long[\"split\"] == config.primary_split].copy()\n",
    "val_metrics = metrics_long[metrics_long[\"split\"] == config.val_split].copy()\n",
    "train_metrics = metrics_long[metrics_long[\"split\"] == config.train_split].copy()\n",
    "summary_df = compute_model_summary(\n",
    "    metrics_wide, [config.primary_split, config.val_split, config.train_split]\n",
    ")\n",
    "if summary_df.empty:\n",
    "    raise RuntimeError(\"Unable to compute summary statistics from metrics.\")\n",
    "\n",
    "summary_reset = summary_df.reset_index()\n",
    "analysis_metadata[\"best_model_id\"] = summary_reset.iloc[0][\"model_id\"]\n",
    "analysis_metadata[\"best_model_display\"] = summary_reset.iloc[0][\"model_display\"]\n",
    "analysis_metadata[\"best_run_name\"] = summary_reset.iloc[0][\"run_name\"]\n",
    "\n",
    "if \"test_pearson_mean\" in summary_reset:\n",
    "    model_display_order = summary_reset.sort_values(\n",
    "        by=\"test_pearson_mean\", ascending=False\n",
    ")[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "else:\n",
    "    model_display_order = summary_reset[\"model_display\"].tolist()\n",
    "\n",
    "display(summary_reset)\n",
    "\n",
    "analysis_state = {\n",
    "    \"run_df\": run_df,\n",
    "    \"metrics_long\": metrics_long,\n",
    "    \"metrics_wide\": metrics_wide,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"summary_df\": summary_df,\n",
    "    \"summary_reset\": summary_reset,\n",
    "    \"model_display_order\": model_display_order,\n",
    "    \"model_short_name_map\": MODEL_ID_TO_SHORT.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e828e42",
   "metadata": {},
   "source": [
    "## 6. Supporting Tables\n",
    "\n",
    "Write helper tables that back figures and downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b0e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:56.024978Z",
     "iopub.status.busy": "2025-12-30T16:09:56.024819Z",
     "iopub.status.idle": "2025-12-30T16:09:56.048637Z",
     "shell.execute_reply": "2025-12-30T16:09:56.048207Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df = analysis_state[\"summary_df\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "metrics_wide = analysis_state[\"metrics_wide\"]\n",
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "val_metrics = analysis_state[\"val_metrics\"]\n",
    "train_metrics = analysis_state[\"train_metrics\"]\n",
    "\n",
    "val_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.val_split}_pearson\" in metrics_wide:\n",
    "    val_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.val_split}_pearson\",\n",
    "    )\n",
    "\n",
    "test_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "    )\n",
    "\n",
    "maybe_store_table(\n",
    "    TABLES,\n",
    "    \"metrics_per_gene_master\",\n",
    "    metrics_long.sort_values([\"split\", \"run_name\", \"model_id\", \"gene\"])\n",
    ")\n",
    "maybe_store_table(TABLES, \"summary_metrics_all_models\", summary_reset)\n",
    "\n",
    "analysis_state.update(\n",
    "    {\n",
    "        \"val_pearson_per_gene\": val_pearson_per_gene,\n",
    "        \"test_pearson_per_gene\": test_pearson_per_gene,\n",
    "        \"split_mean_summary\": summary_reset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148258c",
   "metadata": {},
   "source": [
    "## 7. Test Pearson Heatmap\n",
    "\n",
    "Model-level test Pearson comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829380c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:56.050289Z",
     "iopub.status.busy": "2025-12-30T16:09:56.050135Z",
     "iopub.status.idle": "2025-12-30T16:09:56.541629Z",
     "shell.execute_reply": "2025-12-30T16:09:56.541138Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_order = analysis_state[\"model_display_order\"]\n",
    "if \"test_pearson_mean\" not in summary_reset:\n",
    "    print(\"Test Pearson summary unavailable; skipping heatmap.\")\n",
    "    fig_test_heatmap = None\n",
    "else:\n",
    "    ranked = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "    heatmap_series = best_per_model.set_index(\"model_display\")[\"test_pearson_mean\"]\n",
    "    heatmap_df = heatmap_series.reindex(model_order).dropna().to_frame(name=\"Mean Test Pearson\")\n",
    "    if heatmap_df.empty:\n",
    "        print(\"No aggregated test Pearson values available; skipping heatmap.\")\n",
    "        fig_test_heatmap = None\n",
    "    else:\n",
    "        vmin, vmax = compute_heatmap_limits(heatmap_df.values)\n",
    "        fig_height = max(4, 0.4 * len(heatmap_df))\n",
    "        fig_test_heatmap, ax = plt.subplots(figsize=(4.5, fig_height))\n",
    "        sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            linewidths=0.4,\n",
    "            linecolor=\"#f2f2f2\",\n",
    "            cbar_kws={\"label\": \"Mean Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Mean Test Pearson by Model\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "        sns.despine(fig_test_heatmap, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_all_models\", fig_test_heatmap)\n",
    "if fig_test_heatmap is not None:\n",
    "    display(fig_test_heatmap)\n",
    "    plt.close(fig_test_heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084094b",
   "metadata": {},
   "source": [
    "### Optional Validation Heatmap\n",
    "\n",
    "Runs only if validation Pearson summaries are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f19347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:56.543196Z",
     "iopub.status.busy": "2025-12-30T16:09:56.543033Z",
     "iopub.status.idle": "2025-12-30T16:09:57.004143Z",
     "shell.execute_reply": "2025-12-30T16:09:57.003695Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "test_pivot = analysis_state.get(\"test_pearson_per_gene\")\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "if test_pivot is None or test_pivot.empty:\n",
    "    print(\"Test Pearson data unavailable; skipping heatmap.\")\n",
    "    fig_test_heatmap_top = None\n",
    "else:\n",
    "    top_test_genes = (\n",
    "        test_metrics.groupby(\"gene\")[\"pearson\"].mean().sort_values(ascending=False).head(config.top_gene_count)\n",
    "    )\n",
    "    analysis_state[\"test_top_genes\"] = top_test_genes.index.tolist()\n",
    "    test_top_subset = test_pivot.loc[test_pivot.index.intersection(top_test_genes.index)]\n",
    "    if test_top_subset.empty:\n",
    "        print(\"Top-performing gene subset empty; skipping test heatmap.\")\n",
    "        fig_test_heatmap_top = None\n",
    "    else:\n",
    "        model_order = summary_reset[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "        ordered_columns = [col for col in model_order if col in test_top_subset.columns]\n",
    "        test_top_subset = test_top_subset.reindex(columns=ordered_columns)\n",
    "        vmin, vmax = compute_heatmap_limits(\n",
    "            test_top_subset.values, lower_percentile=10.0, upper_percentile=95.0\n",
    ")\n",
    "        fig_height_top = max(4, 0.6 * len(test_top_subset.index))\n",
    "        fig_test_heatmap_top, ax = plt.subplots(figsize=(10, fig_height_top))\n",
    "        sns.heatmap(\n",
    "            test_top_subset,\n",
    "            cmap=\"crest\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.3,\n",
    "            linecolor=\"#f5f5f5\",\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Test Pearson (top genes by mean across models)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Gene\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=10)\n",
    "        sns.despine(fig_test_heatmap_top, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_top\", fig_test_heatmap_top)\n",
    "if fig_test_heatmap_top is not None:\n",
    "    display(fig_test_heatmap_top)\n",
    "    plt.close(fig_test_heatmap_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb587b",
   "metadata": {},
   "source": [
    "## 8. Test Distribution Profiles\n",
    "\n",
    "Per-gene test-set Pearson distributions by model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3383333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:57.007137Z",
     "iopub.status.busy": "2025-12-30T16:09:57.006979Z",
     "iopub.status.idle": "2025-12-30T16:09:58.019506Z",
     "shell.execute_reply": "2025-12-30T16:09:58.019052Z"
    }
   },
   "outputs": [],
   "source": [
    "test_metrics = analysis_state[\"test_metrics\"].copy()\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "if test_metrics.empty:\n",
    "    print(\"Test metrics unavailable; skipping violin plot.\")\n",
    "    fig_violin = None\n",
    "else:\n",
    "    mean_by_model = summary_reset.groupby(\"model_display\")[\"test_pearson_mean\"].mean()\n",
    "    mean_order_series = mean_by_model.sort_values(ascending=True)\n",
    "    model_order = [model for model in mean_order_series.index if model in model_display_order]\n",
    "    if not model_order:\n",
    "        model_order = model_display_order\n",
    "    violin_palette = sns.color_palette(\"Set2\", n_colors=len(model_order))\n",
    "    palette_map = dict(zip(model_order, violin_palette))\n",
    "    fig_width = max(12, 0.8 * max(6, len(model_order)))\n",
    "    fig_violin, ax = plt.subplots(figsize=(fig_width, 6))\n",
    "    sns.violinplot(\n",
    "        data=test_metrics,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"model_display\",\n",
    "        order=model_order,\n",
    "        hue_order=model_order,\n",
    "        palette=palette_map,\n",
    "        density_norm=\"width\",\n",
    "        inner=\"quartile\",\n",
    "        linewidth=1.0,\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    if len(test_metrics) > 0:\n",
    "        sample_size = min(len(test_metrics), 3000)\n",
    "        jitter_sample = (\n",
    "            test_metrics.sample(sample_size, random_state=config.random_seed)\n",
    "            if len(test_metrics) > sample_size\n",
    "            else test_metrics\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            data=jitter_sample,\n",
    "            x=\"model_display\",\n",
    "            y=\"pearson\",\n",
    "            order=model_order,\n",
    "            hue=\"model_display\",\n",
    "            hue_order=model_order,\n",
    "            palette=palette_map,\n",
    "            dodge=False,\n",
    "            alpha=0.45,\n",
    "            size=3.0,\n",
    "            jitter=0.12,\n",
    "            linewidth=0.3,\n",
    "            edgecolor=\"#2b2b2b\",\n",
    "            marker=\"o\",\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "        )\n",
    "    metric_min = test_metrics[\"pearson\"].min()\n",
    "    metric_max = test_metrics[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Test Pearson\")\n",
    "    ax.set_title(\"Per-gene Test Pearson Distribution by Model\")\n",
    "    ax.axhline(0.0, color=\"#777777\", linestyle=\"--\", linewidth=1)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    sns.despine(fig_violin, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_violin\", fig_violin)\n",
    "if fig_violin is not None:\n",
    "    display(fig_violin)\n",
    "    plt.close(fig_violin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8b859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:58.024490Z",
     "iopub.status.busy": "2025-12-30T16:09:58.024326Z",
     "iopub.status.idle": "2025-12-30T16:09:58.721705Z",
     "shell.execute_reply": "2025-12-30T16:09:58.721253Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "splits_of_interest = [config.train_split, config.val_split, config.primary_split]\n",
    "subset = metrics_long[metrics_long[\"split\"].isin(splits_of_interest)].copy()\n",
    "if subset.empty or \"pearson\" not in subset:\n",
    "    print(\"Pearson metrics unavailable across requested splits; skipping split comparison plot.\")\n",
    "    fig_split_compare = None\n",
    "else:\n",
    "    subset = subset[[\"model_display\", \"split\", \"pearson\"]].dropna()\n",
    "    split_labels = {\n",
    "        config.train_split: \"Train\",\n",
    "        config.val_split: \"Val\",\n",
    "        config.primary_split: \"Test\",\n",
    "    }\n",
    "    subset[\"split_label\"] = subset[\"split\"].map(split_labels).fillna(subset[\"split\"].str.title())\n",
    "    test_means = summary_reset.groupby(\"model_display\")[\"test_pearson_mean\"].mean().sort_values()\n",
    "    ordered_models = [model for model in test_means.index if model in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = model_display_order\n",
    "    split_order = [split_labels[split] for split in splits_of_interest if split in split_labels]\n",
    "    box_colors = sns.color_palette(\"Set2\", n_colors=len(split_order))\n",
    "    box_palette = dict(zip(split_order, box_colors))\n",
    "    fig_width = max(12, 0.75 * max(6, len(ordered_models)))\n",
    "    fig_split_compare, ax = plt.subplots(figsize=(fig_width, 6.5))\n",
    "    sns.boxplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=ordered_models,\n",
    "        hue_order=split_order,\n",
    "        palette=box_palette,\n",
    "        width=0.65,\n",
    "        fliersize=0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = subset.sample(min(len(subset), 4000), random_state=config.random_seed) if len(subset) > 4000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=ordered_models,\n",
    "        hue_order=split_order,\n",
    "        palette=box_palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=3.0,\n",
    "        alpha=0.5,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.4,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    metric_min = subset[\"pearson\"].min()\n",
    "    metric_max = subset[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Pearson\")\n",
    "    ax.set_title(\"Per-gene Pearson by Split and Model\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Split\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_split_compare, left=True, bottom=True)\n",
    "    fig_split_compare.tight_layout()\n",
    "register_figure(FIGURES, \"split_comparison_overview\", fig_split_compare)\n",
    "if fig_split_compare is not None:\n",
    "    display(fig_split_compare)\n",
    "    plt.close(fig_split_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d4842",
   "metadata": {},
   "source": [
    "## 9. Embryonic vs Endothelial Comparisons\n",
    "\n",
    "Train/val boxplots and test-set violin plots for each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45e594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:09:58.726069Z",
     "iopub.status.busy": "2025-12-30T16:09:58.725906Z",
     "iopub.status.idle": "2025-12-30T16:10:00.001306Z",
     "shell.execute_reply": "2025-12-30T16:10:00.000728Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis_state = globals().get(\"analysis_state\", {})\n",
    "metrics_long = analysis_state.get(\"metrics_long\")\n",
    "run_df = analysis_state.get(\"run_df\")\n",
    "model_display_order = analysis_state.get(\"model_display_order\", [])\n",
    "\n",
    "EMBRYONIC_RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_100genes_cpu_embryonic_20251218_114225_xgboost\",\n",
    "]\n",
    "ENDOTHELIAL_RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_100genes_cpu_endothelial_20251220_091232_xgboost\",\n",
    "]\n",
    "EMBRYONIC_RUN_GLOB_SELECTION: list[str] = [\n",
    "    \"*embryonic*\",\n",
    "]\n",
    "ENDOTHELIAL_RUN_GLOB_SELECTION: list[str] = [\n",
    "    \"*endothelial*\",\n",
    "]\n",
    "\n",
    "def _select_run_names(run_df: pd.DataFrame, entries: list[str | Path], globs: list[str]) -> set[str]:\n",
    "    if run_df is None or run_df.empty:\n",
    "        return set()\n",
    "    manual_names = [name for name in (_resolve_run_name(entry) for entry in entries) if name]\n",
    "    glob_patterns = [pattern.strip() for pattern in globs if pattern and pattern.strip()]\n",
    "    selected = set(manual_names)\n",
    "    if glob_patterns:\n",
    "        for run_name in run_df[\"run_name\"].dropna().unique():\n",
    "            if _matches_any(run_name, glob_patterns):\n",
    "                selected.add(run_name)\n",
    "    return selected\n",
    "\n",
    "def _summarize_selection(label: str, run_names: set[str], all_runs: list[str]) -> None:\n",
    "    if run_names:\n",
    "        print(f\"{label} runs ({len(run_names)}):\", sorted(run_names))\n",
    "    else:\n",
    "        print(f\"No {label.lower()} runs matched.\")\n",
    "        print(\"Available run names:\", all_runs)\n",
    "\n",
    "def _subset_metrics(metrics_long: pd.DataFrame, run_names: set[str], splits: list[str], dataset: str) -> pd.DataFrame:\n",
    "    if metrics_long is None or metrics_long.empty or not run_names:\n",
    "        return pd.DataFrame()\n",
    "    subset = metrics_long[\n",
    "        metrics_long[\"run_name\"].isin(run_names)\n",
    "        & metrics_long[\"split\"].isin(splits)\n",
    "    ].copy()\n",
    "    if subset.empty:\n",
    "        return pd.DataFrame()\n",
    "    subset[\"dataset\"] = dataset\n",
    "    return subset\n",
    "\n",
    "if metrics_long is None or run_df is None or run_df.empty:\n",
    "    print(\"Run metadata unavailable; skipping dataset comparison figures.\")\n",
    "    fig_train_val_box = None\n",
    "    fig_test_violin_by_dataset = None\n",
    "else:\n",
    "    all_runs = sorted(run_df[\"run_name\"].dropna().unique())\n",
    "    endothelial_runs = _select_run_names(run_df, ENDOTHELIAL_RUN_DIRECTORY_SELECTION, ENDOTHELIAL_RUN_GLOB_SELECTION)\n",
    "    embryonic_runs = _select_run_names(run_df, EMBRYONIC_RUN_DIRECTORY_SELECTION, EMBRYONIC_RUN_GLOB_SELECTION)\n",
    "\n",
    "    if not embryonic_runs and endothelial_runs and not EMBRYONIC_RUN_DIRECTORY_SELECTION and not EMBRYONIC_RUN_GLOB_SELECTION:\n",
    "        embryonic_runs = set(all_runs) - endothelial_runs\n",
    "    elif not endothelial_runs and embryonic_runs and not ENDOTHELIAL_RUN_DIRECTORY_SELECTION and not ENDOTHELIAL_RUN_GLOB_SELECTION:\n",
    "        endothelial_runs = set(all_runs) - embryonic_runs\n",
    "\n",
    "    _summarize_selection(\"Embryonic\", embryonic_runs, all_runs)\n",
    "    _summarize_selection(\"Endothelial\", endothelial_runs, all_runs)\n",
    "\n",
    "    embryonic_test = _subset_metrics(metrics_long, embryonic_runs, [config.primary_split], \"Embryonic\")\n",
    "    endothelial_test = _subset_metrics(metrics_long, endothelial_runs, [config.primary_split], \"Endothelial\")\n",
    "    embryonic_train_val = _subset_metrics(metrics_long, embryonic_runs, [config.train_split, config.val_split], \"Embryonic\")\n",
    "    endothelial_train_val = _subset_metrics(metrics_long, endothelial_runs, [config.train_split, config.val_split], \"Endothelial\")\n",
    "\n",
    "    combined_test = pd.concat([embryonic_test, endothelial_test], ignore_index=True)\n",
    "    combined_train_val = pd.concat([embryonic_train_val, endothelial_train_val], ignore_index=True)\n",
    "\n",
    "    if combined_train_val.empty and combined_test.empty:\n",
    "        print(\"No data for dataset comparison.\")\n",
    "        fig_train_val_box = None\n",
    "        fig_test_violin_by_dataset = None\n",
    "    else:\n",
    "        # --- Train/Val Boxplot by Dataset ---\n",
    "        if not combined_train_val.empty:\n",
    "            fig_train_val_box, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.boxplot(\n",
    "                data=combined_train_val,\n",
    "                x=\"model_display_name\",\n",
    "                y=\"pearson\",\n",
    "                hue=\"dataset\",\n",
    "                ax=ax,\n",
    "                palette=\"Set2\",\n",
    "            )\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Pearson Correlation\")\n",
    "            ax.set_title(\"Train/Val Pearson by Dataset (Grouped)\")\n",
    "            ax.legend(title=\"Dataset\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            FIGURES[\"train_val_pearson_by_dataset_box\"] = fig_train_val_box\n",
    "        else:\n",
    "            fig_train_val_box = None\n",
    "\n",
    "        # --- Test Violin by Dataset ---\n",
    "        if not combined_test.empty:\n",
    "            fig_test_violin_by_dataset, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.violinplot(\n",
    "                data=combined_test,\n",
    "                x=\"model_display_name\",\n",
    "                y=\"pearson\",\n",
    "                hue=\"dataset\",\n",
    "                split=True,\n",
    "                ax=ax,\n",
    "                palette=\"Set2\",\n",
    "            )\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Test Pearson Correlation\")\n",
    "            ax.set_title(\"Test Pearson by Dataset (Split Violin)\")\n",
    "            ax.legend(title=\"Dataset\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            FIGURES[\"test_pearson_by_dataset_violin\"] = fig_test_violin_by_dataset\n",
    "        else:\n",
    "            fig_test_violin_by_dataset = None\n",
    "\n",
    "if \"analysis_state\" not in globals():\n",
    "    analysis_state = {}\n",
    "analysis_state[\"fig_train_val_box\"] = fig_train_val_box\n",
    "analysis_state[\"fig_test_violin_by_dataset\"] = fig_test_violin_by_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1164d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:00.005283Z",
     "iopub.status.busy": "2025-12-30T16:10:00.005086Z",
     "iopub.status.idle": "2025-12-30T16:10:00.710788Z",
     "shell.execute_reply": "2025-12-30T16:10:00.710321Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "short_map = analysis_state.get(\"model_short_name_map\", MODEL_ID_TO_SHORT)\n",
    "\n",
    "if test_metrics.empty:\n",
    "    print(\"Test metrics unavailable; skipping top-gene visualisation.\")\n",
    "    analysis_state[\"top_gene_figure_keys\"] = []\n",
    "    fig_top_genes = None\n",
    "else:\n",
    "    if \"test_pearson_mean\" in summary_reset:\n",
    "        ranked_models = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    else:\n",
    "        ranked_models = summary_reset\n",
    "    top_models = ranked_models.drop_duplicates(\"model_id\").head(config.top_model_count)[[\"model_id\", \"model_display\"]]\n",
    "    gene_frames: list[pd.DataFrame] = []\n",
    "    for row in top_models.itertuples(index=False):\n",
    "        model_subset = test_metrics[test_metrics[\"model_id\"] == row.model_id]\n",
    "        if model_subset.empty:\n",
    "            continue\n",
    "        gene_stats = (\n",
    "            model_subset.groupby(\"gene\")[\"pearson\"].agg(mean=\"mean\", std=\"std\", count=\"count\").reset_index()\n",
    "        )\n",
    "        gene_stats.rename(\n",
    "            columns={\n",
    "                \"mean\": \"mean_test_pearson\",\n",
    "                \"std\": \"pearson_std\",\n",
    "                \"count\": \"observation_count\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        short_name = short_map.get(row.model_id, to_short_name(row.model_display)) or row.model_display\n",
    "        gene_stats[\"model_display_long\"] = row.model_display\n",
    "        gene_stats[\"model_display_short\"] = short_name\n",
    "        gene_frames.append(gene_stats)\n",
    "    if not gene_frames:\n",
    "        print(\"No genes available for top-model bar chart.\")\n",
    "        analysis_state[\"top_gene_figure_keys\"] = []\n",
    "        fig_top_genes = None\n",
    "    else:\n",
    "        top_gene_df = pd.concat(gene_frames, ignore_index=True)\n",
    "        figure_keys: list[str] = []\n",
    "        for long_name, short_name in (\n",
    "            top_gene_df[[\"model_display_long\", \"model_display_short\"]]\n",
    "            .drop_duplicates()\n",
    "            .itertuples(index=False, name=None)\n",
    "        ):\n",
    "            group = top_gene_df[top_gene_df[\"model_display_long\"] == long_name]\n",
    "            ordered = group.sort_values(\"mean_test_pearson\", ascending=False).head(config.top_gene_count)\n",
    "            if ordered.empty:\n",
    "                continue\n",
    "            fig, ax = plt.subplots(figsize=(9, 6))\n",
    "            colors = sns.color_palette(\"crest\", n_colors=len(ordered))\n",
    "            bars = ax.barh(\n",
    "                ordered[\"gene\"],\n",
    "                ordered[\"mean_test_pearson\"],\n",
    "                color=colors,\n",
    "                edgecolor=\"#2b2b2b\",\n",
    "                linewidth=0.4,\n",
    "            )\n",
    "            ax.set_title(f\"Top {config.top_gene_count} Genes | {short_name}\")\n",
    "            ax.set_xlabel(\"Mean Test Pearson\")\n",
    "            ax.set_ylabel(\"Gene\")\n",
    "            data_max = ordered[\"mean_test_pearson\"].max()\n",
    "            label_offset = max(0.01, data_max * 0.01)\n",
    "            x_max = max(1.0, data_max + 0.05) + label_offset * 4\n",
    "            ax.set_xlim(0, x_max)\n",
    "            ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "            ax.invert_yaxis()\n",
    "            for bar, mean_val in zip(bars, ordered[\"mean_test_pearson\"]):\n",
    "                y_pos = bar.get_y() + bar.get_height() / 2\n",
    "                ax.text(\n",
    "                    mean_val + label_offset,\n",
    "                    y_pos,\n",
    "                    f\"{mean_val:.3f}\",\n",
    "                    ha=\"left\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"#1a1a1a\",\n",
    "                )\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            plt.tight_layout()\n",
    "            slug = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", short_name.lower()).strip(\"_\") or \"model\"\n",
    "            key = f\"top_genes_test_performance_{slug}\"\n",
    "            register_figure(FIGURES, key, fig)\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "            figure_keys.append(key)\n",
    "        analysis_state[\"top_gene_figure_keys\"] = figure_keys\n",
    "        fig_top_genes = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3046ec5",
   "metadata": {},
   "source": [
    "## 10. Generalization Gap Overview\n",
    "\n",
    "Train vs test Pearson summary for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61448f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:00.712239Z",
     "iopub.status.busy": "2025-12-30T16:10:00.712079Z",
     "iopub.status.idle": "2025-12-30T16:10:00.914066Z",
     "shell.execute_reply": "2025-12-30T16:10:00.913623Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "required_cols = {\"train_pearson_mean\", \"test_pearson_mean\"}\n",
    "fig_generalization_gap = None\n",
    "if not required_cols.issubset(summary_reset.columns):\n",
    "    print(\"Train/test summary columns unavailable; skipping generalization gap plot.\")\n",
    "else:\n",
    "    ranked = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "    gap_df = best_per_model[[\"model_display\", \"train_pearson_mean\", \"test_pearson_mean\"]].copy()\n",
    "    gap_df[\"generalization_gap\"] = gap_df[\"train_pearson_mean\"] - gap_df[\"test_pearson_mean\"]\n",
    "    gap_df.sort_values(\"generalization_gap\", ascending=False, inplace=True)\n",
    "    fig_height = max(4, 0.35 * len(gap_df))\n",
    "    fig_generalization_gap, ax = plt.subplots(figsize=(8, fig_height))\n",
    "    colors = sns.color_palette(\"mako\", n_colors=len(gap_df))\n",
    "    bars = ax.barh(\n",
    "        gap_df[\"model_display\"],\n",
    "        gap_df[\"generalization_gap\"],\n",
    "        color=colors,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax.axvline(0.0, color=\"#6d6d6d\", linestyle=\"--\", linewidth=1)\n",
    "    gap_min = gap_df[\"generalization_gap\"].min()\n",
    "    gap_max = gap_df[\"generalization_gap\"].max()\n",
    "    span = max(0.01, gap_max - gap_min)\n",
    "    margin = max(0.08, span * 0.12)\n",
    "    xmin = min(gap_min - margin, gap_min - 0.02)\n",
    "    xmin = min(xmin, -0.15)\n",
    "    xmax = max(gap_max + margin, 0.15)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xlabel(\"Train minus Test Mean Pearson\")\n",
    "    ax.set_ylabel(\"Model\")\n",
    "    ax.set_title(\"Generalization Gap (Train - Test)\")\n",
    "    label_offset = (xmax - xmin) * 0.02\n",
    "    for bar, value in zip(bars, gap_df[\"generalization_gap\"]):\n",
    "        y = bar.get_y() + bar.get_height() / 2\n",
    "        if value >= 0:\n",
    "            ax.text(value + label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"#1f1f1f\")\n",
    "        else:\n",
    "            ax.text(value - label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"right\", fontsize=9, color=\"#1f1f1f\")\n",
    "    sns.despine(fig_generalization_gap, left=True, bottom=True)\n",
    "    fig_generalization_gap.tight_layout()\n",
    "register_figure(FIGURES, \"generalization_gap\", fig_generalization_gap)\n",
    "if fig_generalization_gap is not None:\n",
    "    display(fig_generalization_gap)\n",
    "    plt.close(fig_generalization_gap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1234b1",
   "metadata": {},
   "source": [
    "## 11. Top-Model Diagnostics\n",
    "\n",
    "Quick previews for the best-performing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b7212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:00.915649Z",
     "iopub.status.busy": "2025-12-30T16:10:00.915476Z",
     "iopub.status.idle": "2025-12-30T16:10:00.983400Z",
     "shell.execute_reply": "2025-12-30T16:10:00.982980Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "best_models = summary_reset.head(config.top_model_count).reset_index(drop=True)\n",
    "if best_models.empty:\n",
    "    print(\"No models available for scatter plot previews.\")\n",
    "else:\n",
    "    missing_assets: list[str] = []\n",
    "    for model_row in best_models.itertuples(index=False):\n",
    "        run_rows = run_df[(run_df[\"model_id\"] == model_row.model_id) & (run_df[\"run_name\"] == model_row.run_name)]\n",
    "        if run_rows.empty:\n",
    "            missing_assets.append(\n",
    "                f\"Missing run directory metadata for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        row = run_rows.iloc[0]\n",
    "        model_dir_raw = row[\"model_path\"]\n",
    "        model_dir = model_dir_raw if isinstance(model_dir_raw, Path) else Path(model_dir_raw)\n",
    "        if not model_dir.exists():\n",
    "            missing_assets.append(\n",
    "                f\"Model directory missing on disk for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        scatter_path = model_dir / \"scatter_test.png\"\n",
    "        if scatter_path.exists():\n",
    "            display(Markdown(\n",
    "                f\"**{model_row.model_display}** (run `{model_row.run_name}`)  `scatter_test.png`\"\n",
    "            ))\n",
    "            display(Image(filename=str(scatter_path)))\n",
    "        else:\n",
    "            predictions_raw = row.get(\"predictions_path\") if isinstance(row, pd.Series) else None\n",
    "            predictions_path = None\n",
    "            if predictions_raw:\n",
    "                predictions_path = predictions_raw if isinstance(predictions_raw, Path) else Path(predictions_raw)\n",
    "            if predictions_path and predictions_path.exists():\n",
    "                missing_assets.append(\n",
    "                    f\"`scatter_test.png` not found for {model_row.model_display} (run {model_row.run_name}). \"\n",
    "                    f\"Checked `{to_relative_path(model_dir, config.project_root)}`.\"\n",
    "                )\n",
    "            else:\n",
    "                missing_assets.append(\n",
    "                    f\"No test predictions available for {model_row.model_display} (run {model_row.run_name}); skipping scatter preview.\"\n",
    "                )\n",
    "    if missing_assets:\n",
    "        for message in missing_assets:\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43071670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:00.985011Z",
     "iopub.status.busy": "2025-12-30T16:10:00.984860Z",
     "iopub.status.idle": "2025-12-30T16:10:01.047198Z",
     "shell.execute_reply": "2025-12-30T16:10:01.046816Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "best_models = summary_reset.head(config.top_model_count).reset_index(drop=True)\n",
    "if best_models.empty:\n",
    "    print(\"Best models not identified; skipping training history previews.\")\n",
    "else:\n",
    "    missing_assets: list[str] = []\n",
    "    for model_row in best_models.itertuples(index=False):\n",
    "        run_rows = run_df[(run_df[\"model_id\"] == model_row.model_id) & (run_df[\"run_name\"] == model_row.run_name)]\n",
    "        if run_rows.empty:\n",
    "            missing_assets.append(\n",
    "                f\"Missing run directory metadata for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        row = run_rows.iloc[0]\n",
    "        model_dir_raw = row[\"model_path\"]\n",
    "        model_dir = model_dir_raw if isinstance(model_dir_raw, Path) else Path(model_dir_raw)\n",
    "        if not model_dir.exists():\n",
    "            missing_assets.append(\n",
    "                f\"Model directory missing on disk for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        history_candidates = [model_dir / \"training_history_loss.png\"]\n",
    "        history_dir = model_dir / \"histories\"\n",
    "        if history_dir.exists():\n",
    "            history_candidates.extend(sorted(history_dir.glob(\"*_loss.png\")))\n",
    "        history_path = next((path for path in history_candidates if path.exists()), None)\n",
    "        if history_path is not None:\n",
    "            display(Markdown(\n",
    "                f\"**{model_row.model_display}** (run `{model_row.run_name}`)  `{history_path.name}`\"\n",
    "            ))\n",
    "            display(Image(filename=str(history_path)))\n",
    "        else:\n",
    "            missing_assets.append(\n",
    "                f\"No training history plot available for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "    if missing_assets:\n",
    "        for message in missing_assets:\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e6757",
   "metadata": {},
   "source": [
    "## 12. Export Artifacts\n",
    "\n",
    "Persist figures and tables to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df177c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:01.048682Z",
     "iopub.status.busy": "2025-12-30T16:10:01.048526Z",
     "iopub.status.idle": "2025-12-30T16:10:05.666457Z",
     "shell.execute_reply": "2025-12-30T16:10:05.665923Z"
    }
   },
   "outputs": [],
   "source": [
    "FIGURE_SAVE_PLAN: dict[str, list[Path]] = {\n",
    "    \"test_pearson_heatmap_all_models\": [config.fig_dir / \"test_pearson_heatmap_all_models.png\"],\n",
    "    \"test_pearson_violin\": [config.fig_dir / \"test_pearson_violin.png\"],\n",
    "    \"split_comparison_overview\": [config.fig_dir / \"split_comparison_overview.png\"],\n",
    "    \"test_pearson_heatmap_top\": [config.fig_dir / \"test_pearson_heatmap_top_genes.png\"],\n",
    "    \"generalization_gap\": [config.fig_dir / \"generalization_gap.png\"],\n",
    "    \"best_model_feature_importance_top\": [config.fig_dir / \"best_model_feature_importance_top.png\"],\n",
    "    \"train_val_box_by_dataset\": [config.fig_dir / \"train_val_box_by_dataset.png\"],\n",
    "    \"test_violin_by_dataset\": [config.fig_dir / \"test_violin_by_dataset.png\"],\n",
    "    \"resource_usage_summary\": [config.fig_dir / \"resource_usage_summary.png\"],\n",
    "}\n",
    "\n",
    "top_gene_keys = analysis_state.get(\"top_gene_figure_keys\", [])\n",
    "for key in top_gene_keys:\n",
    "    filename = f\"{key}.png\"\n",
    "    FIGURE_SAVE_PLAN[key] = [config.fig_dir / filename]\n",
    "\n",
    "TABLE_SAVE_PLAN: dict[str, list[Path]] = {\n",
    "    \"metrics_per_gene_master\": [config.reports_dir / \"metrics_per_gene_master.csv\"],\n",
    "    \"summary_metrics_all_models\": [config.reports_dir / \"summary_metrics_all_models.csv\"],\n",
    "}\n",
    "\n",
    "saved_figures: list[Path] = []\n",
    "for key, targets in FIGURE_SAVE_PLAN.items():\n",
    "    fig = FIGURES.get(key)\n",
    "    if fig is None:\n",
    "        print(f\"Skipping figure '{key}' (not generated).\")\n",
    "        continue\n",
    "    for target in targets:\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(target, bbox_inches=\"tight\")\n",
    "        saved_figures.append(target)\n",
    "\n",
    "saved_tables: list[Path] = []\n",
    "for key, targets in TABLE_SAVE_PLAN.items():\n",
    "    table = TABLES.get(key)\n",
    "    if table is None or table.empty:\n",
    "        print(f\"Skipping table '{key}' (not generated or empty).\")\n",
    "        continue\n",
    "    for target in targets:\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        table.to_csv(target, index=False)\n",
    "        saved_tables.append(target)\n",
    "\n",
    "print(\"Saved figures:\")\n",
    "for target in saved_figures:\n",
    "    print(f\" - {target}\")\n",
    "\n",
    "print(\"Saved tables:\")\n",
    "for target in saved_tables:\n",
    "    print(f\" - {target}\")\n",
    "\n",
    "if saved_figures:\n",
    "    analysis_metadata[\"saved_figures\"] = [to_relative_path(path, config.project_root) for path in saved_figures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d7094",
   "metadata": {},
   "source": [
    "## 13. Session Metadata\n",
    "\n",
    "Run summary details for logging and provenance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a75506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:05.668038Z",
     "iopub.status.busy": "2025-12-30T16:10:05.667869Z",
     "iopub.status.idle": "2025-12-30T16:10:05.671723Z",
     "shell.execute_reply": "2025-12-30T16:10:05.671346Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_at = analysis_metadata.get(\"generated_at\", \"n/a\")\n",
    "results_root = analysis_metadata.get(\"results_root\", \"n/a\")\n",
    "run_count = analysis_metadata.get(\"run_count\", \"n/a\")\n",
    "model_count = analysis_metadata.get(\"model_count\", \"n/a\")\n",
    "best_display = analysis_metadata.get(\"best_model_display\", \"n/a\")\n",
    "best_id = analysis_metadata.get(\"best_model_id\", \"n/a\")\n",
    "best_run = analysis_metadata.get(\"best_run_name\", \"n/a\")\n",
    "report_markdown = f\"\"\"### Analysis Metadata\n",
    "- Generated at: `{generated_at}`\n",
    "- Results root: `{results_root}`\n",
    "- Runs analysed: `{run_count}`\n",
    "- Models analysed: `{model_count}`\n",
    "- Best model: `{best_display}` (`{best_id}`)\n",
    "- Source run: `{best_run}`\"\"\"\n",
    "display(Markdown(report_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54883550",
   "metadata": {},
   "source": [
    "## 14. Feature Importance (Best Model)\n",
    "\n",
    "Load feature-importance exports for the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a81c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:05.673072Z",
     "iopub.status.busy": "2025-12-30T16:10:05.672930Z",
     "iopub.status.idle": "2025-12-30T16:10:05.681732Z",
     "shell.execute_reply": "2025-12-30T16:10:05.681321Z"
    }
   },
   "outputs": [],
   "source": [
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "if best_details is None:\n",
    "    summary_reset = analysis_state.get(\"summary_reset\")\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    if summary_reset is None or summary_reset.empty:\n",
    "        raise RuntimeError(\"Best-model summary unavailable; please rerun Section 5 before using Section 13.\")\n",
    "    if run_df is None or run_df.empty:\n",
    "        raise RuntimeError(\"Run metadata missing; rerun Section 4 before using Section 13.\")\n",
    "    leader = summary_reset.iloc[0]\n",
    "    best_model_id = str(leader[\"model_id\"])\n",
    "    best_run_name = str(leader[\"run_name\"])\n",
    "    best_model_display = str(leader.get(\"model_display\", best_model_id))\n",
    "    match = run_df[(run_df[\"model_id\"] == best_model_id) & (run_df[\"run_name\"] == best_run_name)]\n",
    "    if match.empty:\n",
    "        raise RuntimeError(f\"Unable to locate run folder for {best_model_display} (run {best_run_name}).\")\n",
    "    row = match.iloc[0]\n",
    "    model_path_value = row.get(\"model_path\")\n",
    "    if isinstance(model_path_value, Path):\n",
    "        best_model_dir = model_path_value.resolve()\n",
    "    elif isinstance(model_path_value, str) and model_path_value:\n",
    "        best_model_dir = Path(model_path_value).resolve()\n",
    "    else:\n",
    "        raise RuntimeError(\"Model directory not recorded for the leading model.\")\n",
    "    run_path_value = row.get(\"run_path\")\n",
    "    if isinstance(run_path_value, Path):\n",
    "        best_run_dir = run_path_value.resolve()\n",
    "    elif isinstance(run_path_value, str) and run_path_value:\n",
    "        best_run_dir = Path(run_path_value).resolve()\n",
    "    else:\n",
    "        best_run_dir = best_model_dir.parent\n",
    "    metrics_value = row.get(\"metrics_path\")\n",
    "    if isinstance(metrics_value, Path):\n",
    "        metrics_path = metrics_value.resolve()\n",
    "    elif isinstance(metrics_value, str) and metrics_value:\n",
    "        metrics_path = Path(metrics_value).resolve()\n",
    "    else:\n",
    "        metrics_path = None\n",
    "    preds_value = row.get(\"predictions_path\")\n",
    "    if isinstance(preds_value, Path):\n",
    "        predictions_path = preds_value.resolve()\n",
    "    elif isinstance(preds_value, str) and preds_value:\n",
    "        predictions_path = Path(preds_value).resolve()\n",
    "    else:\n",
    "        predictions_path = None\n",
    "    best_details = {\n",
    "        \"model_id\": best_model_id,\n",
    "        \"model_display\": best_model_display,\n",
    "        \"run_name\": best_run_name,\n",
    "        \"model_dir\": best_model_dir,\n",
    "        \"run_dir\": best_run_dir,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"predictions_path\": predictions_path,\n",
    "    }\n",
    "    analysis_state[\"best_model_details\"] = best_details\n",
    "relative_model_dir = to_relative_path(best_details[\"model_dir\"], config.project_root)\n",
    "summary_lines = [\n",
    "    f\"**Best model context**: `{best_details['model_display']}` (`{best_details['model_id']}`) from run `{best_details['run_name']}`\",\n",
    "    \"\",\n",
    "    f\"- Model directory: `{relative_model_dir}`\",\n",
    "]\n",
    "if best_details.get(\"metrics_path\") is not None:\n",
    "    summary_lines.append(\n",
    "        f\"- Metrics file: `{to_relative_path(best_details['metrics_path'], config.project_root)}`\"\n",
    ")\n",
    "if best_details.get(\"predictions_path\") is not None:\n",
    "    summary_lines.append(\n",
    "        f\"- Predictions file: `{to_relative_path(best_details['predictions_path'], config.project_root)}`\"\n",
    ")\n",
    "display(Markdown(\"\\n\".join(summary_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ff037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:05.683041Z",
     "iopub.status.busy": "2025-12-30T16:10:05.682903Z",
     "iopub.status.idle": "2025-12-30T16:10:05.825204Z",
     "shell.execute_reply": "2025-12-30T16:10:05.824771Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_feature_importance_table(model_dir: Path) -> tuple[pd.DataFrame, Optional[Path]]:\n",
    "    \"\"\"Locate and standardise feature importances exported by the training pipeline.\"\"\"\n",
    "    patterns = [\n",
    "        \"feature_importance*.csv\",\n",
    "        \"feature_importances*.csv\",\n",
    "        \"feature_importance*.tsv\",\n",
    "        \"feature_importances*.tsv\",\n",
    "        \"feature_importance*.parquet\",\n",
    "        \"feature_importances*.parquet\",\n",
    "    ]\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns:\n",
    "        candidates.extend(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "    unique_candidates: list[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved in seen:\n",
    "            continue\n",
    "        if resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "    for candidate in sorted(unique_candidates):\n",
    "        try:\n",
    "            if candidate.suffix.lower() == \".parquet\":\n",
    "                df = pd.read_parquet(candidate)\n",
    "            else:\n",
    "                sep = \"\\t\" if candidate.suffix.lower() in {\".tsv\", \".txt\"} else \",\"\n",
    "                df = pd.read_csv(candidate, sep=sep)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {candidate.name}: failed to load ({exc})\")\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        lower_cols = {col.lower(): col for col in df.columns}\n",
    "        feature_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"feature\", \"feature_name\", \"name\", \"variable\", \"feature_id\", \"column\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        importance_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\n",
    "                \"importance\",\n",
    "                \"importance_score\",\n",
    "                \"importance_mean\",\n",
    "                \"score\",\n",
    "                \"value\",\n",
    "                \"gain\",\n",
    "                \"weight\",\n",
    "            )\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        if feature_col is None or importance_col is None:\n",
    "            continue\n",
    "        out = df.copy()\n",
    "        out.rename(columns={feature_col: \"feature\", importance_col: \"importance\"}, inplace=True)\n",
    "        out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "        out[\"importance\"] = pd.to_numeric(out[\"importance\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"feature\", \"importance\"])\n",
    "        if out.empty:\n",
    "            continue\n",
    "        extra_cols = [col for col in out.columns if col not in {\"feature\", \"importance\"}]\n",
    "        out = out[[\"feature\", \"importance\", *extra_cols]]\n",
    "        out.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        return out.reset_index(drop=True), candidate\n",
    "    return pd.DataFrame(columns=[\"feature\", \"importance\"]), None\n",
    "\n",
    "importance_df, importance_path = _load_feature_importance_table(best_details[\"model_dir\"])\n",
    "analysis_state[\"best_model_feature_importances\"] = importance_df\n",
    "analysis_state[\"best_model_feature_importances_path\"] = importance_path\n",
    "if importance_df.empty:\n",
    "    display(Markdown(\n",
    "        \"**Feature importances**: no compatible export found in the best-model directory. \"\n",
    "        \"Upload or regenerate a table named `feature_importance*.csv|tsv|parquet` to enable the following cells.\"\n",
    "    ))\n",
    "else:\n",
    "    note_lines = [\n",
    "        \"**Feature importances**: detected data source.\",\n",
    "    ]\n",
    "    if importance_path is not None:\n",
    "        note_lines.append(\n",
    "            f\"- Source file: `{to_relative_path(importance_path, config.project_root)}`\"\n",
    ")\n",
    "        note_lines.append(f\"- Rows: {len(importance_df):,}\")\n",
    "    display(Markdown(\"\\n\".join(note_lines)))\n",
    "    display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b802ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:05.826670Z",
     "iopub.status.busy": "2025-12-30T16:10:05.826512Z",
     "iopub.status.idle": "2025-12-30T16:10:06.231179Z",
     "shell.execute_reply": "2025-12-30T16:10:06.230740Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping distribution plot.\")\n",
    "    fig_feature_importance_dist = None\n",
    "else:\n",
    "    importance_values = importance_df[\"importance\"].astype(float)\n",
    "    importance_values = importance_values[np.isfinite(importance_values)]\n",
    "    if importance_values.empty:\n",
    "        print(\"Importance column contains no finite values; skipping distribution plot.\")\n",
    "        fig_feature_importance_dist = None\n",
    "    else:\n",
    "        fig_feature_importance_dist, ax = plt.subplots(figsize=(6.5, 4.2))\n",
    "        sns.histplot(importance_values, bins=40, kde=True, color=\"#377eb8\", ax=ax)\n",
    "        ax.set_title(\"Feature Importance Distribution | Best Model\")\n",
    "        ax.set_xlabel(\"Importance score\")\n",
    "        ax.set_ylabel(\"Feature count\")\n",
    "        sns.despine(fig_feature_importance_dist, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        display(fig_feature_importance_dist)\n",
    "        plt.close(fig_feature_importance_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fded366",
   "metadata": {},
   "source": [
    "### Top Feature Importance (Best Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95611ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:06.232746Z",
     "iopub.status.busy": "2025-12-30T16:10:06.232584Z",
     "iopub.status.idle": "2025-12-30T16:10:06.463988Z",
     "shell.execute_reply": "2025-12-30T16:10:06.463527Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "best_details = analysis_state.get(\"best_model_details\", {})\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping top-feature plot.\")\n",
    "    fig_feature_importance_top = None\n",
    "else:\n",
    "    top_n = min(20, len(importance_df))\n",
    "    top_df = importance_df.nlargest(top_n, \"importance\").copy()\n",
    "    top_df[\"feature\"] = top_df[\"feature\"].astype(str)\n",
    "    fig_height = max(4.0, 0.35 * top_n + 1.0)\n",
    "    fig_feature_importance_top, ax = plt.subplots(figsize=(7.5, fig_height))\n",
    "    sns.barplot(\n",
    "        data=top_df,\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        color=\"#4C78A8\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    model_label = best_details.get(\"model_display\", \"Best Model\")\n",
    "    ax.set_title(f\"Top Feature Importance | {model_label}\")\n",
    "    ax.set_xlabel(\"Importance score\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    sns.despine(fig_feature_importance_top, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "register_figure(FIGURES, \"best_model_feature_importance_top\", fig_feature_importance_top)\n",
    "if fig_feature_importance_top is not None:\n",
    "    display(fig_feature_importance_top)\n",
    "    plt.close(fig_feature_importance_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630af67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:06.465565Z",
     "iopub.status.busy": "2025-12-30T16:10:06.465397Z",
     "iopub.status.idle": "2025-12-30T16:10:06.956123Z",
     "shell.execute_reply": "2025-12-30T16:10:06.955574Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "distance_col = None\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping TSS distance analysis.\")\n",
    "else:\n",
    "    candidate_cols = [\n",
    "        \"distance_to_tss_bp\",\n",
    "        \"distance_to_tss\",\n",
    "        \"distance_bp\",\n",
    "        \"tss_distance_bp\",\n",
    "    ]\n",
    "    for candidate in candidate_cols:\n",
    "        if candidate in importance_df.columns:\n",
    "            distance_col = candidate\n",
    "            break\n",
    "    if distance_col is None:\n",
    "        bin_pattern = re.compile(r\"bin_(-?\\d+)_to_(-?\\d+)\")\n",
    "        def _infer_distance(feature_name: str) -> Optional[float]:\n",
    "            if not isinstance(feature_name, str):\n",
    "                return None\n",
    "            token = feature_name.split(\"|\", 1)[-1]\n",
    "            match = bin_pattern.search(token)\n",
    "            if match:\n",
    "                start_bp = float(match.group(1))\n",
    "                end_bp = float(match.group(2))\n",
    "                return 0.5 * (start_bp + end_bp)\n",
    "            return None\n",
    "        inferred = importance_df[\"feature\"].map(_infer_distance)\n",
    "        if inferred.notna().any():\n",
    "            importance_df = importance_df.copy()\n",
    "            importance_df[\"distance_to_tss_bp\"] = inferred\n",
    "            distance_col = \"distance_to_tss_bp\"\n",
    "            analysis_state[\"best_model_feature_importances\"] = importance_df\n",
    "    if distance_col is None:\n",
    "        print(\"Unable to derive feature-to-TSS distances from the available data; skipping scatter plot.\")\n",
    "    else:\n",
    "        analysis_state[\"feature_importance_distance_column\"] = distance_col\n",
    "        valid = importance_df.dropna(subset=[\"importance\", distance_col]).copy()\n",
    "        if valid.empty:\n",
    "            print(\"No features contained both importance scores and TSS distances.\")\n",
    "        else:\n",
    "            valid[\"distance_kb\"] = valid[distance_col].astype(float) / 1_000.0\n",
    "            corr_value = valid[[\"distance_kb\", \"importance\"]].corr(method=\"spearman\").loc[\"distance_kb\", \"importance\"]\n",
    "            fig_distance, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "            sns.scatterplot(\n",
    "                data=valid,\n",
    "                x=\"distance_kb\",\n",
    "                y=\"importance\",\n",
    "                s=32,\n",
    "                alpha=0.6,\n",
    "                edgecolor=\"none\",\n",
    "                color=\"#4daf4a\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            sns.regplot(\n",
    "                data=valid,\n",
    "                x=\"distance_kb\",\n",
    "                y=\"importance\",\n",
    "                scatter=False,\n",
    "                lowess=True,\n",
    "                color=\"#984ea3\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.axvline(0.0, color=\"#999999\", linestyle=\"--\", linewidth=1)\n",
    "            ax.set_xlabel(\"Distance to TSS (kb)\")\n",
    "            ax.set_ylabel(\"Feature importance\")\n",
    "            ax.set_title(\"Feature Importance vs. Distance to TSS | Best Model\")\n",
    "            ax.text(\n",
    "                0.01,\n",
    "                0.98,\n",
    "                f\"Spearman r = {corr_value:.3f}\",\n",
    "                transform=ax.transAxes,\n",
    "                ha=\"left\",\n",
    "                va=\"top\",\n",
    "                fontsize=10,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.7, edgecolor=\"#cccccc\"),\n",
    "            )\n",
    "            sns.despine(fig_distance, left=True, bottom=True)\n",
    "            plt.tight_layout()\n",
    "            display(fig_distance)\n",
    "            plt.close(fig_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b6946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:06.957962Z",
     "iopub.status.busy": "2025-12-30T16:10:06.957558Z",
     "iopub.status.idle": "2025-12-30T16:10:06.982327Z",
     "shell.execute_reply": "2025-12-30T16:10:06.981880Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping ATAC peak view.\")\n",
    "else:\n",
    "    feature_series = importance_df[\"feature\"].astype(str)\n",
    "    if \"feature_type\" in importance_df.columns:\n",
    "        atac_mask = importance_df[\"feature_type\"].astype(str).str.contains(\"atac|peak\", case=False, na=False)\n",
    "    else:\n",
    "        atac_mask = feature_series.str.contains(\"peak\", case=False, na=False)\n",
    "    atac_df = importance_df[atac_mask].copy()\n",
    "    if atac_df.empty:\n",
    "        print(\"No ATAC-related feature names detected; adjust filtering logic if alternative naming is used.\")\n",
    "    else:\n",
    "        top_atac = atac_df.nlargest(min(25, len(atac_df)), \"importance\")\n",
    "        fig_atac, ax = plt.subplots(figsize=(9, max(4, 0.35 * len(top_atac))))\n",
    "        sns.barplot(\n",
    "            data=top_atac,\n",
    "            x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            palette=\"crest\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Top ATAC Feature Importances | Best Model\")\n",
    "        ax.set_xlabel(\"Importance score\")\n",
    "        ax.set_ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        display(fig_atac)\n",
    "        plt.close(fig_atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fab213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:06.983662Z",
     "iopub.status.busy": "2025-12-30T16:10:06.983509Z",
     "iopub.status.idle": "2025-12-30T16:10:07.000182Z",
     "shell.execute_reply": "2025-12-30T16:10:06.999773Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "TOP_FEATURE_COUNT = 1_000\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; unable to derive top features.\")\n",
    "else:\n",
    "    top_n = min(TOP_FEATURE_COUNT, len(importance_df))\n",
    "    top_features_df = importance_df.nlargest(top_n, \"importance\").reset_index(drop=True)\n",
    "    analysis_state[\"best_model_top_features\"] = top_features_df\n",
    "    print(f\"Captured top {top_n} features by importance for the best model.\")\n",
    "    display(top_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c29e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:07.001573Z",
     "iopub.status.busy": "2025-12-30T16:10:07.001418Z",
     "iopub.status.idle": "2025-12-30T16:10:07.019622Z",
     "shell.execute_reply": "2025-12-30T16:10:07.019199Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "distance_col = analysis_state.get(\"feature_importance_distance_column\")\n",
    "distance_window_kb = 50\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; cannot filter by TSS proximity.\")\n",
    "elif distance_col is None or distance_col not in importance_df.columns:\n",
    "    print(\"No inferred distance-to-TSS column available; run the previous distance cell first or provide the column explicitly.\")\n",
    "else:\n",
    "    window_mask = importance_df[distance_col].abs() <= distance_window_kb * 1_000\n",
    "    subset_df = importance_df[window_mask].copy()\n",
    "    if subset_df.empty:\n",
    "        print(\n",
    "            f\"No features fall within {distance_window_kb} kb of the TSS according to column '{distance_col}'.\"\n",
    ")\n",
    "    else:\n",
    "        subset_df.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        analysis_state[\"best_model_features_within_window\"] = subset_df\n",
    "        print(\n",
    "            f\"Identified {len(subset_df):,} features within {distance_window_kb} kb of the TSS (column '{distance_col}').\"\n",
    ")\n",
    "        display(subset_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebee29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:07.021033Z",
     "iopub.status.busy": "2025-12-30T16:10:07.020877Z",
     "iopub.status.idle": "2025-12-30T16:10:07.081426Z",
     "shell.execute_reply": "2025-12-30T16:10:07.081007Z"
    }
   },
   "outputs": [],
   "source": [
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "top_features_df = analysis_state.get(\"best_model_top_features\")\n",
    "subset_df = analysis_state.get(\"best_model_features_within_window\")\n",
    "importance_path = analysis_state.get(\"best_model_feature_importances_path\")\n",
    "distance_col = analysis_state.get(\"feature_importance_distance_column\")\n",
    "distance_window_kb = 50\n",
    "if best_details is None:\n",
    "    print(\"Best model context missing; rerun the first cell in Section 13.\")\n",
    "else:\n",
    "    run_config_path = best_details[\"run_dir\"] / \"run_configuration.json\"\n",
    "    lines = [\n",
    "        \"**Execution scaffolding for reruns**\",\n",
    "        \"\",\n",
    "        f\"- Baseline config JSON: `{to_relative_path(run_config_path, config.project_root)}`\",\n",
    "        f\"- Suggested CLI template: `python -m spear.cli --config-json {to_relative_path(run_config_path, config.project_root)} --models {best_details['model_id']} --run-name {best_details['run_name']}_experimental`\",\n",
    "    ]\n",
    "    if importance_path is not None:\n",
    "        lines.append(\n",
    "            f\"- Feature importance table: `{to_relative_path(importance_path, config.project_root)}`\"\n",
    "        )\n",
    "    if top_features_df is not None and not top_features_df.empty:\n",
    "        lines.append(\n",
    "            \"- Export `analysis_state[\\\"best_model_top_features\\\"]` to CSV/TSV and feed it into your data loader to mimic a top-1k feature run.\",\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\"- Top-1k feature shortlist unavailable; populate feature importances first.\")\n",
    "    if subset_df is not None and not subset_df.empty and distance_col:\n",
    "        lines.append(\n",
    "            f\"- Features within {distance_window_kb} kb (`{distance_col}`) cached in `analysis_state['best_model_features_within_window']`.\",\n",
    "        )\n",
    "        lines.append(\n",
    "            f\"  Update `TrainingConfig.window_bp` to {distance_window_kb * 1_000:,} ({distance_window_kb} kb) in the JSON before re-running.\",\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\n",
    "            f\"- No cached subset for the {distance_window_kb} kb window; rerun the distance cell after providing TSS metadata.\",\n",
    "        )\n",
    "    lines.append(\n",
    "        \"- Reminder: the notebook does not launch training jobs automatically; please run the CLI in a new terminal or submit via SLURM.\",\n",
    "    )\n",
    "    display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167791f",
   "metadata": {},
   "source": [
    "## 15. Resource Usage Comparison\n",
    "\n",
    "Summarize SLURM log resource usage across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15fff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:07.083122Z",
     "iopub.status.busy": "2025-12-30T16:10:07.082951Z",
     "iopub.status.idle": "2025-12-30T16:10:08.393496Z",
     "shell.execute_reply": "2025-12-30T16:10:08.393044Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "log_dir = config.project_root / \"output\" / \"logs\"\n",
    "log_paths = sorted(log_dir.glob(\"spear_cellwise_chunk*.out\"))\n",
    "if not log_paths:\n",
    "    print(\"No Slurm logs found; skipping resource usage summary.\")\n",
    "    fig_resource = None\n",
    "else:\n",
    "    if \"analysis_state\" not in globals():\n",
    "        analysis_state = {}\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    display_lookup: dict[str, str] = {}\n",
    "    model_id_lookup: dict[str, str] = {}\n",
    "    if isinstance(run_df, pd.DataFrame) and not run_df.empty:\n",
    "        if \"model_display\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            display_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_display\"]))\n",
    "        if \"model_id\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            model_id_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_id\"]))\n",
    "\n",
    "    rss_pattern = re.compile(r\"Resource snapshot\\s*\\|\\s*([^|]+)\\|\\s*rss=([0-9.]+)\\s*GiB\", re.IGNORECASE)\n",
    "    rows: list[dict[str, object]] = []\n",
    "    for path in log_paths:\n",
    "        try:\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                lines = fh.readlines()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        run_name = None\n",
    "        max_rss = None\n",
    "        peak_stage = None\n",
    "        for line in lines:\n",
    "            if \"run_name=\" in line:\n",
    "                idx = line.find(\"run_name=\")\n",
    "                token_start = idx + len(\"run_name=\")\n",
    "                rest = line[token_start:]\n",
    "                import re\n",
    "                candidate_match = re.search(r\"[\\w\\-]+\", rest)\n",
    "                if candidate_match:\n",
    "                    run_name = candidate_match.group()\n",
    "            match = rss_pattern.search(line)\n",
    "            if match:\n",
    "                stage_str = match.group(1).strip()\n",
    "                rss_val = float(match.group(2))\n",
    "                if max_rss is None or rss_val > max_rss:\n",
    "                    max_rss = rss_val\n",
    "                    peak_stage = stage_str\n",
    "\n",
    "        if run_name and max_rss is not None:\n",
    "            display_name = display_lookup.get(run_name, run_name)\n",
    "            model_id = model_id_lookup.get(run_name, \"\")\n",
    "            is_cpu_run = \"cpu\" in run_name.lower() or \"ridge\" in run_name.lower() or \"xgboost\" in run_name.lower()\n",
    "            device_label = \"CPU\" if is_cpu_run else \"GPU\"\n",
    "            label = f\"{display_name}\\n{run_name}\"\n",
    "            rows.append({\n",
    "                \"run_label\": run_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"peak_rss_gib\": max_rss,\n",
    "                \"peak_stage\": peak_stage,\n",
    "                \"resolved_name\": display_name,\n",
    "                \"accelerator\": device_label,\n",
    "                \"label\": label,\n",
    "                \"log_file\": path.name,\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        resource_df = pd.DataFrame(rows)\n",
    "        sorted_df = resource_df.sort_values(\"peak_rss_gib\", ascending=False)\n",
    "        analysis_state[\"resource_usage\"] = sorted_df\n",
    "        table = sorted_df.rename(\n",
    "            columns={\n",
    "                \"resolved_name\": \"Model label\",\n",
    "                \"run_label\": \"Run name\",\n",
    "                \"model_id\": \"Model ID\",\n",
    "                \"accelerator\": \"Accelerator\",\n",
    "                \"peak_rss_gib\": \"Peak RSS (GiB)\",\n",
    "                \"peak_stage\": \"Peak stage\",\n",
    "                \"log_file\": \"Log file\",\n",
    "            }\n",
    ")\n",
    "        display(\n",
    "            table[\n",
    "                [\n",
    "                    \"Model label\",\n",
    "                    \"Run name\",\n",
    "                    \"Model ID\",\n",
    "                    \"Accelerator\",\n",
    "                    \"Peak RSS (GiB)\",\n",
    "                    \"Peak stage\",\n",
    "                    \"Log file\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "        valid = sorted_df.dropna(subset=[\"peak_rss_gib\"]).copy()\n",
    "        if valid.empty:\n",
    "            print(\"Resource snapshots not yet present in the logs; rerun after the jobs emit resource metrics.\")\n",
    "        else:\n",
    "            color_map = {\"CPU\": \"#4C72B0\", \"GPU\": \"#DD8452\"}\n",
    "            colors = valid[\"accelerator\"].map(color_map).fillna(\"#808080\")\n",
    "            fig_height = max(3.8, 0.4 * len(valid))\n",
    "            fig_resource, ax = plt.subplots(figsize=(9, fig_height))\n",
    "            bars = ax.barh(valid[\"label\"], valid[\"peak_rss_gib\"], color=colors)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Peak RSS (GiB)\")\n",
    "            ax.set_ylabel(\"Model run\")\n",
    "            ax.set_title(\"Peak Memory Usage Across SPEAR Cell-wise Runs\")\n",
    "            xmax = valid[\"peak_rss_gib\"].max()\n",
    "            if xmax <= 0:\n",
    "                xmax = 1.0\n",
    "            ax.set_xlim(0, xmax * 1.15)\n",
    "            offset = max(1.0, xmax * 0.03)\n",
    "            for bar, (_, row) in zip(bars, valid.iterrows()):\n",
    "                stage_note = \"\"\n",
    "                if isinstance(row[\"peak_stage\"], str) and row[\"peak_stage\"]:\n",
    "                    stage_note = f\" @ {row['peak_stage']}\"\n",
    "                ax.text(\n",
    "                    bar.get_width() + offset,\n",
    "                    bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{row['peak_rss_gib']:.1f} GiB{stage_note}\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8.5,\n",
    "                    color=\"#1f1f1f\",\n",
    "                )\n",
    "            legend_handles = [\n",
    "                Patch(color=color_map[key], label=key)\n",
    "                for key in sorted(valid[\"accelerator\"].dropna().unique())\n",
    "                if key in color_map\n",
    "            ]\n",
    "            if legend_handles:\n",
    "                ax.legend(handles=legend_handles, title=\"Accelerator\", loc=\"lower right\")\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            fig_resource.tight_layout()\n",
    "            register_figure(FIGURES, \"resource_usage_comparison\", fig_resource)\n",
    "            display(fig_resource)\n",
    "            plt.close(fig_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04633d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Summarize per-model runtime + peak resource usage from per-run logs.\n",
    "analysis_state = globals().get(\"analysis_state\", {})\n",
    "if not isinstance(analysis_state, dict):\n",
    "    analysis_state = {}\n",
    "    globals()[\"analysis_state\"] = analysis_state\n",
    "\n",
    "log_dir = config.project_root / \"output\" / \"logs\"\n",
    "log_paths = sorted(log_dir.glob(\"spear_*_compute_*.log\"))\n",
    "if not log_paths:\n",
    "    print(\"No model run logs found under output/logs; skipping resource summary.\")\n",
    "else:\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    display_lookup: dict[str, str] = {}\n",
    "    model_lookup: dict[str, str] = {}\n",
    "    if isinstance(run_df, pd.DataFrame) and not run_df.empty:\n",
    "        if \"model_display\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            display_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_display\"]))\n",
    "        if \"model_id\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            model_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_id\"]))\n",
    "\n",
    "    run_name_pattern = re.compile(r\"run_name=([\\w\\-]+)\")\n",
    "    resource_pattern = re.compile(\n",
    "        r\"Resource snapshot\\s*\\|\\s*(?P<context>[^|]+)\\|\\s*rss=(?P<rss>[0-9.]+)\\s*GiB\\s*\\|\\s*cpu%=(?P<cpu>[0-9.]+)\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    timestamp_pattern = re.compile(r\"^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\")\n",
    "\n",
    "    rows: list[dict[str, object]] = []\n",
    "    for path in log_paths:\n",
    "        try:\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                lines = fh.readlines()\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {path.name}: failed to read ({exc}).\")\n",
    "            continue\n",
    "\n",
    "        run_name = None\n",
    "        max_rss = None\n",
    "        max_cpu = None\n",
    "        start_ts = None\n",
    "        end_ts = None\n",
    "        end_ts_complete = None\n",
    "        for line in lines:\n",
    "            if run_name is None and \"run_name=\" in line:\n",
    "                match = run_name_pattern.search(line)\n",
    "                if match:\n",
    "                    run_name = match.group(1)\n",
    "            if start_ts is None:\n",
    "                ts_match = timestamp_pattern.match(line)\n",
    "                if ts_match:\n",
    "                    try:\n",
    "                        start_ts = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                    except ValueError:\n",
    "                        start_ts = None\n",
    "            ts_match = timestamp_pattern.match(line)\n",
    "            if ts_match:\n",
    "                try:\n",
    "                    end_ts = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            if \"RUN_COMPLETE_STATUS\" in line and ts_match:\n",
    "                try:\n",
    "                    end_ts_complete = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                except ValueError:\n",
    "                    end_ts_complete = None\n",
    "            res_match = resource_pattern.search(line)\n",
    "            if res_match:\n",
    "                rss_val = float(res_match.group(\"rss\"))\n",
    "                cpu_val = float(res_match.group(\"cpu\"))\n",
    "                if max_rss is None or rss_val > max_rss:\n",
    "                    max_rss = rss_val\n",
    "                if max_cpu is None or cpu_val > max_cpu:\n",
    "                    max_cpu = cpu_val\n",
    "\n",
    "        final_end = end_ts_complete or end_ts\n",
    "        total_seconds = None\n",
    "        if start_ts and final_end:\n",
    "            delta = final_end - start_ts\n",
    "            total_seconds = delta.total_seconds()\n",
    "\n",
    "        if run_name:\n",
    "            display_name = display_lookup.get(run_name, run_name)\n",
    "            model_id = model_lookup.get(run_name, \"\")\n",
    "            rows.append({\n",
    "                \"run_name\": run_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"display_name\": display_name,\n",
    "                \"log_file\": path.name,\n",
    "                \"peak_rss_gib\": max_rss,\n",
    "                \"peak_cpu_pct\": max_cpu,\n",
    "                \"start_time\": start_ts,\n",
    "                \"end_time\": final_end,\n",
    "                \"runtime_seconds\": total_seconds,\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        resource_df = pd.DataFrame(rows)\n",
    "        sorted_df = resource_df.sort_values(\"runtime_seconds\", ascending=False, na_position=\"last\")\n",
    "        analysis_state[\"resource_runtime\"] = sorted_df\n",
    "\n",
    "        table = sorted_df.rename(\n",
    "            columns={\n",
    "                \"display_name\": \"Model display name\",\n",
    "                \"run_name\": \"Run name\",\n",
    "                \"model_id\": \"Model ID\",\n",
    "                \"log_file\": \"Log file\",\n",
    "                \"peak_rss_gib\": \"Peak RSS (GiB)\",\n",
    "                \"peak_cpu_pct\": \"Peak CPU %\",\n",
    "                \"runtime_seconds\": \"Runtime (s)\",\n",
    "            }\n",
    "        )\n",
    "        display(\n",
    "            table[\n",
    "                [\n",
    "                    \"Model display name\",\n",
    "                    \"Run name\",\n",
    "                    \"Model ID\",\n",
    "                    \"Peak RSS (GiB)\",\n",
    "                    \"Peak CPU %\",\n",
    "                    \"Runtime (s)\",\n",
    "                    \"Log file\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        valid = sorted_df.dropna(subset=[\"runtime_seconds\"]).copy()\n",
    "        if not valid.empty:\n",
    "            valid[\"runtime_hours\"] = valid[\"runtime_seconds\"] / 3600.0\n",
    "            fig_runtime, ax = plt.subplots(figsize=(9, max(3.8, 0.4 * len(valid))))\n",
    "            ax.barh(valid[\"display_name\"], valid[\"runtime_hours\"], color=\"#55A868\")\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Runtime (hours)\")\n",
    "            ax.set_ylabel(\"Model run\")\n",
    "            ax.set_title(\"Per-run Total Runtime (log-sourced)\")\n",
    "            xmax = valid[\"runtime_hours\"].max()\n",
    "            if xmax <= 0:\n",
    "                xmax = 1.0\n",
    "            ax.set_xlim(0, xmax * 1.15)\n",
    "            offset = max(0.1, xmax * 0.03)\n",
    "            for idx, row in valid.iterrows():\n",
    "                ax.text(\n",
    "                    row[\"runtime_hours\"] + offset,\n",
    "                    row[\"display_name\"],\n",
    "                    f\"{row['runtime_hours']:.2f} h\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8.5,\n",
    "                    color=\"#1f1f1f\",\n",
    "                )\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            fig_runtime.tight_layout()\n",
    "            register_figure(FIGURES, \"runtime_comparison\", fig_runtime)\n",
    "            display(fig_runtime)\n",
    "            plt.close(fig_runtime)\n",
    "    else:\n",
    "        print(\"No matching run logs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f0d3c",
   "metadata": {},
   "source": [
    "## Appendix: Feature Importance Quick Guide\n",
    "\n",
    "Extra diagnostics for exploring per-gene feature importance outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fbe91",
   "metadata": {},
   "source": [
    "### Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b88494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:08.398892Z",
     "iopub.status.busy": "2025-12-30T16:10:08.398729Z",
     "iopub.status.idle": "2025-12-30T16:10:10.122001Z",
     "shell.execute_reply": "2025-12-30T16:10:10.121504Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from spear.visualization import plot_per_gene_feature_panel, plot_cumulative_importance_overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041128d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.123820Z",
     "iopub.status.busy": "2025-12-30T16:10:10.123565Z",
     "iopub.status.idle": "2025-12-30T16:10:10.127134Z",
     "shell.execute_reply": "2025-12-30T16:10:10.126764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the best-performing model from the analysis summary\n",
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "if best_details is None:\n",
    "    raise RuntimeError(\"Best-model details missing; rerun Sections 4-5 before this cell.\")\n",
    "RUN_DIR = Path(best_details[\"model_dir\"]).resolve()\n",
    "RUN_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2127128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.128398Z",
     "iopub.status.busy": "2025-12-30T16:10:10.128256Z",
     "iopub.status.idle": "2025-12-30T16:10:10.217650Z",
     "shell.execute_reply": "2025-12-30T16:10:10.217225Z"
    }
   },
   "outputs": [],
   "source": [
    "aggregate_df = pd.read_csv(RUN_DIR / \"feature_importances_mean.csv\")\n",
    "per_gene_df = pd.read_csv(RUN_DIR / \"feature_importance_per_gene_summary.csv\")\n",
    "print(f'Aggregate features: {len(aggregate_df):,}')\n",
    "print(f'Genes summarized: {len(per_gene_df):,}')\n",
    "display(aggregate_df.head())\n",
    "display(per_gene_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7e102",
   "metadata": {},
   "source": [
    "### Dataset-level overlays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08ca14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.219115Z",
     "iopub.status.busy": "2025-12-30T16:10:10.218957Z",
     "iopub.status.idle": "2025-12-30T16:10:10.262347Z",
     "shell.execute_reply": "2025-12-30T16:10:10.261871Z"
    }
   },
   "outputs": [],
   "source": [
    "overlay_path = RUN_DIR / \"feature_importance_distance_overview.png\"\n",
    "scatter_path = RUN_DIR / \"feature_importance_vs_tss_distance.png\"\n",
    "for label, path in (('Cumulative overlay', overlay_path), ('Scatter', scatter_path)):\n",
    "    if path.exists():\n",
    "        display(Image(filename=str(path)))\n",
    "    else:\n",
    "        print(f'No file found for {label}: {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1689f1",
   "metadata": {},
   "source": [
    "### Per-gene panels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c31a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.264124Z",
     "iopub.status.busy": "2025-12-30T16:10:10.263949Z",
     "iopub.status.idle": "2025-12-30T16:10:10.347788Z",
     "shell.execute_reply": "2025-12-30T16:10:10.347272Z"
    }
   },
   "outputs": [],
   "source": [
    "panel_dir = RUN_DIR / \"per_gene_panels\"\n",
    "panel_paths = sorted(panel_dir.glob('*.png'))\n",
    "print(f'Found {len(panel_paths)} per-gene panels')\n",
    "for path in panel_paths[:4]:\n",
    "    display(Image(filename=str(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f6a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.349362Z",
     "iopub.status.busy": "2025-12-30T16:10:10.349206Z",
     "iopub.status.idle": "2025-12-30T16:10:10.934991Z",
     "shell.execute_reply": "2025-12-30T16:10:10.934439Z"
    }
   },
   "outputs": [],
   "source": [
    "def render_custom_panel(gene_name: str, top_n: int = 12):\n",
    "    subset = aggregate_df[aggregate_df.get('gene_name') == gene_name]\n",
    "    if subset.empty:\n",
    "        raise ValueError(f'No features found for gene {gene_name}')\n",
    "    out_path = RUN_DIR / \"per_gene_panels\" / f'custom_{gene_name}.png'\n",
    "    plot_per_gene_feature_panel(subset, gene_name, out_path, top_n=top_n)\n",
    "    display(Image(filename=str(out_path)))\n",
    "\n",
    "example_gene = per_gene_df.sort_values('importance_mean_sum', ascending=False)['gene'].iloc[0]\n",
    "render_custom_panel(example_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717ff70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:10:10.937897Z",
     "iopub.status.busy": "2025-12-30T16:10:10.937735Z",
     "iopub.status.idle": "2025-12-30T16:10:11.225535Z",
     "shell.execute_reply": "2025-12-30T16:10:11.224958Z"
    }
   },
   "outputs": [],
   "source": [
    "def recompute_overlay(max_distance_kb: float | None = None):\n",
    "    df = aggregate_df.dropna(subset=['importance_mean', 'distance_to_tss_kb'])\n",
    "    if max_distance_kb is not None:\n",
    "        df = df[df['distance_to_tss_kb'].abs() <= max_distance_kb]\n",
    "    out_path = RUN_DIR / \"feature_importance_distance_overlay_custom.png\"\n",
    "    plot_cumulative_importance_overlay(df['importance_mean'], df['distance_to_tss_kb'], out_path, 'Custom FI cumulative profile')\n",
    "    display(Image(filename=str(out_path)))\n",
    "\n",
    "recompute_overlay(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grn_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
