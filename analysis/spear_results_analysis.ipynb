{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcd14c1",
   "metadata": {},
   "source": [
    "# SPEAR Results Analysis\n",
    "\n",
    "Single-cell Prediction of gene Expression from ATAC-seq Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c4fef",
   "metadata": {},
   "source": [
    "This notebook generates SPEAR results figures, diagnostics, and export-ready tables for model comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427303",
   "metadata": {},
   "source": [
    "## Prereqs\n",
    "\n",
    "- Place run outputs under `output/results/spear_results/` (one subfolder per run with `models/` and metrics CSVs).\n",
    "\n",
    "- Ensure logs exist under `output/logs/` with `spear_*` naming if you want resource plots.\n",
    "\n",
    "- Keep `analysis/model_name_lookup.tsv` present (already tracked in repo).\n",
    "\n",
    "- Install dependencies (see README) and select the `spear_env` kernel.\n",
    "\n",
    "### How to run\n",
    "\n",
    "1. Open this notebook inside the repo root.\n",
    "\n",
    "2. Adjust the run include globs if you want to filter; otherwise leave defaults.\n",
    "\n",
    "3. Run all cells top-to-bottom after outputs are in place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012eb1a1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Imports, plotting defaults, and global configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, replace\n",
    "from datetime import datetime\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {'analysis', 'scripts'}:\n",
    "    project_root = project_root.parent\n",
    "src_root = project_root / 'src'\n",
    "for candidate in (src_root, project_root):\n",
    "    if str(candidate) not in sys.path:\n",
    "        sys.path.insert(0, str(candidate))\n",
    "\n",
    "from typing import Iterable, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "# Configure plotting defaults for consistent styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "sns.set_context(\"paper\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.dpi\": 160, \"savefig.dpi\": 320})\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.display.width = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403109de",
   "metadata": {},
   "source": [
    "## 2. Analysis Configuration\n",
    "\n",
    "Project paths, figure/report locations, and default analysis parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralised configuration for the notebook run\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    project_root: Path\n",
    "    results_root: Path\n",
    "    lookup_path: Path\n",
    "    fig_dir: Path\n",
    "    reports_dir: Path\n",
    "    run_include_globs: tuple[str, ...] = (\"*\",)\n",
    "    run_exclude: tuple[str, ...] = tuple()\n",
    "    primary_split: str = \"test\"\n",
    "    val_split: str = \"val\"\n",
    "    train_split: str = \"train\"\n",
    "    top_gene_count: int = 15\n",
    "    top_model_count: int = 3\n",
    "    random_seed: int = 7\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Create required directories if they are missing.\"\"\"\n",
    "        self.fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.lookup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while project_root.name in {\"analysis\", \"scripts\"}:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "config = AnalysisConfig(\n",
    "    project_root=project_root,\n",
    "    results_root=project_root / \"output\" / \"results\" / \"spear_results\",\n",
    "    lookup_path=project_root / \"analysis\" / \"model_name_lookup.tsv\",\n",
    "    fig_dir=project_root / \"analysis\" / \"figs\" / \"spear_results_analysis\",\n",
    "    reports_dir=project_root / \"analysis\" / \"reports\",\n",
    "    run_include_globs=(\"*\",),\n",
    "    run_exclude=tuple(),\n",
    "    random_seed=7,\n",
    "    top_gene_count=15,\n",
    "    top_model_count=3,\n",
    " )\n",
    "\n",
    "if not config.results_root.exists():\n",
    "    raise FileNotFoundError(f\"Results directory missing: {config.results_root}\")\n",
    "if not config.lookup_path.exists():\n",
    "    # Seed the lookup table if it is missing so later steps can append to it.\n",
    "    pd.DataFrame({\n",
    "        \"model_id\": [],\n",
    "        \"model_display_name\": [],\n",
    "        \"model_short_name\": [],\n",
    "    }).to_csv(\n",
    "        config.lookup_path, sep=\"\\t\", index=False\n",
    "    )\n",
    "\n",
    "np.random.seed(config.random_seed)\n",
    "rng = np.random.default_rng(config.random_seed)\n",
    "FIGURES: dict[str, plt.Figure] = {}\n",
    "TABLES: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "# Dataset label appended to plot titles when available.\n",
    "dataset_title_suffix: str | None = None\n",
    "analysis_metadata: dict[str, object] = {\n",
    "    \"generated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"project_root\": config.project_root,\n",
    "    \"results_root\": config.results_root,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970dc054",
   "metadata": {},
   "source": [
    "## 2a. Optional Run Selection Overrides\n",
    "\n",
    "Use these lists to focus the notebook on a subset of runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick toggles (edit these only)\n",
    "# - TARGET_DATASET: switch between embryonic/endothelial\n",
    "# - USE_1000PLUS100: include 1000-gene runs with 100-gene fallback\n",
    "# - PREFER_1000_FALLBACK_100: keep most recent 1000-gene per model, fallback to 100-gene only if missing\n",
    "TARGET_DATASET: str = \"embryonic\"  # \"embryonic\" or \"endothelial\"\n",
    "USE_1000PLUS100: bool = True  # True = 1000 genes with 100 genes fallback\n",
    "PREFER_1000_FALLBACK_100: bool = True  # de-duplicate by model\n",
    "\n",
    "# Populate one or both of the lists below to narrow the analysis scope.\n",
    "RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_1000genes_k5_pg20_20251108_xgboost\",\n",
    "    # Path(\"output/results/spear_results/spear_1000genes_k5_pg20_20251108_random_forest\"),\n",
    "]\n",
    "RUN_GLOB_SELECTION: list[str] = []\n",
    "\n",
    "if USE_1000PLUS100:\n",
    "    RUN_GLOB_SELECTION = [\n",
    "        f\"*{TARGET_DATASET}*1000genes*\",\n",
    "        f\"*{TARGET_DATASET}*100genes*\",\n",
    "    ]\n",
    "    RUN_SUBSET_LABEL: Optional[str] = f\"{TARGET_DATASET}_1000plus100\"\n",
    "    RUN_SUBSET_DESCRIPTION: Optional[str] = f\"{TARGET_DATASET.title()} | 1000 genes (+ 100 genes backup)\"\n",
    "else:\n",
    "    RUN_GLOB_SELECTION = [\n",
    "        f\"*{TARGET_DATASET}*1000genes*\",\n",
    "    ]\n",
    "    RUN_SUBSET_LABEL: Optional[str] = f\"{TARGET_DATASET}_1000genes\"\n",
    "    RUN_SUBSET_DESCRIPTION: Optional[str] = f\"{TARGET_DATASET.title()} | 1000 genes\"\n",
    "\n",
    "\n",
    "# --- Do not edit below unless you want to change the filtering logic itself. ---\n",
    "# --- Do not edit below unless you want to change the filtering logic itself. ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565f8cb",
   "metadata": {},
   "source": [
    "## 3. Run Discovery Utilities\n",
    "\n",
    "Helpers for locating model outputs and attaching display metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da80cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical representation of a single trained model output folder\n",
    "@dataclass(frozen=True)\n",
    "class RunRecord:\n",
    "    run_name: str\n",
    "    model_id: str\n",
    "    run_path: Path\n",
    "    model_path: Path\n",
    "    metrics_path: Optional[Path]\n",
    "    predictions_path: Optional[Path]\n",
    "    training_history_path: Optional[Path]\n",
    "    model_display: Optional[str] = None\n",
    "\n",
    "\n",
    "LOOKUP_SPECIAL_CASES = {\n",
    "    \"cnn\": \"Convolutional Neural Network\",\n",
    "    \"rnn\": \"Recurrent Neural Network\",\n",
    "    \"lstm\": \"Long Short-Term Memory\",\n",
    "    \"mlp\": \"Multilayer Perceptron\",\n",
    "    \"svr\": \"Support Vector Regressor\",\n",
    "    \"ols\": \"Ordinary Least Squares\",\n",
    "    \"xgboost\": \"XGBoost\",\n",
    "    \"catboost\": \"CatBoost\",\n",
    "    \"hist_gradient_boosting\": \"Histogram Gradient Boosting\",\n",
    "    \"extra_trees\": \"Extra Trees\",\n",
    "    \"random_forest\": \"Random Forest\",\n",
    "    \"elastic_net\": \"Elastic Net\",\n",
    "}\n",
    "\n",
    "SHORT_NAME_FALLBACKS = {\n",
    "    \"Multilayer Perceptron\": \"MLP\",\n",
    "    \"Graph Neural Network\": \"GNN\",\n",
    "    \"Convolutional Neural Network\": \"CNN\",\n",
    "    \"Long Short-Term Memory Network\": \"LSTM\",\n",
    "    \"Recurrent Neural Network\": \"RNN\",\n",
    "    \"Transformer Encoder\": \"Transformer\",\n",
    "    \"Ordinary Least Squares\": \"OLS\",\n",
    "    \"Extra Trees\": \"Extra Trees\",\n",
    "    \"Random Forest\": \"Random Forest\",\n",
    "    \"Ridge Regression\": \"Ridge\",\n",
    "}\n",
    "\n",
    "MODEL_ID_TO_DISPLAY: dict[str, str] = {}\n",
    "MODEL_ID_TO_SHORT: dict[str, str] = {}\n",
    "MODEL_DISPLAY_TO_SHORT: dict[str, str] = {}\n",
    "\n",
    "\n",
    "def _default_short_name(display_name: str) -> str:\n",
    "    \"\"\"Generate a lightweight abbreviation when none is provided.\"\"\"\n",
    "    if not isinstance(display_name, str) or not display_name.strip():\n",
    "        return \"\"\n",
    "    tokens = [token for token in display_name.replace(\"(\", \" \").replace(\")\", \" \").split() if token]\n",
    "    if not tokens:\n",
    "        return display_name\n",
    "    acronym = \"\".join(token[0] for token in tokens if token and token[0].isalnum()).upper()\n",
    "    if 1 < len(acronym) <= 5:\n",
    "        return acronym\n",
    "    return display_name\n",
    "\n",
    "\n",
    "def compute_heatmap_limits(\n",
    "    values: pd.DataFrame | np.ndarray,\n",
    "    lower_percentile: float = 5.0,\n",
    "    upper_percentile: float = 95.0,\n",
    "    clip: tuple[float, float] = (0.0, 1.0),\n",
    "    min_buffer: float = 0.01,\n",
    ") -> tuple[float, float]:\n",
    "    # Derive consistent vmin/vmax bounds so heatmaps emphasise the dense value range.\n",
    "    data = np.asarray(values, dtype=float)\n",
    "    data = data[np.isfinite(data)]\n",
    "    if data.size == 0:\n",
    "        return clip\n",
    "    lower = np.percentile(data, lower_percentile)\n",
    "    upper = np.percentile(data, upper_percentile)\n",
    "    buffer = max(min_buffer, (upper - lower) * 0.05)\n",
    "    vmin = max(clip[0], lower - buffer)\n",
    "    vmax = min(clip[1], upper + buffer)\n",
    "    if vmin > vmax:\n",
    "        if clip[0] <= clip[1]:\n",
    "            vmin, vmax = clip\n",
    "        else:\n",
    "            vmin, vmax = float(data.min()), float(data.max())\n",
    "    if np.isclose(vmin, vmax):\n",
    "        spread = max(min_buffer, abs(vmin) * 0.1 or min_buffer)\n",
    "        vmin -= spread\n",
    "        vmax += spread\n",
    "        vmin = max(clip[0], vmin)\n",
    "        vmax = min(clip[1], vmax)\n",
    "        if vmin > vmax:\n",
    "            vmin, vmax = float(data.min()), float(data.max())\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "def to_short_name(name: str | None) -> str:\n",
    "    \"\"\"Return a concise display name for figure titles and filenames.\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    if name in MODEL_DISPLAY_TO_SHORT:\n",
    "        return MODEL_DISPLAY_TO_SHORT[name]\n",
    "    return SHORT_NAME_FALLBACKS.get(name, name)\n",
    "\n",
    "\n",
    "def _read_lookup_table(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame({\n",
    "            \"model_id\": pd.Series(dtype=\"string\"),\n",
    "            \"model_display_name\": pd.Series(dtype=\"string\"),\n",
    "            \"model_short_name\": pd.Series(dtype=\"string\"),\n",
    "        })\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    expected = {\"model_id\", \"model_display_name\"}\n",
    "    missing_cols = expected.difference(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Lookup table missing required columns: {sorted(missing_cols)}\")\n",
    "    if \"model_short_name\" not in df.columns:\n",
    "        df[\"model_short_name\"] = df[\"model_display_name\"].map(_default_short_name)\n",
    "    else:\n",
    "        df[\"model_short_name\"] = df[\"model_short_name\"].fillna(\"\")\n",
    "        missing_short = df[\"model_short_name\"].str.strip() == \"\"\n",
    "        df.loc[missing_short, \"model_short_name\"] = df.loc[missing_short, \"model_display_name\"].map(_default_short_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _update_model_name_maps(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Cache name lookups for downstream plotting helpers.\"\"\"\n",
    "    global MODEL_ID_TO_DISPLAY, MODEL_ID_TO_SHORT, MODEL_DISPLAY_TO_SHORT\n",
    "    if df.empty:\n",
    "        MODEL_ID_TO_DISPLAY = {}\n",
    "        MODEL_ID_TO_SHORT = {}\n",
    "        MODEL_DISPLAY_TO_SHORT = {}\n",
    "        return\n",
    "    standardised = df.fillna(\"\")\n",
    "    MODEL_ID_TO_DISPLAY = {\n",
    "        row.model_id: row.model_display_name or _guess_display_name(row.model_id)\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_ID_TO_SHORT = {\n",
    "        row.model_id: (row.model_short_name or MODEL_ID_TO_DISPLAY[row.model_id])\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "    MODEL_DISPLAY_TO_SHORT = {\n",
    "        MODEL_ID_TO_DISPLAY[row.model_id]: MODEL_ID_TO_SHORT[row.model_id]\n",
    "        for row in standardised.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "\n",
    "def _guess_display_name(model_id: str) -> str:\n",
    "    if model_id in LOOKUP_SPECIAL_CASES:\n",
    "        return LOOKUP_SPECIAL_CASES[model_id]\n",
    "    parts = [part for part in model_id.replace(\"-\", \" \").replace(\"_\", \" \").split(\" \") if part]\n",
    "    if not parts:\n",
    "        return model_id\n",
    "    formatted = []\n",
    "    for token in parts:\n",
    "        if len(token) <= 3:\n",
    "            formatted.append(token.upper())\n",
    "        else:\n",
    "            formatted.append(token.capitalize())\n",
    "    return \" \".join(formatted)\n",
    "\n",
    "\n",
    "def _matches_any(value: str, patterns: Iterable[str]) -> bool:\n",
    "    return any(fnmatch(value, pattern) for pattern in patterns) if patterns else False\n",
    "\n",
    "\n",
    "def _first_existing(path: Path, candidates: Sequence[str]) -> Optional[Path]:\n",
    "    for name in candidates:\n",
    "        candidate = path / name\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def discover_model_runs(\n",
    "    results_root: Path,\n",
    "    include_globs: Iterable[str],\n",
    "    exclude_patterns: Iterable[str],\n",
    ") -> list[RunRecord]:\n",
    "    results_root = Path(results_root)\n",
    "    if not results_root.exists():\n",
    "        raise FileNotFoundError(f\"Results root missing: {results_root}\")\n",
    "    records: list[RunRecord] = []\n",
    "    include = tuple(include_globs) if include_globs else (\"*\",)\n",
    "    exclude = tuple(exclude_patterns) if exclude_patterns else tuple()\n",
    "    for models_dir in sorted(results_root.rglob(\"models\")):\n",
    "        if not models_dir.is_dir():\n",
    "            continue\n",
    "        run_dir = models_dir.parent\n",
    "        run_name = run_dir.name\n",
    "        if not _matches_any(run_name, include):\n",
    "            continue\n",
    "        if exclude and _matches_any(run_name, exclude):\n",
    "            continue\n",
    "        for model_dir in sorted(models_dir.iterdir()):\n",
    "            if not model_dir.is_dir():\n",
    "                continue\n",
    "            model_id = model_dir.name\n",
    "            metrics_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"metrics_per_gene.csv\",\n",
    "                    \"metrics_by_gene.csv\",\n",
    "                    \"metrics_cv.csv\",\n",
    "                ),\n",
    "            )\n",
    "            predictions_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"predictions_raw.csv\",\n",
    "                    \"predictions.csv\",\n",
    "                ),\n",
    "            )\n",
    "            history_path = _first_existing(\n",
    "                model_dir,\n",
    "                (\n",
    "                    \"training_history.csv\",\n",
    "                    \"training_history_loss.csv\",\n",
    "                ),\n",
    "            )\n",
    "            records.append(\n",
    "                RunRecord(\n",
    "                    run_name=run_name,\n",
    "                    model_id=model_id,\n",
    "                    run_path=run_dir,\n",
    "                    model_path=model_dir,\n",
    "                    metrics_path=metrics_path,\n",
    "                    predictions_path=predictions_path,\n",
    "                    training_history_path=history_path,\n",
    "                )\n",
    "            )\n",
    "    return records\n",
    "\n",
    "\n",
    "def ensure_model_lookup(path: Path, model_ids: Iterable[str]) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    df = _read_lookup_table(path)\n",
    "    existing = set(df[\"model_id\"]) if not df.empty else set()\n",
    "    new_rows = []\n",
    "    for model_id in sorted(set(model_ids).difference(existing)):\n",
    "        display_name = _guess_display_name(model_id)\n",
    "        short_name = SHORT_NAME_FALLBACKS.get(display_name, _default_short_name(display_name))\n",
    "        new_rows.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"model_display_name\": display_name,\n",
    "                \"model_short_name\": short_name,\n",
    "            }\n",
    "        )\n",
    "    if new_rows:\n",
    "        additions = pd.DataFrame(new_rows)\n",
    "        df = pd.concat([df, additions], ignore_index=True) if not df.empty else additions\n",
    "        df.sort_values(\"model_id\", inplace=True)\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def attach_lookup(records: Sequence[RunRecord], model_lookup: pd.DataFrame) -> list[RunRecord]:\n",
    "    if model_lookup.empty:\n",
    "        return list(records)\n",
    "    display_map = dict(zip(model_lookup[\"model_id\"], model_lookup[\"model_display_name\"]))\n",
    "    resolved: list[RunRecord] = []\n",
    "    for record in records:\n",
    "        display = display_map.get(record.model_id, _guess_display_name(record.model_id))\n",
    "        resolved.append(replace(record, model_display=display))\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def run_records_to_frame(records: Sequence[RunRecord]) -> pd.DataFrame:\n",
    "    if not records:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"run_name\",\n",
    "                \"model_id\",\n",
    "                \"model_display\",\n",
    "                \"run_path\",\n",
    "                \"model_path\",\n",
    "                \"metrics_path\",\n",
    "                \"predictions_path\",\n",
    "                \"training_history_path\",\n",
    "            ]\n",
    "        )\n",
    "    data = [\n",
    "        {\n",
    "            \"run_name\": r.run_name,\n",
    "            \"model_id\": r.model_id,\n",
    "            \"model_display\": r.model_display or _guess_display_name(r.model_id),\n",
    "            \"run_path\": r.run_path,\n",
    "            \"model_path\": r.model_path,\n",
    "            \"metrics_path\": r.metrics_path,\n",
    "            \"predictions_path\": r.predictions_path,\n",
    "            \"training_history_path\": r.training_history_path,\n",
    "        }\n",
    "        for r in records\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def to_relative_path(path_like: Optional[Path], root: Path) -> Optional[str]:\n",
    "    if path_like is None:\n",
    "        return None\n",
    "    path = Path(path_like)\n",
    "    try:\n",
    "        return str(path.resolve().relative_to(root))\n",
    "    except Exception:\n",
    "        return str(path.resolve())\n",
    "\n",
    "\n",
    "def maybe_store_table(store: dict[str, pd.DataFrame], key: str, table: pd.DataFrame) -> None:\n",
    "    if table is None or table.empty:\n",
    "        return\n",
    "    store[key] = table\n",
    "\n",
    "\n",
    "def _format_title_with_dataset(title: str | None, suffix: str | None) -> str | None:\n",
    "    if not title or not suffix:\n",
    "        return title\n",
    "    if suffix in title:\n",
    "        return title\n",
    "    return f\"{title} | {suffix}\"\n",
    "\n",
    "\n",
    "def register_figure(store: dict[str, object], key: str, fig: Optional[plt.Figure]) -> None:\n",
    "    \"\"\"Track generated matplotlib figures for later export.\"\"\"\n",
    "    if fig is None:\n",
    "        store.pop(key, None)\n",
    "        return\n",
    "    if dataset_title_suffix:\n",
    "        for ax in fig.axes:\n",
    "            current = ax.get_title()\n",
    "            updated = _format_title_with_dataset(current, dataset_title_suffix)\n",
    "            if updated and updated != current:\n",
    "                ax.set_title(updated)\n",
    "        if fig._suptitle is not None:\n",
    "            current = fig._suptitle.get_text()\n",
    "            updated = _format_title_with_dataset(current, dataset_title_suffix)\n",
    "            if updated and updated != current:\n",
    "                fig._suptitle.set_text(updated)\n",
    "    store[key] = fig\n",
    "\n",
    "\n",
    "def load_metrics(records: Sequence[RunRecord]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    long_frames: list[pd.DataFrame] = []\n",
    "    for record in records:\n",
    "        metrics_path = record.metrics_path\n",
    "        if metrics_path is None or not metrics_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to load metrics from {metrics_path}: {exc}\")\n",
    "            continue\n",
    "        required_cols = {\"gene\", \"split\", \"pearson\"}\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            continue\n",
    "        df = df.copy()\n",
    "        df[\"run_name\"] = record.run_name\n",
    "        df[\"model_id\"] = record.model_id\n",
    "        df[\"model_display\"] = record.model_display or _guess_display_name(record.model_id)\n",
    "        long_frames.append(df)\n",
    "    if not long_frames:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    metrics_long = pd.concat(long_frames, ignore_index=True)\n",
    "    base_cols = [col for col in [\"run_name\", \"model_id\", \"model_display\", \"gene\", \"split\"] if col in metrics_long.columns]\n",
    "    metric_cols = [col for col in metrics_long.columns if col not in base_cols]\n",
    "    metrics_long = metrics_long[base_cols + metric_cols]\n",
    "\n",
    "    wide = metrics_long.pivot_table(\n",
    "        index=[\"run_name\", \"model_id\", \"model_display\", \"gene\"],\n",
    "        columns=\"split\",\n",
    "        values=\"pearson\",\n",
    "    )\n",
    "    wide.columns = [f\"{str(col).lower()}_pearson\" for col in wide.columns]\n",
    "    metrics_wide = wide.reset_index()\n",
    "\n",
    "    return metrics_long, metrics_wide\n",
    "\n",
    "\n",
    "def compute_model_summary(\n",
    "    metrics_wide: pd.DataFrame,\n",
    "    splits: Sequence[str],\n",
    ") -> pd.DataFrame:\n",
    "    if metrics_wide.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"model_display\",\n",
    "                \"model_id\",\n",
    "                \"run_name\",\n",
    "                *[f\"{split}_pearson_mean\" for split in splits],\n",
    "                *[f\"{split}_pearson_std\" for split in splits],\n",
    "            ]\n",
    "        )\n",
    "    summaries = []\n",
    "    lower_splits = [split.lower() for split in splits]\n",
    "    for (run_name, model_id, model_display), group in metrics_wide.groupby([\"run_name\", \"model_id\", \"model_display\"], dropna=False):\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"model_display\": model_display,\n",
    "        }\n",
    "        for split, lower in zip(splits, lower_splits):\n",
    "            column = f\"{lower}_pearson\"\n",
    "            if column in group:\n",
    "                values = group[column].dropna()\n",
    "                if not values.empty:\n",
    "                    row[f\"{split}_pearson_mean\"] = values.mean()\n",
    "                    row[f\"{split}_pearson_std\"] = values.std(ddof=1) if len(values) > 1 else float(\"nan\")\n",
    "        summaries.append(row)\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    if \"test_pearson_mean\" in summary_df:\n",
    "        summary_df.sort_values(\"test_pearson_mean\", ascending=False, inplace=True)\n",
    "    summary_df.set_index([\"model_display\", \"model_id\", \"run_name\"], inplace=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42bddd",
   "metadata": {},
   "source": [
    "## 4. Discover and Inspect Run Metadata\n",
    "\n",
    "Enumerate available runs and assemble the run metadata table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "raw_run_records = discover_model_runs(\n",
    "    config.results_root,\n",
    "    config.run_include_globs,\n",
    "    config.run_exclude\n",
    ")\n",
    "model_lookup = ensure_model_lookup(\n",
    "    config.lookup_path,\n",
    "    [record.model_id for record in raw_run_records]\n",
    ")\n",
    "_update_model_name_maps(model_lookup)\n",
    "run_records = attach_lookup(raw_run_records, model_lookup)\n",
    "\n",
    "\n",
    "def _dataset_from_run_name(name: str) -> str | None:\n",
    "    name = str(name).lower()\n",
    "    if \"embryonic\" in name:\n",
    "        return \"embryonic\"\n",
    "    if \"endothelial\" in name:\n",
    "        return \"endothelial\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def _gene_count_from_run_name(name: str) -> int | None:\n",
    "    match = re.search(r\"(\\d+)genes\", str(name).lower())\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return int(match.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _run_timestamp_key(name: str) -> tuple[int, int]:\n",
    "    match = re.search(r\"_(\\d{8})_(\\d{6})\", str(name))\n",
    "    if not match:\n",
    "        return (0, 0)\n",
    "    return (int(match.group(1)), int(match.group(2)))\n",
    "\n",
    "\n",
    "def _has_metrics(rec: RunRecord) -> bool:\n",
    "    path = rec.metrics_path\n",
    "    if not path:\n",
    "        return False\n",
    "    path = path if isinstance(path, Path) else Path(path)\n",
    "    return path.exists()\n",
    "\n",
    "def _model_dir_has_files(rec: RunRecord) -> bool:\n",
    "    path = rec.model_path\n",
    "    if not path:\n",
    "        return False\n",
    "    path = path if isinstance(path, Path) else Path(path)\n",
    "    try:\n",
    "        return path.exists() and any(path.iterdir())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _is_nonempty_record(rec: RunRecord) -> bool:\n",
    "    return _has_metrics(rec) or _model_dir_has_files(rec)\n",
    "\n",
    "\n",
    "\n",
    "def _select_latest_runs(records: list[RunRecord]) -> list[RunRecord]:\n",
    "    if not records:\n",
    "        return []\n",
    "    grouped: dict[tuple[str, int, str], list[RunRecord]] = {}\n",
    "    passthrough: list[RunRecord] = []\n",
    "    for rec in records:\n",
    "        dataset = _dataset_from_run_name(rec.run_name)\n",
    "        gene_count = _gene_count_from_run_name(rec.run_name)\n",
    "        if not dataset or gene_count is None:\n",
    "            passthrough.append(rec)\n",
    "            continue\n",
    "        key = (dataset, gene_count, rec.model_id)\n",
    "        grouped.setdefault(key, []).append(rec)\n",
    "\n",
    "    selected: list[RunRecord] = []\n",
    "    for group in grouped.values():\n",
    "        with_metrics = [rec for rec in group if _has_metrics(rec)]\n",
    "        if not with_metrics:\n",
    "            continue\n",
    "        with_metrics.sort(key=lambda rec: _run_timestamp_key(rec.run_name), reverse=True)\n",
    "        selected.append(with_metrics[0])\n",
    "\n",
    "    return selected + passthrough\n",
    "\n",
    "\n",
    "run_records = _select_latest_runs(list(run_records))\n",
    "\n",
    "# Fallback: if a 1000-gene run is missing metrics, try archive 1000-gene runs first,\n",
    "# then fall back to 100-gene runs when needed.\n",
    "if USE_1000PLUS100:\n",
    "    archive_root = config.results_root / \"archive\"\n",
    "    if archive_root.exists():\n",
    "        successful_1000: set[tuple[str, str]] = set()\n",
    "        for record in raw_run_records:\n",
    "            if \"1000genes\" not in record.run_name:\n",
    "                continue\n",
    "            metrics_path = record.metrics_path\n",
    "            if metrics_path is None or not Path(metrics_path).exists():\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(record.run_name) or \"\"\n",
    "            if ds:\n",
    "                successful_1000.add((ds, record.model_id))\n",
    "\n",
    "        for run_dir in archive_root.iterdir():\n",
    "            if not run_dir.is_dir() or \"1000genes\" not in run_dir.name:\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(run_dir.name) or \"\"\n",
    "            if not ds:\n",
    "                continue\n",
    "            models_dir = run_dir / \"models\"\n",
    "            if not models_dir.exists():\n",
    "                continue\n",
    "            for model_dir in models_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                metrics_path = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if metrics_path and metrics_path.exists():\n",
    "                    successful_1000.add((ds, model_dir.name))\n",
    "\n",
    "        updated_records: list[RunRecord | None] = list(run_records)\n",
    "        for idx, rec in enumerate(list(run_records)):\n",
    "            metrics_path = rec.metrics_path\n",
    "            metrics_missing = metrics_path is None or not Path(metrics_path).exists()\n",
    "            if not metrics_missing:\n",
    "                continue\n",
    "            if \"1000genes\" not in rec.run_name:\n",
    "                continue\n",
    "\n",
    "            dataset_label = _dataset_from_run_name(rec.run_name) or \"\"\n",
    "            alt_model_dir = None\n",
    "            alt_metrics = None\n",
    "            alt_run_dir = None\n",
    "\n",
    "            candidates = [d for d in archive_root.iterdir() if d.is_dir() and \"1000genes\" in d.name]\n",
    "            if dataset_label:\n",
    "                candidates = [d for d in candidates if dataset_label.lower() in d.name.lower()]\n",
    "            for cand in sorted(candidates, key=lambda d: _run_timestamp_key(d.name), reverse=True):\n",
    "                model_dir = cand / \"models\" / rec.model_id\n",
    "                if not model_dir.exists():\n",
    "                    continue\n",
    "                alt_metrics = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if alt_metrics and alt_metrics.exists():\n",
    "                    alt_run_dir = cand\n",
    "                    alt_model_dir = model_dir\n",
    "                    break\n",
    "\n",
    "            if alt_run_dir is None and (dataset_label, rec.model_id) not in successful_1000:\n",
    "                candidates = [d for d in archive_root.iterdir() if d.is_dir() and \"100genes\" in d.name]\n",
    "                if dataset_label:\n",
    "                    candidates = [d for d in candidates if dataset_label.lower() in d.name.lower()]\n",
    "                for cand in sorted(candidates, key=lambda d: _run_timestamp_key(d.name), reverse=True):\n",
    "                    model_dir = cand / \"models\" / rec.model_id\n",
    "                    if not model_dir.exists():\n",
    "                        continue\n",
    "                    alt_metrics = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                    if alt_metrics and alt_metrics.exists():\n",
    "                        alt_run_dir = cand\n",
    "                        alt_model_dir = model_dir\n",
    "                        break\n",
    "\n",
    "            if alt_metrics and alt_metrics.exists() and alt_model_dir and alt_model_dir.exists():\n",
    "                alt_preds = _first_existing(\n",
    "                    alt_model_dir,\n",
    "                    (\n",
    "                        \"predictions_raw.csv\",\n",
    "                        \"predictions.csv\",\n",
    "                    ),\n",
    "                )\n",
    "                alt_history = _first_existing(\n",
    "                    alt_model_dir,\n",
    "                    (\n",
    "                        \"training_history.csv\",\n",
    "                        \"training_history_loss.csv\",\n",
    "                    ),\n",
    "                )\n",
    "                new_rec = RunRecord(\n",
    "                    run_name=alt_run_dir.name,\n",
    "                    model_id=rec.model_id,\n",
    "                    run_path=alt_run_dir,\n",
    "                    model_path=alt_model_dir,\n",
    "                    metrics_path=alt_metrics,\n",
    "                    predictions_path=alt_preds,\n",
    "                    training_history_path=alt_history,\n",
    "                    model_display=rec.model_display,\n",
    "                )\n",
    "                updated_records[idx] = new_rec\n",
    "\n",
    "        run_records = [r for r in updated_records if r is not None]\n",
    "\n",
    "# Add archive 100-gene runs when 1000-gene models are missing (per dataset)\n",
    "if USE_1000PLUS100:\n",
    "    archive_root = config.results_root / \"archive\"\n",
    "    if archive_root.exists():\n",
    "        successful_1000: set[tuple[str, str]] = set()\n",
    "        for record in raw_run_records:\n",
    "            if \"1000genes\" not in record.run_name:\n",
    "                continue\n",
    "            metrics_path = record.metrics_path\n",
    "            if metrics_path is None or not Path(metrics_path).exists():\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(record.run_name) or \"\"\n",
    "            if ds:\n",
    "                successful_1000.add((ds, record.model_id))\n",
    "\n",
    "        for run_dir in archive_root.iterdir():\n",
    "            if not run_dir.is_dir() or \"1000genes\" not in run_dir.name:\n",
    "                continue\n",
    "            ds = _dataset_from_run_name(run_dir.name) or \"\"\n",
    "            if not ds:\n",
    "                continue\n",
    "            models_dir = run_dir / \"models\"\n",
    "            if not models_dir.exists():\n",
    "                continue\n",
    "            for model_dir in models_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                metrics_path = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if metrics_path and metrics_path.exists():\n",
    "                    successful_1000.add((ds, model_dir.name))\n",
    "\n",
    "        dataset_models_1000 = {}\n",
    "        for rec in run_records:\n",
    "            if \"1000genes\" not in rec.run_name:\n",
    "                continue\n",
    "            ds = \"embryonic\" if \"embryonic\" in rec.run_name.lower() else \"endothelial\" if \"endothelial\" in rec.run_name.lower() else \"\"\n",
    "            dataset_models_1000.setdefault(ds, set()).add(rec.model_id)\n",
    "\n",
    "        archive_candidates = {}\n",
    "        for run_dir in archive_root.iterdir():\n",
    "            if not run_dir.is_dir() or \"100genes\" not in run_dir.name:\n",
    "                continue\n",
    "            ds = \"embryonic\" if \"embryonic\" in run_dir.name.lower() else \"endothelial\" if \"endothelial\" in run_dir.name.lower() else \"\"\n",
    "            if not ds:\n",
    "                continue\n",
    "            models_dir = run_dir / \"models\"\n",
    "            if not models_dir.exists():\n",
    "                continue\n",
    "            for model_dir in models_dir.iterdir():\n",
    "                if not model_dir.is_dir():\n",
    "                    continue\n",
    "                metrics_path = _first_existing(model_dir, (\"metrics_per_gene.csv\", \"metrics_by_gene.csv\", \"metrics_cv.csv\"))\n",
    "                if not metrics_path or not metrics_path.exists():\n",
    "                    continue\n",
    "                key = (ds, model_dir.name)\n",
    "                archive_candidates.setdefault(key, []).append((run_dir, model_dir, metrics_path))\n",
    "\n",
    "        def _timestamp_key(name: str) -> tuple[int, int]:\n",
    "            import re\n",
    "            match = re.search(r\"_(\\d{8})_(\\d{6})\", name)\n",
    "            if not match:\n",
    "                return (0, 0)\n",
    "            return (int(match.group(1)), int(match.group(2)))\n",
    "\n",
    "        added_records = []\n",
    "        for (ds, model_id), candidates in archive_candidates.items():\n",
    "            if model_id in dataset_models_1000.get(ds, set()):\n",
    "                continue\n",
    "            candidates.sort(key=lambda item: _timestamp_key(item[0].name), reverse=True)\n",
    "            run_dir, model_dir, metrics_path = candidates[0]\n",
    "            preds_path = _first_existing(model_dir, (\"predictions_raw.csv\", \"predictions.csv\"))\n",
    "            history_path = _first_existing(model_dir, (\"training_history.csv\", \"training_history_loss.csv\"))\n",
    "            new_rec = RunRecord(\n",
    "                run_name=run_dir.name,\n",
    "                model_id=model_id,\n",
    "                run_path=run_dir,\n",
    "                model_path=model_dir,\n",
    "                metrics_path=metrics_path,\n",
    "                predictions_path=preds_path,\n",
    "                training_history_path=history_path,\n",
    "                model_display=MODEL_ID_TO_DISPLAY.get(model_id, _guess_display_name(model_id)),\n",
    "            )\n",
    "            added_records.append(new_rec)\n",
    "        if added_records:\n",
    "            run_records = list(run_records) + added_records\n",
    "\n",
    "\n",
    "def _select_preferred_runs(records: list[RunRecord], prefer_1000: bool) -> list[RunRecord]:\n",
    "    # Pick the most recent 1000-gene run per model/dataset, falling back to 100-gene runs when missing.\n",
    "    if not records:\n",
    "        return []\n",
    "    if not prefer_1000:\n",
    "        return list(records)\n",
    "\n",
    "    selected: list[RunRecord] = []\n",
    "    by_model_dataset: dict[tuple[str, str], list[RunRecord]] = {}\n",
    "    for record in records:\n",
    "        dataset_label = _dataset_from_run_name(record.run_name) or \"\"\n",
    "        key = (record.model_id, dataset_label)\n",
    "        by_model_dataset.setdefault(key, []).append(record)\n",
    "\n",
    "    for (model_id, dataset_label), group in by_model_dataset.items():\n",
    "        group_1000 = [rec for rec in group if \"1000genes\" in rec.run_name and _has_metrics(rec)]\n",
    "        group_100 = [rec for rec in group if \"100genes\" in rec.run_name and _has_metrics(rec)]\n",
    "        group_1000_all = [rec for rec in group if \"1000genes\" in rec.run_name]\n",
    "        group_100_all = [rec for rec in group if \"100genes\" in rec.run_name]\n",
    "        candidates = group_1000 or group_100 or group_1000_all or group_100_all or group\n",
    "        candidates.sort(key=lambda rec: _run_timestamp_key(rec.run_name), reverse=True)\n",
    "        selected.append(candidates[0])\n",
    "\n",
    "    return selected\n",
    "\n",
    "run_records = _select_preferred_runs(list(run_records), PREFER_1000_FALLBACK_100)\n",
    "run_df = run_records_to_frame(run_records)\n",
    "run_df.sort_values([\"run_name\", \"model_id\"], inplace=True)\n",
    "run_df_display = run_df.copy()\n",
    "for column in (\"run_path\", \"model_path\", \"metrics_path\", \"predictions_path\", \"training_history_path\"):\n",
    "    run_df_display[column] = run_df_display[column].map(lambda value: to_relative_path(value, config.project_root))\n",
    "analysis_metadata.update({\n",
    "    \"results_root\": to_relative_path(config.results_root, config.project_root),\n",
    "    \"fig_dir\": to_relative_path(config.fig_dir, config.project_root),\n",
    "    \"reports_dir\": to_relative_path(config.reports_dir, config.project_root),\n",
    "    \"run_count\": run_df[\"run_name\"].nunique(),\n",
    "    \"model_count\": len(run_df),\n",
    "    \"model_lookup_path\": to_relative_path(config.lookup_path, config.project_root),\n",
    "})\n",
    "analysis_metadata.setdefault(\"include_globs\", config.run_include_globs)\n",
    "display(Markdown(f\"**Scanning results root:** `{analysis_metadata['results_root']}`\"))\n",
    "subset_descriptor = analysis_metadata.get(\"subset_descriptor\")\n",
    "if subset_descriptor:\n",
    "    display(Markdown(f\"**Subset criteria:** {subset_descriptor}\"))\n",
    "include_filters = analysis_metadata.get(\"include_globs\")\n",
    "if include_filters:\n",
    "    include_text = ', '.join(str(item) for item in include_filters)\n",
    "    display(Markdown(f\"**Include filters:** `{include_text}`\"))\n",
    "display(Markdown(\n",
    "    f\"**Figure output:** `{analysis_metadata['fig_dir']}` | **Reports:** `{analysis_metadata['reports_dir']}`\"\n",
    "))\n",
    "display(run_df_display)\n",
    "print(\n",
    "    \"Discovered\",\n",
    "    analysis_metadata[\"model_count\"],\n",
    "    \"model folders across\",\n",
    "    analysis_metadata[\"run_count\"],\n",
    "    \"runs.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ed082",
   "metadata": {},
   "source": [
    "## 5. Load Metrics and Compute Summaries\n",
    "\n",
    "Load per-gene metrics and compute split-level summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12236958",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long, metrics_wide = load_metrics(run_records)\n",
    "\n",
    "# Table: mean test Pearson for all runs with metrics\n",
    "all_records = attach_lookup(raw_run_records, model_lookup)\n",
    "all_metrics_long, all_metrics_wide = load_metrics(all_records)\n",
    "if all_metrics_wide.empty or \"test_pearson\" not in all_metrics_wide:\n",
    "    print(\"No test-set metrics available for full run table.\")\n",
    "else:\n",
    "    mean_df = (\n",
    "        all_metrics_wide.groupby([\"run_name\", \"model_id\"])\n",
    "        [\"test_pearson\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"test_pearson\": \"test_pearson_mean\"})\n",
    "    )\n",
    "    path_df = run_records_to_frame(all_records)[[\"run_name\", \"model_id\", \"model_path\"]]\n",
    "    all_run_mean_test_pearson = mean_df.merge(path_df, on=[\"run_name\", \"model_id\"], how=\"left\")\n",
    "    all_run_mean_test_pearson[\"dataset\"] = all_run_mean_test_pearson[\"run_name\"].map(_dataset_from_run_name)\n",
    "    all_run_mean_test_pearson[\"model_path\"] = all_run_mean_test_pearson[\"model_path\"].map(\n",
    "        lambda value: to_relative_path(value, config.project_root)\n",
    "    )\n",
    "    display(all_run_mean_test_pearson.sort_values([\"run_name\", \"model_id\"]))\n",
    "    TABLES[\"all_run_mean_test_pearson\"] = all_run_mean_test_pearson\n",
    "if metrics_long.empty:\n",
    "    raise RuntimeError(\"No metrics available for plotting.\")\n",
    "\n",
    "split_filter = {config.primary_split, config.val_split, config.train_split}\n",
    "available_splits = sorted(metrics_long[\"split\"].unique())\n",
    "missing_splits = split_filter.difference(available_splits)\n",
    "if missing_splits:\n",
    "    print(\"Warning: the following splits are missing from metrics files:\", sorted(missing_splits))\n",
    "\n",
    "test_metrics = metrics_long[metrics_long[\"split\"] == config.primary_split].copy()\n",
    "val_metrics = metrics_long[metrics_long[\"split\"] == config.val_split].copy()\n",
    "train_metrics = metrics_long[metrics_long[\"split\"] == config.train_split].copy()\n",
    "summary_df = compute_model_summary(\n",
    "    metrics_wide, [config.primary_split, config.val_split, config.train_split]\n",
    ")\n",
    "if summary_df.empty:\n",
    "    raise RuntimeError(\"Unable to compute summary statistics from metrics.\")\n",
    "\n",
    "summary_reset = summary_df.reset_index()\n",
    "analysis_metadata[\"best_model_id\"] = summary_reset.iloc[0][\"model_id\"]\n",
    "analysis_metadata[\"best_model_display\"] = summary_reset.iloc[0][\"model_display\"]\n",
    "analysis_metadata[\"best_run_name\"] = summary_reset.iloc[0][\"run_name\"]\n",
    "\n",
    "if \"test_pearson_mean\" in summary_reset:\n",
    "    model_display_order = summary_reset.sort_values(\n",
    "        by=\"test_pearson_mean\", ascending=False\n",
    ")[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "else:\n",
    "    model_display_order = summary_reset[\"model_display\"].tolist()\n",
    "\n",
    "display(summary_reset)\n",
    "\n",
    "analysis_state = {\n",
    "    \"run_df\": run_df,\n",
    "    \"metrics_long\": metrics_long,\n",
    "    \"metrics_wide\": metrics_wide,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"summary_df\": summary_df,\n",
    "    \"summary_reset\": summary_reset,\n",
    "    \"model_display_order\": model_display_order,\n",
    "    \"model_short_name_map\": MODEL_ID_TO_SHORT.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee21a65",
   "metadata": {},
   "source": [
    "## 6. Supporting Tables\n",
    "\n",
    "Write helper tables that back figures and downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a415ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = analysis_state[\"summary_df\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "metrics_wide = analysis_state[\"metrics_wide\"]\n",
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "val_metrics = analysis_state[\"val_metrics\"]\n",
    "train_metrics = analysis_state[\"train_metrics\"]\n",
    "\n",
    "val_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.val_split}_pearson\" in metrics_wide:\n",
    "    val_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.val_split}_pearson\",\n",
    "    )\n",
    "\n",
    "test_pearson_per_gene = pd.DataFrame()\n",
    "if f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "    )\n",
    "\n",
    "maybe_store_table(\n",
    "    TABLES,\n",
    "    \"metrics_per_gene_master\",\n",
    "    metrics_long.sort_values([\"split\", \"run_name\", \"model_id\", \"gene\"])\n",
    ")\n",
    "maybe_store_table(TABLES, \"summary_metrics_all_models\", summary_reset)\n",
    "\n",
    "analysis_state.update(\n",
    "    {\n",
    "        \"val_pearson_per_gene\": val_pearson_per_gene,\n",
    "        \"test_pearson_per_gene\": test_pearson_per_gene,\n",
    "        \"split_mean_summary\": summary_reset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f612b",
   "metadata": {},
   "source": [
    "## 7. Test Pearson Heatmap\n",
    "\n",
    "Model-level test Pearson comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0421f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_order = analysis_state[\"model_display_order\"]\n",
    "if \"test_pearson_mean\" not in summary_reset:\n",
    "    print(\"Test Pearson summary unavailable; skipping heatmap.\")\n",
    "    fig_test_heatmap = None\n",
    "else:\n",
    "    ranked = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "    heatmap_series = best_per_model.set_index(\"model_display\")[\"test_pearson_mean\"]\n",
    "    heatmap_df = heatmap_series.reindex(model_order).dropna().to_frame(name=\"Mean Test Pearson\")\n",
    "    if heatmap_df.empty:\n",
    "        print(\"No aggregated test Pearson values available; skipping heatmap.\")\n",
    "        fig_test_heatmap = None\n",
    "    else:\n",
    "        vmin, vmax = compute_heatmap_limits(heatmap_df.values)\n",
    "        fig_height = max(4, 0.4 * len(heatmap_df))\n",
    "        fig_test_heatmap, ax = plt.subplots(figsize=(4.5, fig_height))\n",
    "        sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            linewidths=0.4,\n",
    "            linecolor=\"#f2f2f2\",\n",
    "            cbar_kws={\"label\": \"Mean Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Mean Test Pearson by Model\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Model\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "        sns.despine(fig_test_heatmap, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_all_models\", fig_test_heatmap)\n",
    "if fig_test_heatmap is not None:\n",
    "    display(fig_test_heatmap)\n",
    "    plt.close(fig_test_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2ba0e",
   "metadata": {},
   "source": [
    "### Optional Validation Heatmap\n",
    "\n",
    "Runs only if validation Pearson summaries are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "test_pivot = analysis_state.get(\"test_pearson_per_gene\")\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "if test_pivot is None or test_pivot.empty:\n",
    "    print(\"Test Pearson data unavailable; skipping heatmap.\")\n",
    "    fig_test_heatmap_top = None\n",
    "else:\n",
    "    top_test_genes = (\n",
    "        test_metrics.groupby(\"gene\")[\"pearson\"].mean().sort_values(ascending=False).head(config.top_gene_count)\n",
    "    )\n",
    "    analysis_state[\"test_top_genes\"] = top_test_genes.index.tolist()\n",
    "    test_top_subset = test_pivot.loc[test_pivot.index.intersection(top_test_genes.index)]\n",
    "    if test_top_subset.empty:\n",
    "        print(\"Top-performing gene subset empty; skipping test heatmap.\")\n",
    "        fig_test_heatmap_top = None\n",
    "    else:\n",
    "        model_order = summary_reset[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "        ordered_columns = [col for col in model_order if col in test_top_subset.columns]\n",
    "        test_top_subset = test_top_subset.reindex(columns=ordered_columns)\n",
    "        vmin, vmax = compute_heatmap_limits(\n",
    "            test_top_subset.values, lower_percentile=10.0, upper_percentile=95.0\n",
    ")\n",
    "        fig_height_top = max(4, 0.6 * len(test_top_subset.index))\n",
    "        fig_test_heatmap_top, ax = plt.subplots(figsize=(10, fig_height_top))\n",
    "        sns.heatmap(\n",
    "            test_top_subset,\n",
    "            cmap=\"crest\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.3,\n",
    "            linecolor=\"#f5f5f5\",\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Test Pearson (top genes by mean across models)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Gene\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=10)\n",
    "        sns.despine(fig_test_heatmap_top, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_top\", fig_test_heatmap_top)\n",
    "if fig_test_heatmap_top is not None:\n",
    "    display(fig_test_heatmap_top)\n",
    "    plt.close(fig_test_heatmap_top)\n",
    "\n",
    "# --- Top genes per model heatmap (model-specific gene sets) ---\n",
    "if test_pivot is None or test_pivot.empty:\n",
    "    fig_test_heatmap_top_per_model = None\n",
    "else:\n",
    "    model_order = summary_reset[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "    ordered_columns = [col for col in model_order if col in test_pivot.columns]\n",
    "    seen_genes: set[str] = set()\n",
    "    gene_order: list[str] = []\n",
    "    for model_name in ordered_columns:\n",
    "        series = test_pivot[model_name].dropna().sort_values(ascending=False).head(config.top_gene_count)\n",
    "        for gene in series.index:\n",
    "            if gene not in seen_genes:\n",
    "                seen_genes.add(gene)\n",
    "                gene_order.append(gene)\n",
    "    if not gene_order:\n",
    "        print(\"No per-model top genes available; skipping heatmap.\")\n",
    "        fig_test_heatmap_top_per_model = None\n",
    "    else:\n",
    "        per_model_subset = test_pivot.reindex(index=gene_order, columns=ordered_columns)\n",
    "        vmin, vmax = compute_heatmap_limits(\n",
    "            per_model_subset.values, lower_percentile=10.0, upper_percentile=95.0\n",
    "        )\n",
    "        fig_height = max(4, 0.4 * len(per_model_subset.index))\n",
    "        fig_test_heatmap_top_per_model, ax = plt.subplots(figsize=(10, fig_height))\n",
    "        sns.heatmap(\n",
    "            per_model_subset,\n",
    "            cmap=\"crest\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            linewidths=0.3,\n",
    "            linecolor=\"#f5f5f5\",\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Test Pearson (top genes per model)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Gene\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "        sns.despine(fig_test_heatmap_top_per_model, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_top_per_model\", fig_test_heatmap_top_per_model)\n",
    "if fig_test_heatmap_top_per_model is not None:\n",
    "    display(fig_test_heatmap_top_per_model)\n",
    "    plt.close(fig_test_heatmap_top_per_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Per-model top-20 heatmap blocks (horizontal layout) ---\n",
    "if test_pivot is None or test_pivot.empty:\n",
    "    fig_test_heatmap_top20_blocks = None\n",
    "else:\n",
    "    model_order = summary_reset[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "    ordered_columns = [col for col in model_order if col in test_pivot.columns]\n",
    "    values = {}\n",
    "    annot = {}\n",
    "    for model_name in ordered_columns:\n",
    "        series = test_pivot[model_name].dropna().sort_values(ascending=False).head(20)\n",
    "        # Pad to 20 rows to keep heatmap rectangular\n",
    "        padded_values = series.tolist() + [float(\"nan\")] * (20 - len(series))\n",
    "        padded_genes = series.index.tolist() + [\"\"] * (20 - len(series))\n",
    "        values[model_name] = padded_values\n",
    "        annot[model_name] = [f\"{g}\\n{v:.2f}\" if g else \"\" for g, v in zip(padded_genes, padded_values)]\n",
    "\n",
    "    if not values:\n",
    "        print(\"No per-model top-20 genes available; skipping block heatmap.\")\n",
    "        fig_test_heatmap_top20_blocks = None\n",
    "    else:\n",
    "        heatmap_df = pd.DataFrame(values, index=[f\"rank {i}\" for i in range(1, 21)])\n",
    "        annot_df = pd.DataFrame(annot, index=heatmap_df.index)\n",
    "        fig_width = max(8, 1.2 * len(heatmap_df.columns))\n",
    "        fig_height = 10\n",
    "        fig_test_heatmap_top20_blocks, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(\n",
    "            heatmap_df,\n",
    "            cmap=\"crest\",\n",
    "            annot=annot_df,\n",
    "            fmt=\"\",\n",
    "            linewidths=0.3,\n",
    "            linecolor=\"#f5f5f5\",\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Test Pearson (top 20 genes per model)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Rank (per model)\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "        sns.despine(fig_test_heatmap_top20_blocks, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_heatmap_top20_per_model_blocks\", fig_test_heatmap_top20_blocks)\n",
    "if fig_test_heatmap_top20_blocks is not None:\n",
    "    display(fig_test_heatmap_top20_blocks)\n",
    "    plt.close(fig_test_heatmap_top20_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91f757",
   "metadata": {},
   "source": [
    "## 8. Test Distribution Profiles\n",
    "\n",
    "Per-gene test-set Pearson distributions by model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11142198",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = analysis_state[\"test_metrics\"].copy()\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "if test_metrics.empty:\n",
    "    print(\"Test metrics unavailable; skipping violin plot.\")\n",
    "    fig_violin = None\n",
    "else:\n",
    "    mean_by_model = summary_reset.groupby(\"model_display\")[\"test_pearson_mean\"].mean()\n",
    "    mean_order_series = mean_by_model.sort_values(ascending=True)\n",
    "    model_order = [model for model in mean_order_series.index if model in model_display_order]\n",
    "    if not model_order:\n",
    "        model_order = model_display_order\n",
    "    violin_palette = sns.color_palette(\"Set2\", n_colors=len(model_order))\n",
    "    palette_map = dict(zip(model_order, violin_palette))\n",
    "    fig_width = max(12, 0.8 * max(6, len(model_order)))\n",
    "    fig_violin, ax = plt.subplots(figsize=(fig_width, 6))\n",
    "    sns.violinplot(\n",
    "        data=test_metrics,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"model_display\",\n",
    "        order=model_order,\n",
    "        hue_order=model_order,\n",
    "        palette=palette_map,\n",
    "        density_norm=\"width\",\n",
    "        inner=\"quartile\",\n",
    "        linewidth=1.0,\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    if len(test_metrics) > 0:\n",
    "        sample_size = min(len(test_metrics), 3000)\n",
    "        jitter_sample = (\n",
    "            test_metrics.sample(sample_size, random_state=config.random_seed)\n",
    "            if len(test_metrics) > sample_size\n",
    "            else test_metrics\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            data=jitter_sample,\n",
    "            x=\"model_display\",\n",
    "            y=\"pearson\",\n",
    "            order=model_order,\n",
    "            hue=\"model_display\",\n",
    "            hue_order=model_order,\n",
    "            palette=palette_map,\n",
    "            dodge=False,\n",
    "            alpha=0.45,\n",
    "            size=3.0,\n",
    "            jitter=0.12,\n",
    "            linewidth=0.3,\n",
    "            edgecolor=\"#2b2b2b\",\n",
    "            marker=\"o\",\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "        )\n",
    "    metric_min = test_metrics[\"pearson\"].min()\n",
    "    metric_max = test_metrics[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Test Pearson\")\n",
    "    ax.set_title(\"Per-gene Test Pearson Distribution by Model\")\n",
    "    ax.axhline(0.0, color=\"#777777\", linestyle=\"--\", linewidth=1)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    sns.despine(fig_violin, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "register_figure(FIGURES, \"test_pearson_violin\", fig_violin)\n",
    "if fig_violin is not None:\n",
    "    display(fig_violin)\n",
    "    plt.close(fig_violin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917d60d",
   "metadata": {},
   "source": [
    "### Split Comparison by Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73993561",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_long = analysis_state[\"metrics_long\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "model_display_order = analysis_state[\"model_display_order\"]\n",
    "splits_of_interest = [config.train_split, config.val_split, config.primary_split]\n",
    "subset = metrics_long[metrics_long[\"split\"].isin(splits_of_interest)].copy()\n",
    "if subset.empty or \"pearson\" not in subset:\n",
    "    print(\"Pearson metrics unavailable across requested splits; skipping split comparison plot.\")\n",
    "    fig_split_compare = None\n",
    "else:\n",
    "    subset = subset[[\"model_display\", \"split\", \"pearson\"]].dropna()\n",
    "    split_labels = {\n",
    "        config.train_split: \"Train\",\n",
    "        config.val_split: \"Val\",\n",
    "        config.primary_split: \"Test\",\n",
    "    }\n",
    "    subset[\"split_label\"] = subset[\"split\"].map(split_labels).fillna(subset[\"split\"].str.title())\n",
    "    test_means = summary_reset.groupby(\"model_display\")[\"test_pearson_mean\"].mean().sort_values(ascending=False)\n",
    "    ordered_models = [model for model in test_means.index if model in subset[\"model_display\"].unique()]\n",
    "    if not ordered_models:\n",
    "        ordered_models = model_display_order\n",
    "    split_order = [split_labels[split] for split in splits_of_interest if split in split_labels]\n",
    "    box_colors = sns.color_palette(\"Set2\", n_colors=len(split_order))\n",
    "    box_palette = dict(zip(split_order, box_colors))\n",
    "    fig_width = max(12, 0.75 * max(6, len(ordered_models)))\n",
    "    fig_split_compare, ax = plt.subplots(figsize=(fig_width, 6.5))\n",
    "    sns.boxplot(\n",
    "        data=subset,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=ordered_models,\n",
    "        hue_order=split_order,\n",
    "        palette=box_palette,\n",
    "        width=0.65,\n",
    "        fliersize=0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    jitter_sample = subset.sample(min(len(subset), 4000), random_state=config.random_seed) if len(subset) > 4000 else subset\n",
    "    sns.stripplot(\n",
    "        data=jitter_sample,\n",
    "        x=\"model_display\",\n",
    "        y=\"pearson\",\n",
    "        hue=\"split_label\",\n",
    "        order=ordered_models,\n",
    "        hue_order=split_order,\n",
    "        palette=box_palette,\n",
    "        dodge=True,\n",
    "        jitter=0.12,\n",
    "        size=3.0,\n",
    "        alpha=0.5,\n",
    "        edgecolor=\"#2b2b2b\",\n",
    "        linewidth=0.4,\n",
    "        marker=\"o\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    metric_min = subset[\"pearson\"].min()\n",
    "    metric_max = subset[\"pearson\"].max()\n",
    "    ymin = min(-0.5, metric_min - 0.05)\n",
    "    ymax = max(1.0, metric_max + 0.05)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Per-gene Pearson\")\n",
    "    ax.set_title(\"Per-gene Pearson by Split and Model\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.legend(title=\"Split\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    sns.despine(fig_split_compare, left=True, bottom=True)\n",
    "    fig_split_compare.tight_layout()\n",
    "register_figure(FIGURES, \"split_comparison_overview\", fig_split_compare)\n",
    "if fig_split_compare is not None:\n",
    "    display(fig_split_compare)\n",
    "    plt.close(fig_split_compare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8fa0a",
   "metadata": {},
   "source": [
    "## 9. Per-Gene Performance Analysis\n",
    "\n",
    "Detailed gene-level performance heatmaps and feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-gene heatmap: genes x models performance\n",
    "metrics_wide = analysis_state.get(\"metrics_wide\", pd.DataFrame())\n",
    "fig_gene_heatmap = None\n",
    "\n",
    "if not metrics_wide.empty and f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "\n",
    "    if not test_pearson_per_gene.empty and len(test_pearson_per_gene) > 3:\n",
    "        # Order genes by mean performance\n",
    "        gene_means = test_pearson_per_gene.mean(axis=1).sort_values(ascending=False)\n",
    "        test_pearson_per_gene = test_pearson_per_gene.loc[gene_means.index]\n",
    "\n",
    "        # Reorder columns by model performance\n",
    "        col_means = test_pearson_per_gene.mean(axis=0).sort_values(ascending=False)\n",
    "        test_pearson_per_gene = test_pearson_per_gene[col_means.index]\n",
    "\n",
    "        fig_height = max(8, 0.15 * len(test_pearson_per_gene))\n",
    "        fig_width = max(9, 0.4 * len(test_pearson_per_gene.columns))\n",
    "        fig_gene_heatmap, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "        vmin, vmax = compute_heatmap_limits(test_pearson_per_gene.values)\n",
    "        sns.heatmap(\n",
    "            test_pearson_per_gene,\n",
    "            cmap=\"RdYlGn\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=False,\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Per-Gene Test Pearson by Model (sorted by mean performance)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Gene\")\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "        fig_gene_heatmap.tight_layout()\n",
    "        register_figure(FIGURES, \"per_gene_heatmap\", fig_gene_heatmap)\n",
    "        display(fig_gene_heatmap)\n",
    "        plt.close(fig_gene_heatmap)\n",
    "    else:\n",
    "        print(\"Insufficient gene-level data for heatmap.\")\n",
    "else:\n",
    "    print(\"Test Pearson per-gene data unavailable for heatmap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ec692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - top features for best models\n",
    "run_df = analysis_state.get(\"run_df\", pd.DataFrame())\n",
    "summary_reset = analysis_state.get(\"summary_reset\", pd.DataFrame())\n",
    "\n",
    "# Helper function to get best model for a dataset\n",
    "def _best_model_for_run(summary_df: pd.DataFrame) -> Optional[pd.Series]:\n",
    "    if summary_df.empty:\n",
    "        return None\n",
    "    if \"test_pearson_mean\" in summary_df:\n",
    "        summary_df = summary_df.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    return summary_df.iloc[0]\n",
    "\n",
    "fig_top_features = None\n",
    "best_models = []\n",
    "\n",
    "# Use the best model overall from summary_reset\n",
    "best_row = _best_model_for_run(summary_reset)\n",
    "\n",
    "if best_row is not None and not run_df.empty:\n",
    "    run_match = run_df[\n",
    "        (run_df[\"run_name\"] == best_row[\"run_name\"]) & (run_df[\"model_id\"] == best_row[\"model_id\"])\n",
    "    ]\n",
    "    if not run_match.empty:\n",
    "        row = run_match.iloc[0]\n",
    "        model_dir = row[\"model_path\"] if isinstance(row[\"model_path\"], Path) else Path(row[\"model_path\"])\n",
    "        \n",
    "        # Load feature importance using existing helper function\n",
    "        from pathlib import Path\n",
    "        patterns = [\n",
    "            \"feature_importance*.csv\",\n",
    "            \"feature_importances*.csv\",\n",
    "            \"feature_importance*.tsv\",\n",
    "            \"feature_importances*.tsv\",\n",
    "            \"feature_importance*.parquet\",\n",
    "            \"feature_importances*.parquet\",\n",
    "        ]\n",
    "        candidates = []\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(pattern))\n",
    "        if not candidates:\n",
    "            for pattern in patterns:\n",
    "                candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "        \n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for path in candidates:\n",
    "            resolved = path.resolve()\n",
    "            if resolved in seen or resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "                continue\n",
    "            seen.add(resolved)\n",
    "            unique_candidates.append(resolved)\n",
    "        \n",
    "        importance_df = None\n",
    "        for candidate in sorted(unique_candidates):\n",
    "            try:\n",
    "                if candidate.suffix.lower() == \".parquet\":\n",
    "                    df = pd.read_parquet(candidate)\n",
    "                else:\n",
    "                    sep = \"\\t\" if candidate.suffix.lower() in {\".tsv\", \".txt\"} else \",\"\n",
    "                    df = pd.read_csv(candidate, sep=sep)\n",
    "            except Exception as exc:\n",
    "                continue\n",
    "            if df.empty:\n",
    "                continue\n",
    "            lower_cols = {col.lower(): col for col in df.columns}\n",
    "            feature_col = next((\n",
    "                lower_cols[key]\n",
    "                for key in (\"feature\", \"feature_name\", \"name\", \"variable\", \"feature_id\", \"column\")\n",
    "                if key in lower_cols\n",
    "            ), None)\n",
    "            importance_col = next((\n",
    "                lower_cols[key]\n",
    "                for key in (\n",
    "                    \"importance\",\n",
    "                    \"importance_score\",\n",
    "                    \"importance_mean\",\n",
    "                    \"score\",\n",
    "                    \"value\",\n",
    "                    \"gain\",\n",
    "                    \"weight\",\n",
    "                )\n",
    "                if key in lower_cols\n",
    "            ), None)\n",
    "            if feature_col is None or importance_col is None:\n",
    "                continue\n",
    "            out = df.copy()\n",
    "            out.rename(columns={feature_col: \"feature\", importance_col: \"importance\"}, inplace=True)\n",
    "            out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "            out[\"importance\"] = pd.to_numeric(out[\"importance\"], errors=\"coerce\")\n",
    "            out = out.dropna(subset=[\"feature\", \"importance\"])\n",
    "            if out.empty:\n",
    "                continue\n",
    "            extra_cols = [col for col in out.columns if col not in {\"feature\", \"importance\"}]\n",
    "            out = out[[\"feature\", \"importance\", *extra_cols]]\n",
    "            out.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "            importance_df = out.reset_index(drop=True)\n",
    "            break\n",
    "        \n",
    "        if importance_df is not None and not importance_df.empty and \"feature\" in importance_df.columns and \"importance\" in importance_df.columns:\n",
    "            importance_df = importance_df.nlargest(config.top_gene_count, \"importance\").copy()\n",
    "            best_models.append((importance_df, best_row[\"model_display\"]))\n",
    "\n",
    "if best_models:\n",
    "    fig_top_features, ax = plt.subplots(figsize=(10, 6))\n",
    "    importance_df, model_name = best_models[0]\n",
    "    importance_df = importance_df.copy()\n",
    "    importance_df[\"feature_short\"] = importance_df[\"feature\"].str.split(\"|\").str[-1].str[:20]\n",
    "    ax.barh(range(len(importance_df)), importance_df[\"importance\"], color=\"#2c7fb8\", alpha=0.85)\n",
    "    ax.set_yticks(range(len(importance_df)))\n",
    "    ax.set_yticklabels(importance_df[\"feature_short\"], fontsize=9)\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {config.top_gene_count} Features | Best model: {model_name}\")\n",
    "    ax.invert_yaxis()\n",
    "    sns.despine(ax=ax, left=True, bottom=True)\n",
    "    fig_top_features.tight_layout()\n",
    "    register_figure(FIGURES, \"top_features\", fig_top_features)\n",
    "    display(fig_top_features)\n",
    "    plt.close(fig_top_features)\n",
    "else:\n",
    "    print(\"No feature importance data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ce3e1",
   "metadata": {},
   "source": [
    "## 10. Embryonic vs Endothelial Comparisons\n",
    "\n",
    "Train/val boxplots and test-set violin plots for each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468741c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_state = globals().get(\"analysis_state\", {})\n",
    "metrics_long = analysis_state.get(\"metrics_long\")\n",
    "run_df = analysis_state.get(\"run_df\")\n",
    "model_display_order = analysis_state.get(\"model_display_order\", [])\n",
    "\n",
    "EMBRYONIC_RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_100genes_cpu_embryonic_20251218_114225_xgboost\",\n",
    "]\n",
    "ENDOTHELIAL_RUN_DIRECTORY_SELECTION: list[str | Path] = [\n",
    "    # Examples:\n",
    "    # \"spear_100genes_cpu_endothelial_20251220_091232_xgboost\",\n",
    "]\n",
    "EMBRYONIC_RUN_GLOB_SELECTION: list[str] = [\n",
    "    \"*embryonic*\",\n",
    "]\n",
    "ENDOTHELIAL_RUN_GLOB_SELECTION: list[str] = [\n",
    "    \"*endothelial*\",\n",
    "]\n",
    "\n",
    "def _select_run_names(run_df: pd.DataFrame, entries: list[str | Path], globs: list[str]) -> set[str]:\n",
    "    if run_df is None or run_df.empty:\n",
    "        return set()\n",
    "    manual_names = [name for name in (_resolve_run_name(entry) for entry in entries) if name]\n",
    "    glob_patterns = [pattern.strip() for pattern in globs if pattern and pattern.strip()]\n",
    "    selected = set(manual_names)\n",
    "    if glob_patterns:\n",
    "        for run_name in run_df[\"run_name\"].dropna().unique():\n",
    "            if _matches_any(run_name, glob_patterns):\n",
    "                selected.add(run_name)\n",
    "    return selected\n",
    "\n",
    "def _summarize_selection(label: str, run_names: set[str], all_runs: list[str]) -> None:\n",
    "    if run_names:\n",
    "        print(f\"{label} runs ({len(run_names)}):\", sorted(run_names))\n",
    "    else:\n",
    "        print(f\"No {label.lower()} runs matched.\")\n",
    "        print(\"Available run names:\", all_runs)\n",
    "\n",
    "def _subset_metrics(metrics_long: pd.DataFrame, run_names: set[str], splits: list[str], dataset: str) -> pd.DataFrame:\n",
    "    if metrics_long is None or metrics_long.empty or not run_names:\n",
    "        return pd.DataFrame()\n",
    "    subset = metrics_long[\n",
    "        metrics_long[\"run_name\"].isin(run_names)\n",
    "        & metrics_long[\"split\"].isin(splits)\n",
    "    ].copy()\n",
    "    if subset.empty:\n",
    "        return pd.DataFrame()\n",
    "    subset[\"dataset\"] = dataset\n",
    "    return subset\n",
    "\n",
    "if metrics_long is None or run_df is None or run_df.empty:\n",
    "    print(\"Run metadata unavailable; skipping dataset comparison figures.\")\n",
    "    fig_train_val_box = None\n",
    "    fig_test_violin_by_dataset = None\n",
    "else:\n",
    "    all_runs = sorted(run_df[\"run_name\"].dropna().unique())\n",
    "    endothelial_runs = _select_run_names(run_df, ENDOTHELIAL_RUN_DIRECTORY_SELECTION, ENDOTHELIAL_RUN_GLOB_SELECTION)\n",
    "    embryonic_runs = _select_run_names(run_df, EMBRYONIC_RUN_DIRECTORY_SELECTION, EMBRYONIC_RUN_GLOB_SELECTION)\n",
    "\n",
    "    if not embryonic_runs and endothelial_runs and not EMBRYONIC_RUN_DIRECTORY_SELECTION and not EMBRYONIC_RUN_GLOB_SELECTION:\n",
    "        embryonic_runs = set(all_runs) - endothelial_runs\n",
    "    elif not endothelial_runs and embryonic_runs and not ENDOTHELIAL_RUN_DIRECTORY_SELECTION and not ENDOTHELIAL_RUN_GLOB_SELECTION:\n",
    "        endothelial_runs = set(all_runs) - embryonic_runs\n",
    "\n",
    "    _summarize_selection(\"Embryonic\", embryonic_runs, all_runs)\n",
    "    _summarize_selection(\"Endothelial\", endothelial_runs, all_runs)\n",
    "\n",
    "    embryonic_test = _subset_metrics(metrics_long, embryonic_runs, [config.primary_split], \"Embryonic\")\n",
    "    endothelial_test = _subset_metrics(metrics_long, endothelial_runs, [config.primary_split], \"Endothelial\")\n",
    "    embryonic_train_val = _subset_metrics(metrics_long, embryonic_runs, [config.train_split, config.val_split], \"Embryonic\")\n",
    "    endothelial_train_val = _subset_metrics(metrics_long, endothelial_runs, [config.train_split, config.val_split], \"Endothelial\")\n",
    "\n",
    "    combined_test = pd.concat([embryonic_test, endothelial_test], ignore_index=True)\n",
    "    combined_train_val = pd.concat([embryonic_train_val, endothelial_train_val], ignore_index=True)\n",
    "\n",
    "    # Ensure expected label column exists for plotting.\n",
    "    if not combined_train_val.empty and 'model_display_name' not in combined_train_val and 'model_display' in combined_train_val:\n",
    "        combined_train_val['model_display_name'] = combined_train_val['model_display']\n",
    "    if not combined_test.empty and 'model_display_name' not in combined_test and 'model_display' in combined_test:\n",
    "        combined_test['model_display_name'] = combined_test['model_display']\n",
    "\n",
    "    if combined_train_val.empty and combined_test.empty:\n",
    "        print(\"No data for dataset comparison.\")\n",
    "        fig_train_val_box = None\n",
    "        fig_test_violin_by_dataset = None\n",
    "    else:\n",
    "        # --- Train/Val Boxplot by Dataset ---\n",
    "        if not combined_train_val.empty:\n",
    "            fig_train_val_box, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.boxplot(\n",
    "                data=combined_train_val,\n",
    "                x=\"model_display_name\",\n",
    "                y=\"pearson\",\n",
    "                hue=\"dataset\",\n",
    "                ax=ax,\n",
    "                palette=\"Set2\",\n",
    "            )\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Pearson Correlation\")\n",
    "            ax.set_title(\"Train/Val Pearson by Dataset (Grouped)\")\n",
    "            ax.legend(title=\"Dataset\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            FIGURES[\"train_val_pearson_by_dataset_box\"] = fig_train_val_box\n",
    "        else:\n",
    "            fig_train_val_box = None\n",
    "\n",
    "        # --- Test Violin by Dataset ---\n",
    "        if not combined_test.empty:\n",
    "            fig_test_violin_by_dataset, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.violinplot(\n",
    "                data=combined_test,\n",
    "                x=\"model_display_name\",\n",
    "                y=\"pearson\",\n",
    "                hue=\"dataset\",\n",
    "                split=True,\n",
    "                ax=ax,\n",
    "                palette=\"Set2\",\n",
    "            )\n",
    "            ax.set_xlabel(\"Model\")\n",
    "            ax.set_ylabel(\"Test Pearson Correlation\")\n",
    "            ax.set_title(\"Test Pearson by Dataset (Split Violin)\")\n",
    "            ax.legend(title=\"Dataset\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "            FIGURES[\"test_pearson_by_dataset_violin\"] = fig_test_violin_by_dataset\n",
    "        else:\n",
    "            fig_test_violin_by_dataset = None\n",
    "\n",
    "if \"analysis_state\" not in globals():\n",
    "    analysis_state = {}\n",
    "analysis_state[\"fig_train_val_box\"] = fig_train_val_box\n",
    "analysis_state[\"fig_test_violin_by_dataset\"] = fig_test_violin_by_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "test_metrics = analysis_state[\"test_metrics\"]\n",
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "short_map = analysis_state.get(\"model_short_name_map\", MODEL_ID_TO_SHORT)\n",
    "\n",
    "top_gene_count = 30\n",
    "\n",
    "if test_metrics.empty:\n",
    "    print(\"Test metrics unavailable; skipping top-gene visualisation.\")\n",
    "    analysis_state[\"top_gene_figure_keys\"] = []\n",
    "    fig_top_genes = None\n",
    "else:\n",
    "    if \"test_pearson_mean\" in summary_reset:\n",
    "        ranked_models = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    else:\n",
    "        ranked_models = summary_reset\n",
    "    top_model_limit = 1  # only the best model for top-gene performance plots\n",
    "    top_models = ranked_models.drop_duplicates(\"model_id\").head(top_model_limit)[[\"model_id\", \"model_display\"]]\n",
    "    gene_frames: list[pd.DataFrame] = []\n",
    "    for row in top_models.itertuples(index=False):\n",
    "        model_subset = test_metrics[test_metrics[\"model_id\"] == row.model_id]\n",
    "        if model_subset.empty:\n",
    "            continue\n",
    "        gene_stats = (\n",
    "            model_subset.groupby(\"gene\")[\"pearson\"].agg(mean=\"mean\", std=\"std\", count=\"count\").reset_index()\n",
    "        )\n",
    "        gene_stats.rename(\n",
    "            columns={\n",
    "                \"mean\": \"mean_test_pearson\",\n",
    "                \"std\": \"pearson_std\",\n",
    "                \"count\": \"observation_count\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        short_name = short_map.get(row.model_id, to_short_name(row.model_display)) or row.model_display\n",
    "        gene_stats[\"model_display_long\"] = row.model_display\n",
    "        gene_stats[\"model_display_short\"] = short_name\n",
    "        gene_frames.append(gene_stats)\n",
    "    if not gene_frames:\n",
    "        print(\"No genes available for top-model bar chart.\")\n",
    "        analysis_state[\"top_gene_figure_keys\"] = []\n",
    "        fig_top_genes = None\n",
    "    else:\n",
    "        top_gene_df = pd.concat(gene_frames, ignore_index=True)\n",
    "        figure_keys: list[str] = []\n",
    "        for long_name, short_name in (\n",
    "            top_gene_df[[\"model_display_long\", \"model_display_short\"]]\n",
    "            .drop_duplicates()\n",
    "            .itertuples(index=False, name=None)\n",
    "        ):\n",
    "            group = top_gene_df[top_gene_df[\"model_display_long\"] == long_name]\n",
    "            ordered = group.sort_values(\"mean_test_pearson\", ascending=False).head(top_gene_count)\n",
    "            if ordered.empty:\n",
    "                continue\n",
    "            fig, ax = plt.subplots(figsize=(9, 6))\n",
    "            colors = sns.color_palette(\"crest\", n_colors=len(ordered))\n",
    "            bars = ax.barh(\n",
    "                ordered[\"gene\"],\n",
    "                ordered[\"mean_test_pearson\"],\n",
    "                color=colors,\n",
    "                edgecolor=\"#2b2b2b\",\n",
    "                linewidth=0.4,\n",
    "            )\n",
    "            ax.set_title(f\"Top {top_gene_count} Genes | {short_name}\")\n",
    "            ax.set_xlabel(\"Mean Test Pearson\")\n",
    "            ax.set_ylabel(\"Gene\")\n",
    "            data_max = ordered[\"mean_test_pearson\"].max()\n",
    "            label_offset = max(0.01, data_max * 0.01)\n",
    "            x_max = max(1.0, data_max + 0.05) + label_offset * 4\n",
    "            ax.set_xlim(0, x_max)\n",
    "            ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "            ax.invert_yaxis()\n",
    "            for bar, mean_val in zip(bars, ordered[\"mean_test_pearson\"]):\n",
    "                y_pos = bar.get_y() + bar.get_height() / 2\n",
    "                ax.text(\n",
    "                    mean_val + label_offset,\n",
    "                    y_pos,\n",
    "                    f\"{mean_val:.3f}\",\n",
    "                    ha=\"left\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"#1a1a1a\",\n",
    "                )\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            plt.tight_layout()\n",
    "            slug = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", short_name.lower()).strip(\"_\") or \"model\"\n",
    "            key = f\"top_genes_test_performance_{slug}\"\n",
    "            register_figure(FIGURES, key, fig)\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "            figure_keys.append(key)\n",
    "        analysis_state[\"top_gene_figure_keys\"] = figure_keys\n",
    "        fig_top_genes = None\n",
    "\n",
    "\n",
    "# --- Metric distributions and heatmaps (test split) ---\n",
    "metric_df = analysis_state.get(\"metrics_long\")\n",
    "if metric_df is None or metric_df.empty:\n",
    "    print(\"Metrics unavailable; skipping metric boxplots and heatmaps.\")\n",
    "else:\n",
    "    test_metric_df = metric_df[metric_df[\"split\"] == config.primary_split].copy()\n",
    "    if \"model_display\" not in test_metric_df.columns and \"model_display_name\" in test_metric_df.columns:\n",
    "        test_metric_df[\"model_display\"] = test_metric_df[\"model_display_name\"]\n",
    "\n",
    "    metrics_to_plot = {\n",
    "        \"pearson\": \"Pearson\",\n",
    "        \"spearman\": \"Spearman\",\n",
    "        \"r2\": \"R2\",\n",
    "        \"rmse\": \"RMSE\",\n",
    "    }\n",
    "\n",
    "    pearson_order = (\n",
    "        summary_reset.groupby(\"model_display\")[\"test_pearson_mean\"].mean().sort_values(ascending=False).index\n",
    "        if \"test_pearson_mean\" in summary_reset\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    for metric_key, title_label in metrics_to_plot.items():\n",
    "        if metric_key not in test_metric_df.columns:\n",
    "            continue\n",
    "        order = [name for name in pearson_order if name in test_metric_df[\"model_display\"].unique()]\n",
    "        if not order:\n",
    "            order = sorted(test_metric_df[\"model_display\"].dropna().unique())\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.violinplot(\n",
    "            data=test_metric_df,\n",
    "            x=\"model_display\",\n",
    "            y=metric_key,\n",
    "            order=order,\n",
    "            ax=ax,\n",
    "            palette=\"Set2\",\n",
    "            inner=None,\n",
    "            cut=0,\n",
    "            linewidth=1.0,\n",
    "        )\n",
    "\n",
    "        # Clip violins to the right half.\n",
    "        for idx, poly in enumerate(ax.collections[:len(order)]):\n",
    "            for path in poly.get_paths():\n",
    "                verts = path.vertices\n",
    "                verts[:, 0] = np.clip(verts[:, 0], idx, np.inf)\n",
    "\n",
    "        # Left-half boxplot.\n",
    "        data_by_model = [\n",
    "            test_metric_df.loc[test_metric_df[\"model_display\"] == name, metric_key].dropna().values\n",
    "            for name in order\n",
    "        ]\n",
    "        positions = np.arange(len(order)) - 0.2\n",
    "        box = ax.boxplot(\n",
    "            data_by_model,\n",
    "            positions=positions,\n",
    "            widths=0.2,\n",
    "            patch_artist=True,\n",
    "            showfliers=False,\n",
    "            medianprops={\"color\": \"#222222\", \"linewidth\": 1.0},\n",
    "        )\n",
    "        for patch in box[\"boxes\"]:\n",
    "            patch.set(facecolor=\"#ffffff\", edgecolor=\"#222222\", linewidth=1.0)\n",
    "        for whisker in box[\"whiskers\"]:\n",
    "            whisker.set(color=\"#222222\", linewidth=1.0)\n",
    "        for cap in box[\"caps\"]:\n",
    "            cap.set(color=\"#222222\", linewidth=1.0)\n",
    "\n",
    "        # Scatter overlay aligned with the boxplots.\n",
    "        rng_local = np.random.default_rng(config.random_seed)\n",
    "        for idx, vals in enumerate(data_by_model):\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            jitter = rng_local.normal(0, 0.03, size=len(vals))\n",
    "            x_vals = np.full(len(vals), positions[idx]) + jitter\n",
    "            ax.scatter(\n",
    "                x_vals,\n",
    "                vals,\n",
    "                s=10,\n",
    "                alpha=0.35,\n",
    "                color=\"#2b2b2b\",\n",
    "                linewidth=0,\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"Test {title_label} by Model\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(title_label)\n",
    "        ax.set_xticks(np.arange(len(order)))\n",
    "        ax.set_xticklabels(order, rotation=45, ha=\"right\")\n",
    "        ax.set_xlim(-0.6, len(order) - 0.4)\n",
    "        sns.despine(ax=ax, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        register_figure(FIGURES, f\"test_{metric_key}_boxplot\", fig)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Combined mean-metric heatmap (Pearson/Spearman/R2/RMSE)\n",
    "    metric_means = {}\n",
    "    for metric_key, title_label in metrics_to_plot.items():\n",
    "        if metric_key not in test_metric_df.columns:\n",
    "            continue\n",
    "        series = test_metric_df.groupby(\"model_display\")[metric_key].mean().dropna()\n",
    "        if not series.empty:\n",
    "            metric_means[title_label] = series\n",
    "\n",
    "    if metric_means:\n",
    "        combined_df = pd.DataFrame(metric_means)\n",
    "        model_order = summary_reset[[\"model_display\", \"model_id\"]].drop_duplicates(\"model_id\")[\"model_display\"].tolist()\n",
    "        ordered_index = [name for name in model_order if name in combined_df.index]\n",
    "        combined_df = combined_df.reindex(index=ordered_index)\n",
    "        fig_height = max(4, 0.4 * len(combined_df.index))\n",
    "        metric_cols = list(combined_df.columns)\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=len(metric_cols),\n",
    "            figsize=(1.6 * len(metric_cols), fig_height),\n",
    "            squeeze=False,\n",
    "        )\n",
    "        for idx, metric in enumerate(metric_cols):\n",
    "            ax = axes[0][idx]\n",
    "            col_data = combined_df[[metric]]\n",
    "            vmin_col, vmax_col = compute_heatmap_limits(col_data.values, lower_percentile=10.0, upper_percentile=95.0)\n",
    "            sns.heatmap(\n",
    "                col_data,\n",
    "                cmap=\"viridis\",\n",
    "                vmin=vmin_col,\n",
    "                vmax=vmax_col,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                linewidths=0.2,\n",
    "                linecolor=\"#f5f5f5\",\n",
    "                cbar=False,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(\"\")\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel(\"Model\" if idx == 0 else \"\")\n",
    "            if idx == 0:\n",
    "                ax.yaxis.labelpad = 20\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "            if idx != 0:\n",
    "                ax.set_yticklabels([])\n",
    "        fig.suptitle(\"Summary metrics (test means)\", fontsize=12)\n",
    "        sns.despine(fig, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        register_figure(FIGURES, \"test_metric_mean_heatmap_combined\", fig)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dcc34",
   "metadata": {},
   "source": [
    "## 11. Generalization Gap Overview\n",
    "\n",
    "Train vs test Pearson summary for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "required_cols = {\"train_pearson_mean\", \"test_pearson_mean\"}\n",
    "fig_generalization_gap = None\n",
    "if not required_cols.issubset(summary_reset.columns):\n",
    "    print(\"Train/test summary columns unavailable; skipping generalization gap plot.\")\n",
    "else:\n",
    "    ranked = summary_reset.sort_values(\"test_pearson_mean\", ascending=False)\n",
    "    best_per_model = ranked.drop_duplicates(\"model_id\")\n",
    "    gap_df = best_per_model[[\"model_display\", \"train_pearson_mean\", \"test_pearson_mean\"]].copy()\n",
    "    gap_df[\"generalization_gap\"] = gap_df[\"train_pearson_mean\"] - gap_df[\"test_pearson_mean\"]\n",
    "    gap_df.sort_values(\"generalization_gap\", ascending=False, inplace=True)\n",
    "    fig_height = max(4, 0.35 * len(gap_df))\n",
    "    fig_generalization_gap, ax = plt.subplots(figsize=(8, fig_height))\n",
    "    colors = sns.color_palette(\"mako\", n_colors=len(gap_df))\n",
    "    bars = ax.barh(\n",
    "        gap_df[\"model_display\"],\n",
    "        gap_df[\"generalization_gap\"],\n",
    "        color=colors,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax.axvline(0.0, color=\"#6d6d6d\", linestyle=\"--\", linewidth=1)\n",
    "    gap_min = gap_df[\"generalization_gap\"].min()\n",
    "    gap_max = gap_df[\"generalization_gap\"].max()\n",
    "    span = max(0.01, gap_max - gap_min)\n",
    "    margin = max(0.08, span * 0.12)\n",
    "    xmin = min(gap_min - margin, gap_min - 0.02)\n",
    "    xmin = min(xmin, -0.15)\n",
    "    xmax = max(gap_max + margin, 0.15)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xlabel(\"Train minus Test Mean Pearson\")\n",
    "    ax.set_ylabel(\"Model\")\n",
    "    ax.set_title(\"Generalization Gap (Train - Test)\")\n",
    "    label_offset = (xmax - xmin) * 0.02\n",
    "    for bar, value in zip(bars, gap_df[\"generalization_gap\"]):\n",
    "        y = bar.get_y() + bar.get_height() / 2\n",
    "        if value >= 0:\n",
    "            ax.text(value + label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"left\", fontsize=9, color=\"#1f1f1f\")\n",
    "        else:\n",
    "            ax.text(value - label_offset, y, f\"{value:.3f}\", va=\"center\", ha=\"right\", fontsize=9, color=\"#1f1f1f\")\n",
    "    sns.despine(fig_generalization_gap, left=True, bottom=True)\n",
    "    fig_generalization_gap.tight_layout()\n",
    "register_figure(FIGURES, \"generalization_gap\", fig_generalization_gap)\n",
    "if fig_generalization_gap is not None:\n",
    "    display(fig_generalization_gap)\n",
    "    plt.close(fig_generalization_gap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f54094",
   "metadata": {},
   "source": [
    "## 12. Top-Model Diagnostics\n",
    "\n",
    "Quick previews for the best-performing models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "best_models = summary_reset.head(config.top_model_count).reset_index(drop=True)\n",
    "if best_models.empty:\n",
    "    print(\"No models available for scatter plot previews.\")\n",
    "else:\n",
    "    missing_assets: list[str] = []\n",
    "    for model_row in best_models.itertuples(index=False):\n",
    "        run_rows = run_df[(run_df[\"model_id\"] == model_row.model_id) & (run_df[\"run_name\"] == model_row.run_name)]\n",
    "        if run_rows.empty:\n",
    "            missing_assets.append(\n",
    "                f\"Missing run directory metadata for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        row = run_rows.iloc[0]\n",
    "        model_dir_raw = row[\"model_path\"]\n",
    "        model_dir = model_dir_raw if isinstance(model_dir_raw, Path) else Path(model_dir_raw)\n",
    "        if not model_dir.exists():\n",
    "            missing_assets.append(\n",
    "                f\"Model directory missing on disk for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        scatter_path = model_dir / \"scatter_test.png\"\n",
    "        if scatter_path.exists():\n",
    "            display(Markdown(\n",
    "                f\"**{model_row.model_display}** (run `{model_row.run_name}`) \u2014 `scatter_test.png`\"\n",
    "            ))\n",
    "            display(Image(filename=str(scatter_path)))\n",
    "        else:\n",
    "            predictions_raw = row.get(\"predictions_path\") if isinstance(row, pd.Series) else None\n",
    "            predictions_path = None\n",
    "            if predictions_raw:\n",
    "                predictions_path = predictions_raw if isinstance(predictions_raw, Path) else Path(predictions_raw)\n",
    "            if predictions_path and predictions_path.exists():\n",
    "                missing_assets.append(\n",
    "                    f\"`scatter_test.png` not found for {model_row.model_display} (run {model_row.run_name}). \"\n",
    "                    f\"Checked `{to_relative_path(model_dir, config.project_root)}`.\"\n",
    "                )\n",
    "            else:\n",
    "                missing_assets.append(\n",
    "                    f\"No test predictions available for {model_row.model_display} (run {model_row.run_name}); skipping scatter preview.\"\n",
    "                )\n",
    "    if missing_assets:\n",
    "        for message in missing_assets:\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82185fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reset = analysis_state[\"summary_reset\"]\n",
    "run_df = analysis_state[\"run_df\"]\n",
    "\n",
    "best_models = summary_reset.head(config.top_model_count).reset_index(drop=True)\n",
    "if best_models.empty:\n",
    "    print(\"Best models not identified; skipping training history previews.\")\n",
    "else:\n",
    "    missing_assets: list[str] = []\n",
    "    for model_row in best_models.itertuples(index=False):\n",
    "        run_rows = run_df[(run_df[\"model_id\"] == model_row.model_id) & (run_df[\"run_name\"] == model_row.run_name)]\n",
    "        if run_rows.empty:\n",
    "            missing_assets.append(\n",
    "                f\"Missing run directory metadata for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        row = run_rows.iloc[0]\n",
    "        model_dir_raw = row[\"model_path\"]\n",
    "        model_dir = model_dir_raw if isinstance(model_dir_raw, Path) else Path(model_dir_raw)\n",
    "        if not model_dir.exists():\n",
    "            missing_assets.append(\n",
    "                f\"Model directory missing on disk for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "            continue\n",
    "        history_candidates = [model_dir / \"training_history_loss.png\"]\n",
    "        history_dir = model_dir / \"histories\"\n",
    "        if history_dir.exists():\n",
    "            history_candidates.extend(sorted(history_dir.glob(\"*_loss.png\")))\n",
    "        history_path = next((path for path in history_candidates if path.exists()), None)\n",
    "        if history_path is not None:\n",
    "            display(Markdown(\n",
    "                f\"**{model_row.model_display}** (run `{model_row.run_name}`) \u2014 `{history_path.name}`\"\n",
    "            ))\n",
    "            display(Image(filename=str(history_path)))\n",
    "        else:\n",
    "            missing_assets.append(\n",
    "                f\"No training history plot available for {model_row.model_display} (run {model_row.run_name}).\"\n",
    "            )\n",
    "    if missing_assets:\n",
    "        for message in missing_assets:\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472eac75",
   "metadata": {},
   "source": [
    "## 13. Export Artifacts\n",
    "\n",
    "Persist figures and tables to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56470205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional exploratory figures (moved from manuscript figures)\n",
    "metrics_wide = analysis_state.get(\"metrics_wide\")\n",
    "summary_reset = analysis_state.get(\"summary_reset\")\n",
    "\n",
    "# Per-gene heatmap\n",
    "fig_gene_heatmap = None\n",
    "if isinstance(metrics_wide, pd.DataFrame) and not metrics_wide.empty and f\"{config.primary_split}_pearson\" in metrics_wide:\n",
    "    test_pearson_per_gene = metrics_wide.pivot_table(\n",
    "        index=\"gene\",\n",
    "        columns=\"model_display\",\n",
    "        values=f\"{config.primary_split}_pearson\",\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "    if not test_pearson_per_gene.empty and len(test_pearson_per_gene) > 3:\n",
    "        gene_means = test_pearson_per_gene.mean(axis=1).sort_values(ascending=False)\n",
    "        test_pearson_per_gene = test_pearson_per_gene.loc[gene_means.index]\n",
    "        col_means = test_pearson_per_gene.mean(axis=0).sort_values(ascending=False)\n",
    "        test_pearson_per_gene = test_pearson_per_gene[col_means.index]\n",
    "        fig_height = max(8, 0.15 * len(test_pearson_per_gene))\n",
    "        fig_width = max(9, 0.4 * len(test_pearson_per_gene.columns))\n",
    "        fig_gene_heatmap, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        vmin, vmax = compute_heatmap_limits(test_pearson_per_gene.values)\n",
    "        sns.heatmap(\n",
    "            test_pearson_per_gene,\n",
    "            cmap=\"RdYlGn\",\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            annot=False,\n",
    "            cbar_kws={\"label\": \"Test Pearson\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Per-Gene Test Pearson by Model (sorted by mean performance)\")\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"Gene\")\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "        fig_gene_heatmap.tight_layout()\n",
    "        register_figure(FIGURES, \"per_gene_heatmap\", fig_gene_heatmap)\n",
    "        display(fig_gene_heatmap)\n",
    "        plt.close(fig_gene_heatmap)\n",
    "\n",
    "# Top features (best model)\n",
    "fig_top_features = None\n",
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "if best_details is None:\n",
    "    print(\"Best model context missing; skipping top feature summary.\")\n",
    "else:\n",
    "    importance_df, _ = _load_feature_importance_table(best_details[\"model_dir\"])\n",
    "    if importance_df.empty:\n",
    "        print(\"No feature importance data available for top-features plot.\")\n",
    "    else:\n",
    "        top_df = importance_df.nlargest(20, \"importance\").copy()\n",
    "        top_df[\"feature_short\"] = top_df[\"feature\"].str.split(\"|\").str[-1].str[:24]\n",
    "        fig_top_features, ax = plt.subplots(figsize=(9, 6))\n",
    "        ax.barh(range(len(top_df)), top_df[\"importance\"], color=\"#2c7fb8\", alpha=0.85)\n",
    "        ax.set_yticks(range(len(top_df)))\n",
    "        ax.set_yticklabels(top_df[\"feature_short\"], fontsize=8)\n",
    "        ax.set_xlabel(\"Importance\")\n",
    "        ax.set_title(f\"Top Features | Best Model: {best_details['model_display']}\")\n",
    "        ax.invert_yaxis()\n",
    "        sns.despine(fig_top_features, left=True, bottom=True)\n",
    "        fig_top_features.tight_layout()\n",
    "        register_figure(FIGURES, \"top_features\", fig_top_features)\n",
    "        display(fig_top_features)\n",
    "        plt.close(fig_top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51aa9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_SAVE_PLAN: dict[str, list[Path]] = {\n",
    "    \"test_pearson_heatmap_all_models\": [config.fig_dir / \"test_pearson_heatmap_all_models.png\"],\n",
    "    \"test_pearson_violin\": [config.fig_dir / \"test_pearson_violin.png\"],\n",
    "    \"split_comparison_overview\": [config.fig_dir / \"split_comparison_overview.png\"],\n",
    "    \"test_pearson_heatmap_top20_per_model_blocks\": [config.fig_dir / \"test_pearson_heatmap_top20_per_model_blocks.png\"],\n",
    "    \"test_pearson_boxplot\": [config.fig_dir / \"test_pearson_boxplot.png\"],\n",
    "    \"test_spearman_boxplot\": [config.fig_dir / \"test_spearman_boxplot.png\"],\n",
    "    \"test_r2_boxplot\": [config.fig_dir / \"test_r2_boxplot.png\"],\n",
    "    \"test_rmse_boxplot\": [config.fig_dir / \"test_rmse_boxplot.png\"],\n",
    "    \"test_metric_mean_heatmap_combined\": [config.fig_dir / \"test_metric_mean_heatmap_combined.png\"],\n",
    "    \"generalization_gap\": [config.fig_dir / \"generalization_gap.png\"],\n",
    "    \"best_model_feature_importance_top\": [config.fig_dir / \"best_model_feature_importance_top.png\"],\n",
    "    \"train_val_box_by_dataset\": [config.fig_dir / \"train_val_box_by_dataset.png\"],\n",
    "    \"test_violin_by_dataset\": [config.fig_dir / \"test_violin_by_dataset.png\"],\n",
    "    \"resource_usage_summary\": [config.fig_dir / \"resource_usage_summary.png\"],\n",
    "    \"per_gene_heatmap\": [config.fig_dir / \"per_gene_heatmap.png\"],\n",
    "    \"top_features\": [config.fig_dir / \"top_features.png\"],\n",
    "}\n",
    "\n",
    "top_gene_keys = analysis_state.get(\"top_gene_figure_keys\", [])\n",
    "for key in top_gene_keys:\n",
    "    filename = f\"{key}.png\"\n",
    "    FIGURE_SAVE_PLAN[key] = [config.fig_dir / filename]\n",
    "\n",
    "TABLE_SAVE_PLAN: dict[str, list[Path]] = {\n",
    "    \"metrics_per_gene_master\": [config.reports_dir / \"metrics_per_gene_master.csv\"],\n",
    "    \"summary_metrics_all_models\": [config.reports_dir / \"summary_metrics_all_models.csv\"],\n",
    "}\n",
    "\n",
    "saved_figures: list[Path] = []\n",
    "for key, targets in FIGURE_SAVE_PLAN.items():\n",
    "    fig = FIGURES.get(key)\n",
    "    if fig is None:\n",
    "        print(f\"Skipping figure '{key}' (not generated).\")\n",
    "        continue\n",
    "    for target in targets:\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(target, bbox_inches=\"tight\")\n",
    "        saved_figures.append(target)\n",
    "\n",
    "saved_tables: list[Path] = []\n",
    "for key, targets in TABLE_SAVE_PLAN.items():\n",
    "    table = TABLES.get(key)\n",
    "    if table is None or table.empty:\n",
    "        print(f\"Skipping table '{key}' (not generated or empty).\")\n",
    "        continue\n",
    "    for target in targets:\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        table.to_csv(target, index=False)\n",
    "        saved_tables.append(target)\n",
    "\n",
    "print(\"Saved figures:\")\n",
    "for target in saved_figures:\n",
    "    print(f\" - {target}\")\n",
    "\n",
    "print(\"Saved tables:\")\n",
    "for target in saved_tables:\n",
    "    print(f\" - {target}\")\n",
    "\n",
    "if saved_figures:\n",
    "    analysis_metadata[\"saved_figures\"] = [to_relative_path(path, config.project_root) for path in saved_figures]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10367c1",
   "metadata": {},
   "source": [
    "## 14. Session Metadata\n",
    "\n",
    "Run summary details for logging and provenance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_at = analysis_metadata.get(\"generated_at\", \"n/a\")\n",
    "results_root = analysis_metadata.get(\"results_root\", \"n/a\")\n",
    "run_count = analysis_metadata.get(\"run_count\", \"n/a\")\n",
    "model_count = analysis_metadata.get(\"model_count\", \"n/a\")\n",
    "best_display = analysis_metadata.get(\"best_model_display\", \"n/a\")\n",
    "best_id = analysis_metadata.get(\"best_model_id\", \"n/a\")\n",
    "best_run = analysis_metadata.get(\"best_run_name\", \"n/a\")\n",
    "report_markdown = f\"\"\"### Analysis Metadata\n",
    "- Generated at: `{generated_at}`\n",
    "- Results root: `{results_root}`\n",
    "- Runs analysed: `{run_count}`\n",
    "- Models analysed: `{model_count}`\n",
    "- Best model: `{best_display}` (`{best_id}`)\n",
    "- Source run: `{best_run}`\"\"\"\n",
    "display(Markdown(report_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff17111",
   "metadata": {},
   "source": [
    "## 15. Feature Importance (Best Model)\n",
    "\n",
    "Load feature-importance exports for the best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b739459",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "if best_details is None:\n",
    "    summary_reset = analysis_state.get(\"summary_reset\")\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    if summary_reset is None or summary_reset.empty:\n",
    "        raise RuntimeError(\"Best-model summary unavailable; please rerun Section 5 before using Section 13.\")\n",
    "    if run_df is None or run_df.empty:\n",
    "        raise RuntimeError(\"Run metadata missing; rerun Section 4 before using Section 13.\")\n",
    "    leader = summary_reset.iloc[0]\n",
    "    best_model_id = str(leader[\"model_id\"])\n",
    "    best_run_name = str(leader[\"run_name\"])\n",
    "    best_model_display = str(leader.get(\"model_display\", best_model_id))\n",
    "    match = run_df[(run_df[\"model_id\"] == best_model_id) & (run_df[\"run_name\"] == best_run_name)]\n",
    "    if match.empty:\n",
    "        raise RuntimeError(f\"Unable to locate run folder for {best_model_display} (run {best_run_name}).\")\n",
    "    row = match.iloc[0]\n",
    "    model_path_value = row.get(\"model_path\")\n",
    "    if isinstance(model_path_value, Path):\n",
    "        best_model_dir = model_path_value.resolve()\n",
    "    elif isinstance(model_path_value, str) and model_path_value:\n",
    "        best_model_dir = Path(model_path_value).resolve()\n",
    "    else:\n",
    "        raise RuntimeError(\"Model directory not recorded for the leading model.\")\n",
    "    run_path_value = row.get(\"run_path\")\n",
    "    if isinstance(run_path_value, Path):\n",
    "        best_run_dir = run_path_value.resolve()\n",
    "    elif isinstance(run_path_value, str) and run_path_value:\n",
    "        best_run_dir = Path(run_path_value).resolve()\n",
    "    else:\n",
    "        best_run_dir = best_model_dir.parent\n",
    "    metrics_value = row.get(\"metrics_path\")\n",
    "    if isinstance(metrics_value, Path):\n",
    "        metrics_path = metrics_value.resolve()\n",
    "    elif isinstance(metrics_value, str) and metrics_value:\n",
    "        metrics_path = Path(metrics_value).resolve()\n",
    "    else:\n",
    "        metrics_path = None\n",
    "    preds_value = row.get(\"predictions_path\")\n",
    "    if isinstance(preds_value, Path):\n",
    "        predictions_path = preds_value.resolve()\n",
    "    elif isinstance(preds_value, str) and preds_value:\n",
    "        predictions_path = Path(preds_value).resolve()\n",
    "    else:\n",
    "        predictions_path = None\n",
    "    best_details = {\n",
    "        \"model_id\": best_model_id,\n",
    "        \"model_display\": best_model_display,\n",
    "        \"run_name\": best_run_name,\n",
    "        \"model_dir\": best_model_dir,\n",
    "        \"run_dir\": best_run_dir,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"predictions_path\": predictions_path,\n",
    "    }\n",
    "    analysis_state[\"best_model_details\"] = best_details\n",
    "relative_model_dir = to_relative_path(best_details[\"model_dir\"], config.project_root)\n",
    "summary_lines = [\n",
    "    f\"**Best model context**: `{best_details['model_display']}` (`{best_details['model_id']}`) from run `{best_details['run_name']}`\",\n",
    "    \"\",\n",
    "    f\"- Model directory: `{relative_model_dir}`\",\n",
    "]\n",
    "if best_details.get(\"metrics_path\") is not None:\n",
    "    summary_lines.append(\n",
    "        f\"- Metrics file: `{to_relative_path(best_details['metrics_path'], config.project_root)}`\"\n",
    ")\n",
    "if best_details.get(\"predictions_path\") is not None:\n",
    "    summary_lines.append(\n",
    "        f\"- Predictions file: `{to_relative_path(best_details['predictions_path'], config.project_root)}`\"\n",
    ")\n",
    "display(Markdown(\"\\n\".join(summary_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_feature_importance_table(model_dir: Path) -> tuple[pd.DataFrame, Optional[Path]]:\n",
    "    \"\"\"Locate and standardise feature importances exported by the training pipeline.\"\"\"\n",
    "    patterns = [\n",
    "        \"feature_importance*.csv\",\n",
    "        \"feature_importances*.csv\",\n",
    "        \"feature_importance*.tsv\",\n",
    "        \"feature_importances*.tsv\",\n",
    "        \"feature_importance*.parquet\",\n",
    "        \"feature_importances*.parquet\",\n",
    "    ]\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns:\n",
    "        candidates.extend(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        for pattern in patterns:\n",
    "            candidates.extend(model_dir.glob(f\"**/{pattern}\"))\n",
    "    unique_candidates: list[Path] = []\n",
    "    seen: set[Path] = set()\n",
    "    for path in candidates:\n",
    "        resolved = path.resolve()\n",
    "        if resolved in seen:\n",
    "            continue\n",
    "        if resolved.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "            continue\n",
    "        seen.add(resolved)\n",
    "        unique_candidates.append(resolved)\n",
    "    for candidate in sorted(unique_candidates):\n",
    "        try:\n",
    "            if candidate.suffix.lower() == \".parquet\":\n",
    "                df = pd.read_parquet(candidate)\n",
    "            else:\n",
    "                sep = \"\\t\" if candidate.suffix.lower() in {\".tsv\", \".txt\"} else \",\"\n",
    "                df = pd.read_csv(candidate, sep=sep)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {candidate.name}: failed to load ({exc})\")\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        lower_cols = {col.lower(): col for col in df.columns}\n",
    "        feature_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\"feature\", \"feature_name\", \"name\", \"variable\", \"feature_id\", \"column\")\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        importance_col = next((\n",
    "            lower_cols[key]\n",
    "            for key in (\n",
    "                \"importance\",\n",
    "                \"importance_score\",\n",
    "                \"importance_mean\",\n",
    "                \"score\",\n",
    "                \"value\",\n",
    "                \"gain\",\n",
    "                \"weight\",\n",
    "            )\n",
    "            if key in lower_cols\n",
    "        ), None)\n",
    "        if feature_col is None or importance_col is None:\n",
    "            continue\n",
    "        out = df.copy()\n",
    "        out.rename(columns={feature_col: \"feature\", importance_col: \"importance\"}, inplace=True)\n",
    "        out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "        out[\"importance\"] = pd.to_numeric(out[\"importance\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"feature\", \"importance\"])\n",
    "        if out.empty:\n",
    "            continue\n",
    "        extra_cols = [col for col in out.columns if col not in {\"feature\", \"importance\"}]\n",
    "        out = out[[\"feature\", \"importance\", *extra_cols]]\n",
    "        out.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        return out.reset_index(drop=True), candidate\n",
    "    return pd.DataFrame(columns=[\"feature\", \"importance\"]), None\n",
    "\n",
    "importance_df, importance_path = _load_feature_importance_table(best_details[\"model_dir\"])\n",
    "analysis_state[\"best_model_feature_importances\"] = importance_df\n",
    "analysis_state[\"best_model_feature_importances_path\"] = importance_path\n",
    "if importance_df.empty:\n",
    "    display(Markdown(\n",
    "        \"**Feature importances**: no compatible export found in the best-model directory. \"\n",
    "        \"Upload or regenerate a table named `feature_importance*.csv|tsv|parquet` to enable the following cells.\"\n",
    "    ))\n",
    "else:\n",
    "    note_lines = [\n",
    "        \"**Feature importances**: detected data source.\",\n",
    "    ]\n",
    "    if importance_path is not None:\n",
    "        note_lines.append(\n",
    "            f\"- Source file: `{to_relative_path(importance_path, config.project_root)}`\"\n",
    ")\n",
    "        note_lines.append(f\"- Rows: {len(importance_df):,}\")\n",
    "    display(Markdown(\"\\n\".join(note_lines)))\n",
    "    display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping distribution plot.\")\n",
    "    fig_feature_importance_dist = None\n",
    "else:\n",
    "    importance_values = importance_df[\"importance\"].astype(float)\n",
    "    importance_values = importance_values[np.isfinite(importance_values)]\n",
    "    if importance_values.empty:\n",
    "        print(\"Importance column contains no finite values; skipping distribution plot.\")\n",
    "        fig_feature_importance_dist = None\n",
    "    else:\n",
    "        fig_feature_importance_dist, ax = plt.subplots(figsize=(6.5, 4.2))\n",
    "        sns.histplot(importance_values, bins=40, kde=True, color=\"#377eb8\", ax=ax)\n",
    "        ax.set_title(\"Feature Importance Distribution | Best Model\")\n",
    "        ax.set_xlabel(\"Importance score\")\n",
    "        ax.set_ylabel(\"Feature count\")\n",
    "        sns.despine(fig_feature_importance_dist, left=True, bottom=True)\n",
    "        plt.tight_layout()\n",
    "        display(fig_feature_importance_dist)\n",
    "        plt.close(fig_feature_importance_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0248dfa",
   "metadata": {},
   "source": [
    "### Top Feature Importance (Best Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ede288",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "best_details = analysis_state.get(\"best_model_details\", {})\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping top-feature plot.\")\n",
    "    fig_feature_importance_top = None\n",
    "else:\n",
    "    top_n = min(20, len(importance_df))\n",
    "    top_df = importance_df.nlargest(top_n, \"importance\").copy()\n",
    "    top_df[\"feature\"] = top_df[\"feature\"].astype(str)\n",
    "    fig_height = max(4.0, 0.35 * top_n + 1.0)\n",
    "    fig_feature_importance_top, ax = plt.subplots(figsize=(7.5, fig_height))\n",
    "    sns.barplot(\n",
    "        data=top_df,\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        color=\"#4C78A8\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    model_label = best_details.get(\"model_display\", \"Best Model\")\n",
    "    ax.set_title(f\"Top Feature Importance | {model_label}\")\n",
    "    ax.set_xlabel(\"Importance score\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    sns.despine(fig_feature_importance_top, left=True, bottom=True)\n",
    "    plt.tight_layout()\n",
    "register_figure(FIGURES, \"best_model_feature_importance_top\", fig_feature_importance_top)\n",
    "if fig_feature_importance_top is not None:\n",
    "    display(fig_feature_importance_top)\n",
    "    plt.close(fig_feature_importance_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "distance_col = None\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping TSS distance analysis.\")\n",
    "else:\n",
    "    candidate_cols = [\n",
    "        \"distance_to_tss_bp\",\n",
    "        \"distance_to_tss\",\n",
    "        \"distance_bp\",\n",
    "        \"tss_distance_bp\",\n",
    "    ]\n",
    "    for candidate in candidate_cols:\n",
    "        if candidate in importance_df.columns:\n",
    "            distance_col = candidate\n",
    "            break\n",
    "    if distance_col is None:\n",
    "        bin_pattern = re.compile(r\"bin_(-?\\d+)_to_(-?\\d+)\")\n",
    "        def _infer_distance(feature_name: str) -> Optional[float]:\n",
    "            if not isinstance(feature_name, str):\n",
    "                return None\n",
    "            token = feature_name.split(\"|\", 1)[-1]\n",
    "            match = bin_pattern.search(token)\n",
    "            if match:\n",
    "                start_bp = float(match.group(1))\n",
    "                end_bp = float(match.group(2))\n",
    "                return 0.5 * (start_bp + end_bp)\n",
    "            return None\n",
    "        inferred = importance_df[\"feature\"].map(_infer_distance)\n",
    "        if inferred.notna().any():\n",
    "            importance_df = importance_df.copy()\n",
    "            importance_df[\"distance_to_tss_bp\"] = inferred\n",
    "            distance_col = \"distance_to_tss_bp\"\n",
    "            analysis_state[\"best_model_feature_importances\"] = importance_df\n",
    "    if distance_col is None:\n",
    "        print(\"Unable to derive feature-to-TSS distances from the available data; skipping scatter plot.\")\n",
    "    else:\n",
    "        analysis_state[\"feature_importance_distance_column\"] = distance_col\n",
    "        valid = importance_df.dropna(subset=[\"importance\", distance_col]).copy()\n",
    "        if valid.empty:\n",
    "            print(\"No features contained both importance scores and TSS distances.\")\n",
    "        else:\n",
    "            valid[\"distance_kb\"] = valid[distance_col].astype(float) / 1_000.0\n",
    "            corr_value = valid[[\"distance_kb\", \"importance\"]].corr(method=\"spearman\").loc[\"distance_kb\", \"importance\"]\n",
    "            fig_distance, ax = plt.subplots(figsize=(7.5, 4.8))\n",
    "            sns.scatterplot(\n",
    "                data=valid,\n",
    "                x=\"distance_kb\",\n",
    "                y=\"importance\",\n",
    "                s=32,\n",
    "                alpha=0.6,\n",
    "                edgecolor=\"none\",\n",
    "                color=\"#4daf4a\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            sns.regplot(\n",
    "                data=valid,\n",
    "                x=\"distance_kb\",\n",
    "                y=\"importance\",\n",
    "                scatter=False,\n",
    "                lowess=True,\n",
    "                color=\"#984ea3\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.axvline(0.0, color=\"#999999\", linestyle=\"--\", linewidth=1)\n",
    "            ax.set_xlabel(\"Distance to TSS (kb)\")\n",
    "            ax.set_ylabel(\"Feature importance\")\n",
    "            ax.set_title(\"Feature Importance vs. Distance to TSS | Best Model\")\n",
    "            ax.text(\n",
    "                0.01,\n",
    "                0.98,\n",
    "                f\"Spearman r = {corr_value:.3f}\",\n",
    "                transform=ax.transAxes,\n",
    "                ha=\"left\",\n",
    "                va=\"top\",\n",
    "                fontsize=10,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.7, edgecolor=\"#cccccc\"),\n",
    "            )\n",
    "            sns.despine(fig_distance, left=True, bottom=True)\n",
    "            plt.tight_layout()\n",
    "            display(fig_distance)\n",
    "            plt.close(fig_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b52ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; skipping ATAC peak view.\")\n",
    "else:\n",
    "    feature_series = importance_df[\"feature\"].astype(str)\n",
    "    if \"feature_type\" in importance_df.columns:\n",
    "        atac_mask = importance_df[\"feature_type\"].astype(str).str.contains(\"atac|peak\", case=False, na=False)\n",
    "    else:\n",
    "        atac_mask = feature_series.str.contains(\"peak\", case=False, na=False)\n",
    "    atac_df = importance_df[atac_mask].copy()\n",
    "    if atac_df.empty:\n",
    "        print(\"No ATAC-related feature names detected; adjust filtering logic if alternative naming is used.\")\n",
    "    else:\n",
    "        top_atac = atac_df.nlargest(min(25, len(atac_df)), \"importance\")\n",
    "        fig_atac, ax = plt.subplots(figsize=(9, max(4, 0.35 * len(top_atac))))\n",
    "        sns.barplot(\n",
    "            data=top_atac,\n",
    "            x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            palette=\"crest\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(\"Top ATAC Feature Importances | Best Model\")\n",
    "        ax.set_xlabel(\"Importance score\")\n",
    "        ax.set_ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        display(fig_atac)\n",
    "        plt.close(fig_atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2543d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "TOP_FEATURE_COUNT = 1_000\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; unable to derive top features.\")\n",
    "else:\n",
    "    top_n = min(TOP_FEATURE_COUNT, len(importance_df))\n",
    "    top_features_df = importance_df.nlargest(top_n, \"importance\").reset_index(drop=True)\n",
    "    analysis_state[\"best_model_top_features\"] = top_features_df\n",
    "    print(f\"Captured top {top_n} features by importance for the best model.\")\n",
    "    display(top_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = analysis_state.get(\"best_model_feature_importances\")\n",
    "distance_col = analysis_state.get(\"feature_importance_distance_column\")\n",
    "distance_window_kb = 50\n",
    "if importance_df is None or importance_df.empty:\n",
    "    print(\"Feature importance data unavailable; cannot filter by TSS proximity.\")\n",
    "elif distance_col is None or distance_col not in importance_df.columns:\n",
    "    print(\"No inferred distance-to-TSS column available; run the previous distance cell first or provide the column explicitly.\")\n",
    "else:\n",
    "    window_mask = importance_df[distance_col].abs() <= distance_window_kb * 1_000\n",
    "    subset_df = importance_df[window_mask].copy()\n",
    "    if subset_df.empty:\n",
    "        print(\n",
    "            f\"No features fall within \u00b1{distance_window_kb} kb of the TSS according to column '{distance_col}'.\"\n",
    ")\n",
    "    else:\n",
    "        subset_df.sort_values(\"importance\", ascending=False, inplace=True)\n",
    "        analysis_state[\"best_model_features_within_window\"] = subset_df\n",
    "        print(\n",
    "            f\"Identified {len(subset_df):,} features within \u00b1{distance_window_kb} kb of the TSS (column '{distance_col}').\"\n",
    ")\n",
    "        display(subset_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c104a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "top_features_df = analysis_state.get(\"best_model_top_features\")\n",
    "subset_df = analysis_state.get(\"best_model_features_within_window\")\n",
    "importance_path = analysis_state.get(\"best_model_feature_importances_path\")\n",
    "distance_col = analysis_state.get(\"feature_importance_distance_column\")\n",
    "distance_window_kb = 50\n",
    "if best_details is None:\n",
    "    print(\"Best model context missing; rerun the first cell in Section 13.\")\n",
    "else:\n",
    "    run_config_path = best_details[\"run_dir\"] / \"run_configuration.json\"\n",
    "    lines = [\n",
    "        \"**Execution scaffolding for reruns**\",\n",
    "        \"\",\n",
    "        f\"- Baseline config JSON: `{to_relative_path(run_config_path, config.project_root)}`\",\n",
    "        f\"- Suggested CLI template: `python -m spear.cli --config-json {to_relative_path(run_config_path, config.project_root)} --models {best_details['model_id']} --run-name {best_details['run_name']}_experimental`\",\n",
    "    ]\n",
    "    if importance_path is not None:\n",
    "        lines.append(\n",
    "            f\"- Feature importance table: `{to_relative_path(importance_path, config.project_root)}`\"\n",
    "        )\n",
    "    if top_features_df is not None and not top_features_df.empty:\n",
    "        lines.append(\n",
    "            \"- Export `analysis_state[\\\"best_model_top_features\\\"]` to CSV/TSV and feed it into your data loader to mimic a top-1k feature run.\",\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\"- Top-1k feature shortlist unavailable; populate feature importances first.\")\n",
    "    if subset_df is not None and not subset_df.empty and distance_col:\n",
    "        lines.append(\n",
    "            f\"- Features within \u00b1{distance_window_kb} kb (`{distance_col}`) cached in `analysis_state['best_model_features_within_window']`.\",\n",
    "        )\n",
    "        lines.append(\n",
    "            f\"  Update `TrainingConfig.window_bp` to {distance_window_kb * 1_000:,} (\u00b1{distance_window_kb} kb) in the JSON before re-running.\",\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\n",
    "            f\"- No cached subset for the \u00b1{distance_window_kb} kb window; rerun the distance cell after providing TSS metadata.\",\n",
    "        )\n",
    "    lines.append(\n",
    "        \"- Reminder: the notebook does not launch training jobs automatically; please run the CLI in a new terminal or submit via SLURM.\",\n",
    "    )\n",
    "    display(Markdown(\"\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5d66c",
   "metadata": {},
   "source": [
    "## 16. Resource Usage Comparison\n",
    "\n",
    "Summarize SLURM log resource usage across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea226422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "log_dir = config.project_root / \"output\" / \"logs\"\n",
    "log_paths = sorted(log_dir.glob(\"spear_cellwise_chunk*.out\"))\n",
    "if not log_paths:\n",
    "    print(\"No Slurm logs found; skipping resource usage summary.\")\n",
    "    fig_resource = None\n",
    "else:\n",
    "    if \"analysis_state\" not in globals():\n",
    "        analysis_state = {}\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    display_lookup: dict[str, str] = {}\n",
    "    model_id_lookup: dict[str, str] = {}\n",
    "    if isinstance(run_df, pd.DataFrame) and not run_df.empty:\n",
    "        if \"model_display\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            display_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_display\"]))\n",
    "        if \"model_id\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            model_id_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_id\"]))\n",
    "\n",
    "    rss_pattern = re.compile(r\"Resource snapshot\\s*\\|\\s*([^|]+)\\|\\s*rss=([0-9.]+)\\s*GiB\", re.IGNORECASE)\n",
    "    rows: list[dict[str, object]] = []\n",
    "    for path in log_paths:\n",
    "        try:\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                lines = fh.readlines()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        run_name = None\n",
    "        max_rss = None\n",
    "        peak_stage = None\n",
    "        for line in lines:\n",
    "            if \"run_name=\" in line:\n",
    "                idx = line.find(\"run_name=\")\n",
    "                token_start = idx + len(\"run_name=\")\n",
    "                rest = line[token_start:]\n",
    "                import re\n",
    "                candidate_match = re.search(r\"[\\w\\-]+\", rest)\n",
    "                if candidate_match:\n",
    "                    run_name = candidate_match.group()\n",
    "            match = rss_pattern.search(line)\n",
    "            if match:\n",
    "                stage_str = match.group(1).strip()\n",
    "                rss_val = float(match.group(2))\n",
    "                if max_rss is None or rss_val > max_rss:\n",
    "                    max_rss = rss_val\n",
    "                    peak_stage = stage_str\n",
    "\n",
    "        if run_name and max_rss is not None:\n",
    "            display_name = display_lookup.get(run_name, run_name)\n",
    "            model_id = model_id_lookup.get(run_name, \"\")\n",
    "            is_cpu_run = \"cpu\" in run_name.lower() or \"ridge\" in run_name.lower() or \"xgboost\" in run_name.lower()\n",
    "            device_label = \"CPU\" if is_cpu_run else \"GPU\"\n",
    "            label = f\"{display_name}\\n{run_name}\"\n",
    "            rows.append({\n",
    "                \"run_label\": run_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"peak_rss_gib\": max_rss,\n",
    "                \"peak_stage\": peak_stage,\n",
    "                \"resolved_name\": display_name,\n",
    "                \"accelerator\": device_label,\n",
    "                \"label\": label,\n",
    "                \"log_file\": path.name,\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        resource_df = pd.DataFrame(rows)\n",
    "        sorted_df = resource_df.sort_values(\"peak_rss_gib\", ascending=False)\n",
    "        analysis_state[\"resource_usage\"] = sorted_df\n",
    "        table = sorted_df.rename(\n",
    "            columns={\n",
    "                \"resolved_name\": \"Model label\",\n",
    "                \"run_label\": \"Run name\",\n",
    "                \"model_id\": \"Model ID\",\n",
    "                \"accelerator\": \"Accelerator\",\n",
    "                \"peak_rss_gib\": \"Peak RSS (GiB)\",\n",
    "                \"peak_stage\": \"Peak stage\",\n",
    "                \"log_file\": \"Log file\",\n",
    "            }\n",
    ")\n",
    "        display(\n",
    "            table[\n",
    "                [\n",
    "                    \"Model label\",\n",
    "                    \"Run name\",\n",
    "                    \"Model ID\",\n",
    "                    \"Accelerator\",\n",
    "                    \"Peak RSS (GiB)\",\n",
    "                    \"Peak stage\",\n",
    "                    \"Log file\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "        valid = sorted_df.dropna(subset=[\"peak_rss_gib\"]).copy()\n",
    "        if valid.empty:\n",
    "            print(\"Resource snapshots not yet present in the logs; rerun after the jobs emit resource metrics.\")\n",
    "        else:\n",
    "            color_map = {\"CPU\": \"#4C72B0\", \"GPU\": \"#DD8452\"}\n",
    "            colors = valid[\"accelerator\"].map(color_map).fillna(\"#808080\")\n",
    "            fig_height = max(3.8, 0.4 * len(valid))\n",
    "            fig_resource, ax = plt.subplots(figsize=(9, fig_height))\n",
    "            bars = ax.barh(valid[\"label\"], valid[\"peak_rss_gib\"], color=colors)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Peak RSS (GiB)\")\n",
    "            ax.set_ylabel(\"Model run\")\n",
    "            ax.set_title(\"Peak Memory Usage Across SPEAR Cell-wise Runs\")\n",
    "            xmax = valid[\"peak_rss_gib\"].max()\n",
    "            if xmax <= 0:\n",
    "                xmax = 1.0\n",
    "            ax.set_xlim(0, xmax * 1.15)\n",
    "            offset = max(1.0, xmax * 0.03)\n",
    "            for bar, (_, row) in zip(bars, valid.iterrows()):\n",
    "                stage_note = \"\"\n",
    "                if isinstance(row[\"peak_stage\"], str) and row[\"peak_stage\"]:\n",
    "                    stage_note = f\" @ {row['peak_stage']}\"\n",
    "                ax.text(\n",
    "                    bar.get_width() + offset,\n",
    "                    bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{row['peak_rss_gib']:.1f} GiB{stage_note}\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8.5,\n",
    "                    color=\"#1f1f1f\",\n",
    "                )\n",
    "            legend_handles = [\n",
    "                Patch(color=color_map[key], label=key)\n",
    "                for key in sorted(valid[\"accelerator\"].dropna().unique())\n",
    "                if key in color_map\n",
    "            ]\n",
    "            if legend_handles:\n",
    "                ax.legend(handles=legend_handles, title=\"Accelerator\", loc=\"lower right\")\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            fig_resource.tight_layout()\n",
    "            register_figure(FIGURES, \"resource_usage_summary\", fig_resource)\n",
    "            display(fig_resource)\n",
    "            plt.close(fig_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Summarize per-model runtime + peak resource usage from per-run logs.\n",
    "analysis_state = globals().get(\"analysis_state\", {})\n",
    "if not isinstance(analysis_state, dict):\n",
    "    analysis_state = {}\n",
    "    globals()[\"analysis_state\"] = analysis_state\n",
    "\n",
    "log_dir = config.project_root / \"output\" / \"logs\"\n",
    "log_paths = sorted(log_dir.glob(\"spear_*_compute_*.log\"))\n",
    "if not log_paths:\n",
    "    print(\"No model run logs found under output/logs; skipping resource summary.\")\n",
    "else:\n",
    "    run_df = analysis_state.get(\"run_df\")\n",
    "    display_lookup: dict[str, str] = {}\n",
    "    model_lookup: dict[str, str] = {}\n",
    "    if isinstance(run_df, pd.DataFrame) and not run_df.empty:\n",
    "        if \"model_display\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            display_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_display\"]))\n",
    "        if \"model_id\" in run_df.columns and \"run_name\" in run_df.columns:\n",
    "            model_lookup = dict(zip(run_df[\"run_name\"], run_df[\"model_id\"]))\n",
    "\n",
    "    run_name_pattern = re.compile(r\"run_name=([\\w\\-]+)\")\n",
    "    resource_pattern = re.compile(\n",
    "        r\"Resource snapshot\\s*\\|\\s*(?P<context>[^|]+)\\|\\s*rss=(?P<rss>[0-9.]+)\\s*GiB\\s*\\|\\s*cpu%=(?P<cpu>[0-9.]+)\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    timestamp_pattern = re.compile(r\"^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})\")\n",
    "\n",
    "    rows: list[dict[str, object]] = []\n",
    "    for path in log_paths:\n",
    "        try:\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                lines = fh.readlines()\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {path.name}: failed to read ({exc}).\")\n",
    "            continue\n",
    "\n",
    "        run_name = None\n",
    "        max_rss = None\n",
    "        max_cpu = None\n",
    "        start_ts = None\n",
    "        end_ts = None\n",
    "        end_ts_complete = None\n",
    "        for line in lines:\n",
    "            if run_name is None and \"run_name=\" in line:\n",
    "                match = run_name_pattern.search(line)\n",
    "                if match:\n",
    "                    run_name = match.group(1)\n",
    "            if start_ts is None:\n",
    "                ts_match = timestamp_pattern.match(line)\n",
    "                if ts_match:\n",
    "                    try:\n",
    "                        start_ts = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                    except ValueError:\n",
    "                        start_ts = None\n",
    "            ts_match = timestamp_pattern.match(line)\n",
    "            if ts_match:\n",
    "                try:\n",
    "                    end_ts = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            if \"RUN_COMPLETE_STATUS\" in line and ts_match:\n",
    "                try:\n",
    "                    end_ts_complete = datetime.strptime(ts_match.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                except ValueError:\n",
    "                    end_ts_complete = None\n",
    "            res_match = resource_pattern.search(line)\n",
    "            if res_match:\n",
    "                rss_val = float(res_match.group(\"rss\"))\n",
    "                cpu_val = float(res_match.group(\"cpu\"))\n",
    "                if max_rss is None or rss_val > max_rss:\n",
    "                    max_rss = rss_val\n",
    "                if max_cpu is None or cpu_val > max_cpu:\n",
    "                    max_cpu = cpu_val\n",
    "\n",
    "        final_end = end_ts_complete or end_ts\n",
    "        total_seconds = None\n",
    "        if start_ts and final_end:\n",
    "            delta = final_end - start_ts\n",
    "            total_seconds = delta.total_seconds()\n",
    "\n",
    "        if run_name:\n",
    "            display_name = display_lookup.get(run_name, run_name)\n",
    "            model_id = model_lookup.get(run_name, \"\")\n",
    "            rows.append({\n",
    "                \"run_name\": run_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"display_name\": display_name,\n",
    "                \"log_file\": path.name,\n",
    "                \"peak_rss_gib\": max_rss,\n",
    "                \"peak_cpu_pct\": max_cpu,\n",
    "                \"start_time\": start_ts,\n",
    "                \"end_time\": final_end,\n",
    "                \"runtime_seconds\": total_seconds,\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        resource_df = pd.DataFrame(rows)\n",
    "        sorted_df = resource_df.sort_values(\"runtime_seconds\", ascending=False, na_position=\"last\")\n",
    "        analysis_state[\"resource_runtime\"] = sorted_df\n",
    "\n",
    "        table = sorted_df.rename(\n",
    "            columns={\n",
    "                \"display_name\": \"Model display name\",\n",
    "                \"run_name\": \"Run name\",\n",
    "                \"model_id\": \"Model ID\",\n",
    "                \"log_file\": \"Log file\",\n",
    "                \"peak_rss_gib\": \"Peak RSS (GiB)\",\n",
    "                \"peak_cpu_pct\": \"Peak CPU %\",\n",
    "                \"runtime_seconds\": \"Runtime (s)\",\n",
    "            }\n",
    "        )\n",
    "        display(\n",
    "            table[\n",
    "                [\n",
    "                    \"Model display name\",\n",
    "                    \"Run name\",\n",
    "                    \"Model ID\",\n",
    "                    \"Peak RSS (GiB)\",\n",
    "                    \"Peak CPU %\",\n",
    "                    \"Runtime (s)\",\n",
    "                    \"Log file\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        valid = sorted_df.dropna(subset=[\"runtime_seconds\"]).copy()\n",
    "        if not valid.empty:\n",
    "            valid[\"runtime_hours\"] = valid[\"runtime_seconds\"] / 3600.0\n",
    "            fig_runtime, ax = plt.subplots(figsize=(9, max(3.8, 0.4 * len(valid))))\n",
    "            ax.barh(valid[\"display_name\"], valid[\"runtime_hours\"], color=\"#55A868\")\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Runtime (hours)\")\n",
    "            ax.set_ylabel(\"Model run\")\n",
    "            ax.set_title(\"Per-run Total Runtime (log-sourced)\")\n",
    "            xmax = valid[\"runtime_hours\"].max()\n",
    "            if xmax <= 0:\n",
    "                xmax = 1.0\n",
    "            ax.set_xlim(0, xmax * 1.15)\n",
    "            offset = max(0.1, xmax * 0.03)\n",
    "            for idx, row in valid.iterrows():\n",
    "                ax.text(\n",
    "                    row[\"runtime_hours\"] + offset,\n",
    "                    row[\"display_name\"],\n",
    "                    f\"{row['runtime_hours']:.2f} h\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8.5,\n",
    "                    color=\"#1f1f1f\",\n",
    "                )\n",
    "            sns.despine(ax=ax, left=True, bottom=True)\n",
    "            fig_runtime.tight_layout()\n",
    "            register_figure(FIGURES, \"runtime_comparison\", fig_runtime)\n",
    "            display(fig_runtime)\n",
    "            plt.close(fig_runtime)\n",
    "    else:\n",
    "        print(\"No matching run logs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11318170",
   "metadata": {},
   "source": [
    "## Appendix: Feature Importance Quick Guide\n",
    "\n",
    "Extra diagnostics for exploring per-gene feature importance outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a305de",
   "metadata": {},
   "source": [
    "### Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from spear.visualization import plot_per_gene_feature_panel, plot_cumulative_importance_overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23962433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best-performing model from the analysis summary\n",
    "best_details = analysis_state.get(\"best_model_details\")\n",
    "if best_details is None:\n",
    "    raise RuntimeError(\"Best-model details missing; rerun Sections 4-5 before this cell.\")\n",
    "RUN_DIR = Path(best_details[\"model_dir\"]).resolve()\n",
    "RUN_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4538a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_path = RUN_DIR / 'feature_importances_mean.csv'\n",
    "per_gene_path = RUN_DIR / 'feature_importance_per_gene_summary.csv'\n",
    "if not aggregate_path.exists() or not per_gene_path.exists():\n",
    "    print('Feature importance files missing; skipping feature importance preview.')\n",
    "    aggregate_df = pd.DataFrame()\n",
    "    per_gene_df = pd.DataFrame()\n",
    "else:\n",
    "    aggregate_df = pd.read_csv(aggregate_path)\n",
    "    per_gene_df = pd.read_csv(per_gene_path)\n",
    "    print(f'Aggregate features: {len(aggregate_df):,}')\n",
    "    print(f'Genes summarized: {len(per_gene_df):,}')\n",
    "    display(aggregate_df.head())\n",
    "    display(per_gene_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7780e0",
   "metadata": {},
   "source": [
    "### Dataset-level overlays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_path = RUN_DIR / \"feature_importance_distance_overview.png\"\n",
    "scatter_path = RUN_DIR / \"feature_importance_vs_tss_distance.png\"\n",
    "for label, path in (('Cumulative overlay', overlay_path), ('Scatter', scatter_path)):\n",
    "    if path.exists():\n",
    "        display(Image(filename=str(path)))\n",
    "    else:\n",
    "        print(f'No file found for {label}: {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98182e11",
   "metadata": {},
   "source": [
    "### Per-gene panels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b26de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_dir = RUN_DIR / \"per_gene_panels\"\n",
    "panel_paths = sorted(panel_dir.glob('*.png'))\n",
    "print(f'Found {len(panel_paths)} per-gene panels')\n",
    "for path in panel_paths[:4]:\n",
    "    display(Image(filename=str(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb89ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_custom_panel(gene_name: str, top_n: int = 12):\n",
    "    subset = aggregate_df[aggregate_df.get('gene_name') == gene_name]\n",
    "    if subset.empty:\n",
    "        raise ValueError(f'No features found for gene {gene_name}')\n",
    "    out_path = RUN_DIR / 'per_gene_panels' / f'custom_{gene_name}.png'\n",
    "    plot_per_gene_feature_panel(subset, gene_name, out_path, top_n=top_n)\n",
    "    display(Image(filename=str(out_path)))\n",
    "\n",
    "if per_gene_df.empty or 'importance_mean_sum' not in per_gene_df.columns:\n",
    "    print('No per-gene feature importance data available; skipping example panel.')\n",
    "else:\n",
    "    example_gene = per_gene_df.sort_values('importance_mean_sum', ascending=False)['gene'].iloc[0]\n",
    "    render_custom_panel(example_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_overlay(max_distance_kb: float | None = None):\n",
    "    required = {'importance_mean', 'distance_to_tss_kb'}\n",
    "    if aggregate_df.empty or not required.issubset(aggregate_df.columns):\n",
    "        print('Feature importance overlay unavailable; missing data.')\n",
    "        return\n",
    "    df = aggregate_df.dropna(subset=['importance_mean', 'distance_to_tss_kb'])\n",
    "    if max_distance_kb is not None:\n",
    "        df = df[df['distance_to_tss_kb'].abs() <= max_distance_kb]\n",
    "    out_path = RUN_DIR / 'feature_importance_distance_overlay_custom.png'\n",
    "    plot_cumulative_importance_overlay(\n",
    "        df['importance_mean'],\n",
    "        df['distance_to_tss_kb'],\n",
    "        out_path,\n",
    "        'Custom FI cumulative profile'\n",
    "    )\n",
    "    display(Image(filename=str(out_path)))\n",
    "\n",
    "recompute_overlay(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grn_ml_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}